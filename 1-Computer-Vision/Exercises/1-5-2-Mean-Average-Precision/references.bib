%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jonathan Moran at 2022-09-25 15:08:31 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{electronics10030279,
	abstract = {Recent outstanding results of supervised object detection in competitions and challenges are often associated with specific metrics and datasets. The evaluation of such methods applied in different contexts have increased the demand for annotated datasets. Annotation tools represent the location and size of objects in distinct formats, leading to a lack of consensus on the representation. Such a scenario often complicates the comparison of object detection methods. This work alleviates this problem along the following lines: (i) It provides an overview of the most relevant evaluation methods used in object detection competitions, highlighting their peculiarities, differences, and ad- vantages; (ii) it examines the most used annotation formats, showing how different implementations may influence the assessment results; and (iii) it provides a novel open-source toolkit supporting dif- ferent annotation formats and 15 performance metrics, making it easy for researchers to evaluate the performance of their detection algorithms in most known datasets. In addition, this work proposes a new metric, also included in the toolkit, for evaluating object detection in videos that is based on the spatio-temporal overlap between the ground-truth and detected bounding boxes.},
	article-number = {279},
	author = {Padilla, Rafael and Passos, Wesley L. and Dias, Thadeu L. B. and Netto, Sergio L. and da Silva, Eduardo A. B.},
	date-added = {2022-09-25 15:07:44 -0700},
	date-modified = {2022-09-25 15:08:29 -0700},
	doi = {10.3390/electronics10030279},
	issn = {2079-9292},
	journal = {Electronics},
	number = {3},
	title = {A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit},
	url = {https://www.mdpi.com/2079-9292/10/3/279},
	volume = {10},
	year = {2021}}

@article{VOC,
	abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher and Winn, John and Zisserman, Andrew},
	date-modified = {2022-09-25 15:07:23 -0700},
	doi = {10.1007/s11263-009-0275-4},
	journal = {International Journal of Computer Vision},
	month = {06},
	pages = {303-338},
	title = {The Pascal Visual Object Classes (VOC) challenge},
	volume = {88},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1007/s11263-009-0275-4}}
