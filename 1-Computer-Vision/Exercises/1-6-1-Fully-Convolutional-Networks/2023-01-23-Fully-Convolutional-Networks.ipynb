{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dff515c",
      "metadata": {
        "id": "1dff515c"
      },
      "source": [
        "# Exercise 1.6.1 — Fully Convolutional Networks\n",
        "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
        "From the Self-Driving Car Engineer Nanodegree offered at Udacity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b11fc494",
      "metadata": {
        "id": "b11fc494"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c37e77",
      "metadata": {
        "id": "21c37e77"
      },
      "source": [
        "* Use the $1\\times1$ [`Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) convolutional layer to effectively \"replace\" a [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer by preserving (not modifying) the input shape;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e99aa2a",
      "metadata": {
        "id": "5e99aa2a"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8ff654",
      "metadata": {
        "id": "7b8ff654"
      },
      "outputs": [],
      "source": [
        "### Importing the required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c2dfcf",
      "metadata": {
        "id": "03c2dfcf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from typing import List, Union, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9662e3a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 42
        },
        "id": "9662e3a3",
        "outputId": "118ba652-27cb-4494-af57-8af06584cbfc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.9.2'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53376af5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 42
        },
        "id": "53376af5",
        "outputId": "71d05257-9168-445c-f215-20b4e66770e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa5c7375",
      "metadata": {
        "id": "fa5c7375"
      },
      "outputs": [],
      "source": [
        "### Setting the environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7157b7f1",
      "metadata": {
        "id": "7157b7f1"
      },
      "outputs": [],
      "source": [
        "ENV_COLAB = True                # True if running in Google Colab instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d4b8e9",
      "metadata": {
        "id": "82d4b8e9"
      },
      "outputs": [],
      "source": [
        "# Root directory\n",
        "DIR_BASE = '' if not ENV_COLAB else '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54f73e13",
      "metadata": {
        "id": "54f73e13"
      },
      "outputs": [],
      "source": [
        "# Subdirectory to save output files\n",
        "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
        "# Subdirectory pointing to input data\n",
        "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2301524",
      "metadata": {
        "id": "e2301524"
      },
      "outputs": [],
      "source": [
        "### Creating subdirectories (if not exists)\n",
        "os.makedirs(DIR_OUT, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "988098fe",
      "metadata": {
        "id": "988098fe"
      },
      "source": [
        "### 1.1. Fully Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526d1f90",
      "metadata": {
        "id": "526d1f90"
      },
      "source": [
        "[Fully convolutional networks](https://d2l.ai/chapter_computer-vision/fcn.html#fully-convolutional-networks) is a type of [convolutional neural network]() (CNN) which extracts feature maps from input images. Unlike traditional CNNs, a fully convolutional network transforms the dimensions of the intermediate feature maps (i.e., output tensors) back to the original height and width of the input image using [transposed convolutions](https://d2l.ai/chapter_computer-vision/transposed-conv.html#sec-transposed-conv) (covered in Sect. 1.2). What makes fully convolutional networks unique is their ability to preserve 2D spatial information; this is unlike [fully _connected_ networks](https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html) which flatten a 2D image to a 1D vector.\n",
        "\n",
        "Convolution layers serve as the backbone of modern-day perception nets — they operate under a set of working principles and constraints (translation equivariance, spatial locality, efficiency) that help make intractible image processing workloads possible. With fully convolutional networks we can reduce spatial dimensionality and process images of arbitrary depth (i.e., channels), from three-channel RGB images to n-dimensional satellite [spectral images](https://en.wikipedia.org/wiki/Spectral_imaging) by modifying the number of filters used and the number of output channels expected in the convolutional layers. \n",
        "\n",
        "In summary, fully convolutional networks (FCN) allow us to take advantage of the spatial relationships between pixels in input images. The convolutional layers making up an FCN can be configured to handle three- or more channel images and can either preserve, reduce or upscale their respective inputs. This functionality provided by the fully convolutional network is what often motivates their use in both research and in practise today."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4_sCks4lRcyy",
      "metadata": {
        "id": "4_sCks4lRcyy"
      },
      "source": [
        "#### From Fully-Connected to Fully-Convolutional Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XQMQ2Q1zRcvP",
      "metadata": {
        "id": "XQMQ2Q1zRcvP"
      },
      "source": [
        "In this task, we rewrite a Dense fully-connected layer ([`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) as a 2D Convolutional layer ([`tf.keras.layers.Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)). To do so, we follow the simple guidelines for converting a fully-connected to a fully-convolutional layer:\n",
        "   * The number of _outputs_ of the fully-connected layer becomes the _kernel size_ of the fully-convolutional layer;\n",
        "   * The number of _inputs_ of the fully-connected layer becomes the number of _weights_ of the fully-convolutional layer.\n",
        "    \n",
        "In addition to the above rules-of-thumb, we also enforce the following specifications for our 1x1 2D Convolutional layer:\n",
        "   * **Filter size**: $1\\times 1$;\n",
        "   * **Stride**: $1$;\n",
        "   * **Padding**: Zero-padding.\n",
        "   \n",
        "Note that the filter size is used interchangeably with **kernel size** here. By setting our filter to a size of $1\\times 1$, we are analogising fully-connected layers with fully-convolutional layers; however, with this convolutional layer we introduce the ability of the network to preserve spatial information in the input tensor. This is in contrast with the Dense fully-connected layer which does not have the ability to preserve spatial information, since it flattens the input image into a one-dimensional vector. \n",
        "\n",
        "The ability to preserve 2D spatial information can be used to identify features in the input image and make predictions more accurately when compared to the original Fully Connected Network. Additionally, the use of a $1\\times 1$ convolutional layer can reduce the number of parameters in the network and therefore make it more computationally-efficient. By choosing a filter size of $1\\times 1$, we get the ability to reduce network parameters while maintaining a significant amount of spatial information throughout the network, which might not be the case when selecting larger filter sizes (e.g., $3\\times 3$ or $5\\times 5$). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a1fa279",
      "metadata": {
        "id": "3a1fa279"
      },
      "source": [
        "### 1.2. Transposed Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3433560f",
      "metadata": {
        "id": "3433560f"
      },
      "source": [
        "[Transposed convolutions](https://d2l.ai/chapter_computer-vision/transposed-conv.html) help [upsample](https://en.wikipedia.org/wiki/Upsampling) the output of a previous layer to a higher resolution or spatial dimension (height and width). Transposed convolutions work in contrast with typical convolutional layers, which tend to downsample (reduce) the spatial dimensions of the input and are therefore sometimes considered to be \"deconvolution\" layers.\n",
        "\n",
        "The word \"transposed\" here refers to the act of _transferring something_ to a different place or context. Here, transposed convolutions _transfer_ patches of data from a matrix onto a [sparse tensor](https://nvidia.github.io/MinkowskiEngine/tutorial/sparse_tensor_basic.html) (i.e., a vector with many zero-valued entries). Using the patches of data, the sparse regions of the tensor are assigned values (\"filled\"). A neat animation of this process created by [V. Dumoulin](https://github.com/vdumoulin/conv_arithmetic) [1] is replicated below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f-l7qDGiZcZJ",
      "metadata": {
        "id": "f-l7qDGiZcZJ"
      },
      "source": [
        "<table style=\"width:100%; table-layout:fixed;\">\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"figures/conv_arithmetic/no_padding_no_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"figures/conv_arithmetic/arbitrary_padding_no_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"figures/conv_arithmetic/same_padding_no_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"figures/conv_arithmetic/full_padding_no_strides_transposed.gif\"></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No padding, no strides, transposed</td>\n",
        "    <td>Arbitrary padding, no strides, transposed</td>\n",
        "    <td>Half padding, no strides, transposed</td>\n",
        "    <td>Full padding, no strides, transposed</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><img width=\"150px\" src=\"figures/conv_arithmetic/no_padding_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"figures/conv_arithmetic/padding_strides_transposed.gif\"></td>\n",
        "    <td><img width=\"150px\" src=\"figures/conv_arithmetic/padding_strides_odd_transposed.gif\"></td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No padding, strides, transposed</td>\n",
        "    <td>Padding, strides, transposed</td>\n",
        "    <td>Padding, strides, transposed (odd)</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\textrm{Figure 1. Transposed Convolution Operations — Visualised (credit: V. Dumoulin).}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "In the above animations by V. Dumoulin [1], we note that the blue grid corresponds to an input feature map, whereas the cyan grid is the output feature map."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e48c0b2b",
      "metadata": {
        "id": "e48c0b2b"
      },
      "source": [
        "#### Why \"transposed\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fxkJLT1MbS9Z",
      "metadata": {
        "id": "fxkJLT1MbS9Z"
      },
      "source": [
        "The transposed convolution process is akin to a matrix transpose; we can implement the convolution with the following:\n",
        "\n",
        "Given an input vector $\\mathrm{x}$ and a weight matrix $\\mathrm{W}$, the forward propagation function of the convolution is given by $\\mathrm{y} = \\mathrm{W}\\cdot \\mathrm{x}$, where $\\mathrm{y}$ is the output vector. The backpropagation function of the convolution can be represented as a multiplication of the input with the transposed weight matrix $\\mathrm{W}^{\\top}$, since the backpropagation follows the chain rule with definition $\\nabla_{\\mathrm{x}}\\mathrm{y} = \\mathrm{W}^{\\top}$. Therefore, the transposed convolution layer can be boiled down to just swapping the forward propagation function of the transposed convolution layer with the backpropagation function of the previous convolution layer. Since the math is essentially the same as with traditional convolution layers, the property of differentiability is thus retained which makes the training of transposed convolution layers the same as with previous networks.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13sMtjI7d4Rd",
      "metadata": {
        "id": "13sMtjI7d4Rd"
      },
      "source": [
        "#### More details"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nUqO5bupd6J-",
      "metadata": {
        "id": "nUqO5bupd6J-"
      },
      "source": [
        "The transposed convolution layer, like the regular convolution layer, depends also on the configuration of the [padding and stride](https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html) amounts. Unlike the traditional convolution layer, transposed convolution layers apply the padding to the _output_, rather than the _input_. So with a $\\left(\\textrm{height}, \\textrm{width}\\right)$ padding of e.g., $(1, 1)$, the transposed layer will _remove_ the first and last rows of the output tensor after it has passed through the layer. \n",
        "\n",
        "The _stride_ of a transposed convolution layer is also slightly different than in the regular convolution layer. The stride affects the amount of overlap between the adjacent intermediate kernel tensor elements and the _output_ feature map. Changing the $\\left(\\textrm{height}, \\textrm{width}\\right)$ stride from e.g., $(1, 1)$ to $(2, 2)$ will increase both the height and width of the intermediate tensors and therefore increase the amount of overlap between the intermediate tensors and the _output_ feature map. With a regular convolution layer, the stride parameter affects the amount of overlap between the adjacent kernel elements and the _input_ feature map.\n",
        "\n",
        "For example, assuming an input feature map of dimensions $3\\times 3 \\times 1$, and given a desired upsampling to dimensions $6\\times 6\\times 1$, we can initialise a [`tf.keras.layers.Conv2DTranspose`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) layer with a stride of $(2, 2)$, a kernel size of $(1, 1)$, and when using a padding configuration of `\"SAME\"` we get an output feature map of dimensions $6\\times 6\\times 1$. To illustrate this in code, consider the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eIhLAHEjEkn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eIhLAHEjEkn",
        "outputId": "73c33db2-a38e-42ba-d3ec-e41a3c55656e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([1, 3, 3, 1])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Defining our input feature map\n",
        "# Input dimensions: [BATCH_SIZE, HEIGHT_IN, WIDTH_IN, CHANNELS_IN]\n",
        "x = tf.convert_to_tensor(\n",
        "    np.random.randn(1, 3, 3, 1), \n",
        "    dtype=tf.float32\n",
        ")\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LNgWI2Dci9MK",
      "metadata": {
        "id": "LNgWI2Dci9MK"
      },
      "outputs": [],
      "source": [
        "### Creating the Conv2DTranspose layer with above configurations\n",
        "model_conv_t = tf.keras.models.Sequential()\n",
        "model_conv_t.add(\n",
        "    tf.keras.layers.Conv2DTranspose(\n",
        "        filters=1,                  # Num. input channels\n",
        "        kernel_size=(1, 1),\n",
        "        strides=(2, 2),\n",
        "        padding='SAME'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9imFslghjgOF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9imFslghjgOF",
        "outputId": "535456d7-917d-4758-a9f7-b4f420601868"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([1, 6, 6, 1])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Applying the input feature map to the transposed convolution layer\n",
        "model_conv_t(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2LNgOcKLjrhc",
      "metadata": {
        "id": "2LNgOcKLjrhc"
      },
      "source": [
        "With this example we see that the dimensions of the output feature map is indeed $6\\times 6 \\times 1$ (note: the first dimension of value $1$ is the batch size — here `x` contains only one input sample). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b75383f3",
      "metadata": {
        "id": "b75383f3"
      },
      "source": [
        "## 2. Programming Task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SndrKz75Pebi",
      "metadata": {
        "id": "SndrKz75Pebi"
      },
      "source": [
        "NOTE: the code provided here has been migrated to the TensorFlow 2.x API. Some functionality may differ from the original implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c7b42b",
      "metadata": {
        "id": "95c7b42b"
      },
      "source": [
        "### 2.1. Fully Convolutional Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d94a9b",
      "metadata": {
        "id": "98d94a9b"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `quiz.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gvtFP7-KJ-GK",
      "metadata": {
        "id": "gvtFP7-KJ-GK"
      },
      "outputs": [],
      "source": [
        "# custom init with the seed set to 0 by default\n",
        "def custom_init(\n",
        "        shape: Union[tf.Tensor, List[int], Tuple[int]], \n",
        "        dtype: tf.dtypes.DType=tf.float32,\n",
        "        seed: int=0,\n",
        "        partition_info=None\n",
        ") -> tf.Tensor:\n",
        "    \"\"\"Initialises the weights of a layer.\n",
        "    \n",
        "    Samples the values at random from a parameterised normal distribution.\n",
        "    \n",
        "    :param shape: Shape of the weight vector (i.e., number of weights).\n",
        "    :param dtype: Data type of the weight vector values to return.\n",
        "    :param seed: Value of the random seed to create.\n",
        "    :param partition_info: Optional info about paritioning of a tensor,\n",
        "        not used in TF2.x API.\n",
        "    :returns: weight vector of randomly initialised values.\n",
        "    \"\"\"\n",
        "    return tf.random.normal(\n",
        "        shape=shape, \n",
        "        dtype=dtype, \n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1(\n",
        "        filters: int,\n",
        "        kernel_size: Union[List[int], Tuple[int], tf.Tensor]=(1, 1),\n",
        "        stride: int=1\n",
        ") -> tf.Tensor:\n",
        "    \"\"\"Initialises a 1x1 2D Convolutional layer.\n",
        "    \n",
        "    To convert a fully-connected to a fully-convolutional layer, we initialise\n",
        "    the 2D Convolutional layer parameters according to the following:\n",
        "       1. The number of outputs becomes the kernel size,\n",
        "       2. The number of inputs becomes the number of weights.\n",
        "    \n",
        "    NOTE: The `tf.layers.conv2d` API has been deprecated since TF1.15,\n",
        "    therefore we use the `tf.keras.layers.Conv2D` layer in TF2.x to initialise\n",
        "    the layer with modified arguments.\n",
        "    \n",
        "    :param filters: the dimensions of the output.\n",
        "    :param kernel_size: the dimensions of the kernel, i.e., size of the window\n",
        "    used in the convolution / sliding window operations.\n",
        "    :param stride: the amount of pixels to \"shift\" the filter over the input on\n",
        "    each sliding window operation.\n",
        "    :returns: the configured `Conv2D` layer.\n",
        "    \"\"\"\n",
        "    return tf.keras.layers.Conv2D(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=stride,\n",
        "        padding='VALID',\n",
        "        kernel_initializer=custom_init\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b69cdf43",
      "metadata": {
        "id": "b69cdf43"
      },
      "source": [
        "#### Testing the FCN layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D-yHC3eOTjYf",
      "metadata": {
        "id": "D-yHC3eOTjYf"
      },
      "outputs": [],
      "source": [
        "### Setting the parameters\n",
        "# Number of output channels (i.e., number of kernels)\n",
        "NUM_OUTPUTS = 2\n",
        "KERNEL_SIZE = (1, 1)\n",
        "# Number of pixels to \"move over\" for each sliding window operation\n",
        "STRIDE = (1, 1)\n",
        "# Batch size (i.e., number of samples per iteration)\n",
        "BATCH_SIZE = 1\n",
        "# Number of input channels\n",
        "CHANNELS_IN = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xa2JbYpyTliF",
      "metadata": {
        "id": "xa2JbYpyTliF"
      },
      "outputs": [],
      "source": [
        "### Creating an input tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h5yl-oeaTlcF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5yl-oeaTlcF",
        "outputId": "94d23407-2fab-4268-da75-6aa57e5317db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\n",
              "array([[[[ 0.5990718 ],\n",
              "         [ 0.34034762]],\n",
              "\n",
              "        [[-1.0601754 ],\n",
              "         [-0.4629273 ]]]], dtype=float32)>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = tf.convert_to_tensor(\n",
        "    np.random.randn(BATCH_SIZE, NUM_OUTPUTS, NUM_OUTPUTS, CHANNELS_IN), \n",
        "    dtype=tf.float32\n",
        ")\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XoWJXFKITq0m",
      "metadata": {
        "id": "XoWJXFKITq0m"
      },
      "source": [
        "The above \"dataset\" is defined as a 4-D tensor with a number of samples equal to one (i.e., `BATCH_SIZE = 1`). Since we are going to be comparing the effect of the `Conv2D` layer to the `Dense` layer on the output image size, we want to set our input \"image\" to be of size (`NUM_OUTPUT`, `NUM_OUTPUT`, `CHANNELS_IN`). That is, we are expecting the input and output size of the tensor to be the same (unmodified) after it is passed through either the `Conv2D` or the `Dense` layer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vxk55xSgTuhF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxk55xSgTuhF",
        "outputId": "7cd107b6-f13b-4ab7-866c-6de95a6ce5a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0OP6BempTude",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OP6BempTude",
        "outputId": "8c3d544b-b9b5-4b0f-fa33-d6af8c5d2ed8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([1, 2, 2, 1])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [batch_size, in_height, in_width, in_channels]\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZPRT4Y3ATysa",
      "metadata": {
        "id": "ZPRT4Y3ATysa"
      },
      "source": [
        "##### Creating the Conv2D layer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tjtuKPCbTyda",
      "metadata": {
        "id": "tjtuKPCbTyda"
      },
      "outputs": [],
      "source": [
        "model_conv = tf.keras.models.Sequential()\n",
        "model_conv.add(\n",
        "    conv_1x1(\n",
        "        filters=NUM_OUTPUTS, \n",
        "        kernel_size=KERNEL_SIZE, \n",
        "        stride=STRIDE\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ItUCeeZ-T52u",
      "metadata": {
        "id": "ItUCeeZ-T52u"
      },
      "source": [
        "##### Creating the Dense layer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qua_wsv5TyZg",
      "metadata": {
        "id": "Qua_wsv5TyZg"
      },
      "outputs": [],
      "source": [
        "model_dense = tf.keras.models.Sequential()\n",
        "model_dense.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=NUM_OUTPUTS,\n",
        "        kernel_initializer=custom_init\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0OlNGkX2T8vg",
      "metadata": {
        "id": "0OlNGkX2T8vg"
      },
      "source": [
        "##### Passing the input tensor through each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jaAMs3baTyVz",
      "metadata": {
        "id": "jaAMs3baTyVz"
      },
      "outputs": [],
      "source": [
        "conv_out = model_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p7HsAH5kTyR7",
      "metadata": {
        "id": "p7HsAH5kTyR7"
      },
      "outputs": [],
      "source": [
        "dense_out = model_dense(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lskgJr-dUGE7",
      "metadata": {
        "id": "lskgJr-dUGE7"
      },
      "source": [
        "##### Comparing the output shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NcM0rcskUHSM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcM0rcskUHSM",
        "outputId": "0594ea47-a57e-459d-8b7a-b1dee5b12d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv_out.shape == dense_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79osb_W5UF_6",
      "metadata": {
        "id": "79osb_W5UF_6"
      },
      "source": [
        "Since we assume the $1\\times 1$ convolutional layer configuration, we observe here that both network layers produce the same output shape of $(1, 2, 2, 2)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ybow-u_QMrp",
      "metadata": {
        "id": "7ybow-u_QMrp"
      },
      "source": [
        "### 2.2. Transposed Convolutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3FimaznmO9L6",
      "metadata": {
        "id": "3FimaznmO9L6"
      },
      "outputs": [],
      "source": [
        "# TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zF1_tbu9Prio",
      "metadata": {
        "id": "zF1_tbu9Prio"
      },
      "source": [
        "## 3. Closing Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vgzdo0caPrav",
      "metadata": {
        "id": "Vgzdo0caPrav"
      },
      "source": [
        "##### Alternatives\n",
        "* TODO.\n",
        "\n",
        "##### Extensions of task\n",
        "* TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DjfeAE3bPrUL",
      "metadata": {
        "id": "DjfeAE3bPrUL"
      },
      "source": [
        "## 4. Future Work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5QnuD7ArPrOb",
      "metadata": {
        "id": "5QnuD7ArPrOb"
      },
      "source": [
        "* TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xTd8tdPVP2EC",
      "metadata": {
        "id": "xTd8tdPVP2EC"
      },
      "source": [
        "## Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tvnrEkB4P175",
      "metadata": {
        "id": "tvnrEkB4P175"
      },
      "source": [
        "This assignment was prepared by David Siller, Kelvin Lwin et al., 2020 (link [here]).\n",
        "\n",
        "References\n",
        "* [1] Dumoulin, V. et al. A Guide to Convolution Arithmetic for Deep Learning. arXiv. [doi:10.48550/arXiv.1603.07285](https://arxiv.org/abs/1603.07285).\n",
        "\n",
        "\n",
        "Helpful resources:\n",
        "* [14.10. Transposed Convolution | Dive Into Deep Learning](https://d2l.ai/chapter_computer-vision/transposed-conv.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KoWL8WwmZ910",
      "metadata": {
        "id": "KoWL8WwmZ910"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}