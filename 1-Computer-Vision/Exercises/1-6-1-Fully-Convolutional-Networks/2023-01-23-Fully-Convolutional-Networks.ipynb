{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dff515c",
      "metadata": {
        "id": "1dff515c"
      },
      "source": [
        "# Exercise 1.6.1 â€” Fully Convolutional Networks\n",
        "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
        "From the Self-Driving Car Engineer Nanodegree offered at Udacity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b11fc494",
      "metadata": {
        "id": "b11fc494"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c37e77",
      "metadata": {
        "id": "21c37e77"
      },
      "source": [
        "* TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e99aa2a",
      "metadata": {
        "id": "5e99aa2a"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8ff654",
      "metadata": {
        "id": "7b8ff654"
      },
      "outputs": [],
      "source": [
        "### Importing the required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c2dfcf",
      "metadata": {
        "id": "03c2dfcf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from typing import List, Union, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9662e3a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 42
        },
        "id": "9662e3a3",
        "outputId": "f83d84f6-19b8-413d-fece-106cd91d49aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53376af5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 42
        },
        "id": "53376af5",
        "outputId": "fdb5023e-73b9-4eda-e800-586b245aa655"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa5c7375",
      "metadata": {
        "id": "fa5c7375"
      },
      "outputs": [],
      "source": [
        "### Setting the environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7157b7f1",
      "metadata": {
        "id": "7157b7f1"
      },
      "outputs": [],
      "source": [
        "ENV_COLAB = True                # True if running in Google Colab instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d4b8e9",
      "metadata": {
        "id": "82d4b8e9"
      },
      "outputs": [],
      "source": [
        "# Root directory\n",
        "DIR_BASE = '' if not ENV_COLAB else '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54f73e13",
      "metadata": {
        "id": "54f73e13"
      },
      "outputs": [],
      "source": [
        "# Subdirectory to save output files\n",
        "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
        "# Subdirectory pointing to input data\n",
        "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2301524",
      "metadata": {
        "id": "e2301524"
      },
      "outputs": [],
      "source": [
        "### Creating subdirectories (if not exists)\n",
        "os.makedirs(DIR_OUT, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "988098fe",
      "metadata": {
        "id": "988098fe"
      },
      "source": [
        "### 1.1. Fully Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526d1f90",
      "metadata": {
        "id": "526d1f90"
      },
      "source": [
        "TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### From Fully-Connected to Fully-Convolutional Layers"
      ],
      "metadata": {
        "id": "4_sCks4lRcyy"
      },
      "id": "4_sCks4lRcyy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we rewrite a Dense fully-connected layer (`tf.keras.layers.Dense`) as a 2D Convolutional layer (`tf.keras.layers.Conv2D`). To do so, we follow the simple guidelines for converting a fully-connected to a fully-convolutional layer:\n",
        "   * The number of _outputs_ of the fully-connected layer becomes the _kernel size_ of the fully-convolutional layer;\n",
        "   * The number of _inputs_ of the fully-connected layer becomes the number of _weights_ of the fully-convolutional layer.\n",
        "    \n",
        "In addition to the above rules-of-thumb, we also enforce the following specifications for our 1x1 2D Convolutional layer:\n",
        "   * **Filter size**: $1\\times 1$;\n",
        "   * **Stride**: $1$;\n",
        "   * **Padding**: Zero-padding.\n",
        "   \n",
        "Note that the filter size is used interchangeably with **kernel size** here. By setting our filter to a size of $1\\times 1$, we are analogising fully-connected layers with fully-convolutional layers; however, with this convolutional layer we introduce the ability of the network to preserve spatial information in the input tensor. This is in contrast with the Dense fully-connected layer which does not have the ability to preserve spatial information, since it flattens the input image into a one-dimensional vector. \n",
        "\n",
        "The ability to preserve 2D spatial information can be used to identify features in the input image and make predictions more accurately when compared to the original Fully Connected Network. Additionally, the use of a $1\\times 1$ convolutional layer can reduce the number of parameters in the network and therefore make it more computationally-efficient. By choosing a filter size of $1\\times 1$, we get the ability to reduce network parameters while maintaining a significant amount of spatial information throughout the network, which might not be the case when selecting larger filter sizes (e.g., $3\\times 3$ or $5\\times 5$). "
      ],
      "metadata": {
        "id": "XQMQ2Q1zRcvP"
      },
      "id": "XQMQ2Q1zRcvP"
    },
    {
      "cell_type": "markdown",
      "id": "3a1fa279",
      "metadata": {
        "id": "3a1fa279"
      },
      "source": [
        "### 1.2. Transposed Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3433560f",
      "metadata": {
        "id": "3433560f"
      },
      "source": [
        "TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b75383f3",
      "metadata": {
        "id": "b75383f3"
      },
      "source": [
        "## 2. Programming Task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: the code provided here has been migrated to the TensorFlow 2.x API. Some functionality may differ from the original implementation."
      ],
      "metadata": {
        "id": "SndrKz75Pebi"
      },
      "id": "SndrKz75Pebi"
    },
    {
      "cell_type": "markdown",
      "id": "95c7b42b",
      "metadata": {
        "id": "95c7b42b"
      },
      "source": [
        "### 2.1. Fully Convolutional Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d94a9b",
      "metadata": {
        "id": "98d94a9b"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `quiz.py`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom init with the seed set to 0 by default\n",
        "def custom_init(\n",
        "        shape: Union[tf.Tensor, List[int], Tuple[int]], \n",
        "        dtype: tf.dtypes.DType=tf.float32,\n",
        "        seed: int=0,\n",
        "        partition_info=None\n",
        ") -> tf.Tensor:\n",
        "    \"\"\"Initialises the weights of a layer.\n",
        "    \n",
        "    Samples the values at random from a parameterised normal distribution.\n",
        "    \n",
        "    :param shape: Shape of the weight vector (i.e., number of weights).\n",
        "    :param dtype: Data type of the weight vector values to return.\n",
        "    :param seed: Value of the random seed to create.\n",
        "    :param partition_info: Optional info about paritioning of a tensor,\n",
        "        not used in TF2.x API.\n",
        "    :returns: weight vector of randomly initialised values.\n",
        "    \"\"\"\n",
        "    return tf.random.normal(\n",
        "        shape=shape, \n",
        "        dtype=dtype, \n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1(\n",
        "        filters: int,\n",
        "        kernel_size: Union[List[int], Tuple[int], tf.Tensor]=(1, 1),\n",
        "        stride: int=1\n",
        ") -> tf.Tensor:\n",
        "    \"\"\"Initialises a 1x1 2D Convolutional layer.\n",
        "    \n",
        "    To convert a fully-connected to a fully-convolutional layer, we initialise\n",
        "    the 2D Convolutional layer parameters according to the following:\n",
        "       1. The number of outputs becomes the kernel size,\n",
        "       2. The number of inputs becomes the number of weights.\n",
        "    \n",
        "    NOTE: The `tf.layers.conv2d` API has been deprecated since TF1.15,\n",
        "    therefore we use the `tf.keras.layers.Conv2D` layer in TF2.x to initialise\n",
        "    the layer with modified arguments.\n",
        "    \n",
        "    :param filters: the dimensions of the output.\n",
        "    :param kernel_size: the dimensions of the kernel, i.e., size of the window\n",
        "    used in the convolution / sliding window operations.\n",
        "    :param stride: the amount of pixels to \"shift\" the filter over the input on\n",
        "    each sliding window operation.\n",
        "    :returns: the configured `Conv2D` layer.\n",
        "    \"\"\"\n",
        "    return tf.keras.layers.Conv2D(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=stride,\n",
        "        padding='VALID',\n",
        "        kernel_initializer=custom_init\n",
        "    )"
      ],
      "metadata": {
        "id": "gvtFP7-KJ-GK"
      },
      "id": "gvtFP7-KJ-GK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b69cdf43",
      "metadata": {
        "id": "b69cdf43"
      },
      "source": [
        "#### Testing the FCN layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting the parameters\n",
        "# Number of output channels (i.e., number of kernels)\n",
        "NUM_OUTPUTS = 2\n",
        "KERNEL_SIZE = (1, 1)\n",
        "# Number of pixels to \"move over\" for each sliding window operation\n",
        "STRIDE = (1, 1)\n",
        "# Batch size (i.e., number of samples per iteration)\n",
        "BATCH_SIZE = 1\n",
        "# Number of input channels\n",
        "CHANNELS_IN = 1"
      ],
      "metadata": {
        "id": "D-yHC3eOTjYf"
      },
      "id": "D-yHC3eOTjYf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating an input tensor"
      ],
      "metadata": {
        "id": "xa2JbYpyTliF"
      },
      "id": "xa2JbYpyTliF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.convert_to_tensor(\n",
        "    np.random.randn(BATCH_SIZE, NUM_OUTPUTS, NUM_OUTPUTS, CHANNELS_IN), \n",
        "    dtype=tf.float32\n",
        ")\n",
        "x"
      ],
      "metadata": {
        "id": "h5yl-oeaTlcF"
      },
      "id": "h5yl-oeaTlcF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above \"dataset\" is defined as a 4-D tensor with a number of samples equal to one (i.e., `BATCH_SIZE = 1`). Since we are going to be comparing the effect of the `Conv2D` layer to the `Dense` layer on the output image size, we want to set our input \"image\" to be of size (`NUM_OUTPUT`, `NUM_OUTPUT`, `CHANNELS_IN`). That is, we are expecting the input and output size of the tensor to be the same (unmodified) after it is passed through either the `Conv2D` or the `Dense` layer.  "
      ],
      "metadata": {
        "id": "XoWJXFKITq0m"
      },
      "id": "XoWJXFKITq0m"
    },
    {
      "cell_type": "code",
      "source": [
        "type(x)"
      ],
      "metadata": {
        "id": "vxk55xSgTuhF"
      },
      "id": "vxk55xSgTuhF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [batch_size, in_height, in_width, in_channels]\n",
        "x.shape"
      ],
      "metadata": {
        "id": "0OP6BempTude"
      },
      "id": "0OP6BempTude",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating the Conv2D layer model"
      ],
      "metadata": {
        "id": "ZPRT4Y3ATysa"
      },
      "id": "ZPRT4Y3ATysa"
    },
    {
      "cell_type": "code",
      "source": [
        "model_conv = tf.keras.models.Sequential()\n",
        "model_conv.add(\n",
        "    conv_1x1(\n",
        "        filters=NUM_OUTPUTS, \n",
        "        kernel_size=KERNEL_SIZE, \n",
        "        stride=STRIDE\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "tjtuKPCbTyda"
      },
      "id": "tjtuKPCbTyda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating the Dense layer model"
      ],
      "metadata": {
        "id": "ItUCeeZ-T52u"
      },
      "id": "ItUCeeZ-T52u"
    },
    {
      "cell_type": "code",
      "source": [
        "model_dense = tf.keras.models.Sequential()\n",
        "model_dense.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=NUM_OUTPUTS,\n",
        "        kernel_initializer=custom_init\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Qua_wsv5TyZg"
      },
      "id": "Qua_wsv5TyZg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Passing the input tensor through each model"
      ],
      "metadata": {
        "id": "0OlNGkX2T8vg"
      },
      "id": "0OlNGkX2T8vg"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_out = model_conv(x)"
      ],
      "metadata": {
        "id": "jaAMs3baTyVz"
      },
      "id": "jaAMs3baTyVz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_out = model_dense(x)"
      ],
      "metadata": {
        "id": "p7HsAH5kTyR7"
      },
      "id": "p7HsAH5kTyR7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Comparing the output shape"
      ],
      "metadata": {
        "id": "lskgJr-dUGE7"
      },
      "id": "lskgJr-dUGE7"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_out.shape == dense_out.shape"
      ],
      "metadata": {
        "id": "NcM0rcskUHSM"
      },
      "id": "NcM0rcskUHSM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "79osb_W5UF_6"
      },
      "id": "79osb_W5UF_6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Transposed Convolutions"
      ],
      "metadata": {
        "id": "7ybow-u_QMrp"
      },
      "id": "7ybow-u_QMrp"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO."
      ],
      "metadata": {
        "id": "3FimaznmO9L6"
      },
      "id": "3FimaznmO9L6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Closing Remarks"
      ],
      "metadata": {
        "id": "zF1_tbu9Prio"
      },
      "id": "zF1_tbu9Prio"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Alternatives\n",
        "* TODO.\n",
        "\n",
        "##### Extensions of task\n",
        "* TODO."
      ],
      "metadata": {
        "id": "Vgzdo0caPrav"
      },
      "id": "Vgzdo0caPrav"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Future Work"
      ],
      "metadata": {
        "id": "DjfeAE3bPrUL"
      },
      "id": "DjfeAE3bPrUL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TODO."
      ],
      "metadata": {
        "id": "5QnuD7ArPrOb"
      },
      "id": "5QnuD7ArPrOb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Credits"
      ],
      "metadata": {
        "id": "xTd8tdPVP2EC"
      },
      "id": "xTd8tdPVP2EC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment was prepared by David Siller, Kelvin Lwin et al., 2020 (link [here]).\n",
        "\n",
        "References\n",
        "* TODO.\n",
        "\n",
        "\n",
        "Helpful resources:\n",
        "* TODO."
      ],
      "metadata": {
        "id": "tvnrEkB4P175"
      },
      "id": "tvnrEkB4P175"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}