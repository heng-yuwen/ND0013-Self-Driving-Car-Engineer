{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10d3c8fa",
      "metadata": {
        "id": "10d3c8fa"
      },
      "source": [
        "# Exercise 1.7.1 — Scene Understanding\n",
        "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
        "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8342f66",
      "metadata": {
        "id": "a8342f66"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce8a817",
      "metadata": {
        "id": "3ce8a817"
      },
      "source": [
        "* Compute the Mean Intersection over Union (IoU) of the multi-class segmentation label predictions;\n",
        "* Implement a standard convolution block and compare it to the MobileNet convolution block. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a226974",
      "metadata": {
        "id": "9a226974"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36e37f8f",
      "metadata": {
        "id": "36e37f8f"
      },
      "outputs": [],
      "source": [
        "### Importing required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "040b9ad5",
      "metadata": {
        "id": "040b9ad5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from typing import List, Union, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3396c3",
      "metadata": {
        "id": "6f3396c3"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab345e3",
      "metadata": {
        "id": "9ab345e3"
      },
      "outputs": [],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b54c301",
      "metadata": {
        "id": "4b54c301"
      },
      "outputs": [],
      "source": [
        "### Setting the environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5279089e",
      "metadata": {
        "id": "5279089e"
      },
      "outputs": [],
      "source": [
        "ENV_COLAB = True                # True if running in Google Colab instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda135f9",
      "metadata": {
        "id": "eda135f9"
      },
      "outputs": [],
      "source": [
        "# Root directory\n",
        "DIR_BASE = '' if not ENV_COLAB else '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8bc08a8",
      "metadata": {
        "id": "a8bc08a8"
      },
      "outputs": [],
      "source": [
        "# Subdirectory to save output files\n",
        "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
        "# Subdirectory pointing to input data\n",
        "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c1e4cd",
      "metadata": {
        "id": "e7c1e4cd"
      },
      "outputs": [],
      "source": [
        "### Creating subdirectories (if not exists)\n",
        "os.makedirs(DIR_OUT, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e58534",
      "metadata": {
        "id": "21e58534"
      },
      "source": [
        "### 1.1. Scene Understanding "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d70ec3",
      "metadata": {
        "id": "73d70ec3"
      },
      "source": [
        "#### Background"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd081af",
      "metadata": {
        "id": "3fd081af"
      },
      "source": [
        "TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f34dd32",
      "metadata": {
        "id": "5f34dd32"
      },
      "source": [
        "#### Metrics — Intersection over Union (IoU)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab143bf",
      "metadata": {
        "id": "3ab143bf"
      },
      "source": [
        "In the [very first exercise](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-1-Choosing-Metrics/2022-07-25-Choosing-Metrics-IoU.ipynb) of this course, we covered the Intersection over Union (IoU) metric and its application to the bounding box prediction task. Now, we use the IoU metric again but this time for semantic segmentation and scene understanding.\n",
        "\n",
        "We start with the same general formula for the IoU score given by:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{IoU} &= \\frac{\\textrm{Area of Intersection}}{\\textrm{Area of Union}},\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "but now we calculate the IoU score using the following binary classification metrics:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{IoU} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "With this form of the IoU equation, all we need to do is compute the true positive ($\\mathrm{TP}$), true negative ($\\mathrm{TN}$), and the false positive ($\\mathrm{FN}$), false negative ($\\mathrm{FN}$) rates. For the image segmentation task, this boils down to the pixel-wise classification predictions. Thankfully, the algorithms we designed in [Sect. 2.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-1-Choosing-Metrics/2022-07-25-Choosing-Metrics-IoU.ipynb) of Exercise 1.1.1 hold; all we need to do is compute the pixel-wise classification metrics for each class using the same tabular approach as before. With these metrics, we evaluate the $\\mathrm{IoU}$ formula and obtain a score indicating the amount of \"overlap\" between the predicted region and the true region of each segmented object. \n",
        "\n",
        "Let's illustrate this with a simple example:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54cb770",
      "metadata": {
        "id": "d54cb770"
      },
      "source": [
        "```python\n",
        "ground_truth_labels = [\n",
        "    [0, 0, 0, 0], \n",
        "    [1, 1, 1, 1],\n",
        "    [2, 2, 2, 2], \n",
        "    [3, 3, 3, 3],\n",
        "]\n",
        "predicted_labels = [\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 3, 0, 1],\n",
        "    [2, 2, 2, 3],\n",
        "    [3, 1, 0, 0],\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69dc4fe7",
      "metadata": {
        "id": "69dc4fe7"
      },
      "source": [
        "Above we define a set of _ground-truth_ and _predicted_ labels. Each row in the matrix corresponds to a class; looking at the first row of the `ground_truth_labels` (\"$\\mathrm{A}$\") matrix, we see that class `0` should appear at all four pixel locations. Looking at the first row of `predicted_labels` (\"$\\mathrm{B}$\" matrix), we see instead that only three of the four pixel locations were given a correct prediction of class `0`. In other words, we have in the first row a $\\mathrm{TP} = 3$. Now, we need to compute for class `0` the false positive ($\\mathrm{FP}$) rate. To do this, we examine the _other_ pixel locations (i.e., other rows of the `predicted_labels` matrix), and add up any occurrences of class label `0` where the corresponding entries in `ground_truth_labels` do not match. Since class label `0` was predicted _incorrectly_ at pixel locations $\\mathrm{B}_{2, 3}$, $\\mathrm{B}_{3, 3}$, and $\\mathrm{B}_{4, 4}$, we have a $\\mathrm{FP} = 3$. Now let's complete the calculations for the two other metrics: true negative ($\\mathrm{TN}$) and false negative ($\\mathrm{FN}$). The $\\mathrm{TN}$ value for this problem is easy to compute, since we assume all predictions here were valid (i.e., that we expected a class label to be predicted for every pixel in `predicted_labels`). That means our $\\mathrm{TN} = 0$. Lastly, our $\\mathrm{FN}$ rate is computed as the number of _incorrect_ predictions for class `0`. Looking at the first row of the `predicted_labels` matrix (i.e., the \"predictions\" for class `0`), we count the number of label predictions that are _not_ equal to class `0` to get our false negative rate. With _one_ incorrect class `0` prediction at the first index $\\mathrm{B}_{1, 1} = $ `1`, we have therefore a $\\mathrm{FN} = 1$. \n",
        "\n",
        "With these four classification metrics out of the way, we can obtain the $\\mathrm{IoU}$ score for class `0` as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{IoU}_{0} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}} = \\frac{3}{3 + 3 + 1 + 0} = \\frac{3}{7} \\approx 0.4286.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Now that we have the $\\mathrm{IoU}$ score for class `0` computed, we repeat the process for the other three rows (classes) in `predicted_labels` to obtain each classes' respective $\\mathrm{IoU}$ score. Once we have completed the calculations of all four classes, we can take the average to obtain the $\\mathrm{IoU}_{\\textrm{mean}}$, as simply:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{IoU}_{\\textrm{mean}} &= \\frac{1}{n}\\sum_{i=0}^{n} \\mathrm{IoU}_{i},\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "which is nothing but the sum of the per-class $\\mathrm{IoU}$ scores divided by the total number of classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ke35mrjgWjFL",
      "metadata": {
        "id": "ke35mrjgWjFL"
      },
      "source": [
        "### 1.2. Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e37fc7",
      "metadata": {
        "id": "55e37fc7"
      },
      "source": [
        "[Convolution](https://en.wikipedia.org/wiki/Convolution) is a measure of overlap between two functions as one slides over the other. Mathematically, it is a sum of products given by the following:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\left(f * g\\right)\\left(t\\right) := \\int_{-\\infty}^{\\infty} f\\left(\\tau\\right)\\cdot g\\left(t-\\tau\\right)d\\tau.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "A [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layers) in a neural network performs the convolution operation by applying a filter over the input tensor. After the same filter is repeatedly applied to the input, a feature map is created which shows the positions and intensity of a detected feature in an input. \n",
        "\n",
        "Convolutional layers are extremely useful in neural networks, but they often come with a high computational cost due to the number of parameters required for each input (and each channel of the input). In this section we will look at the basics of regular convolution and review the number of parameters required. Then, we will introduce several alternative convolution methods — namely, depth-wise, depth-wise separable and point-wise convolution, which help reduce this computational cost. These alternatives are especially useful for networks intended to be run on mobile devices and embedded hardware where computational resources are limited."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B3WcU3QlWi5j",
      "metadata": {
        "id": "B3WcU3QlWi5j"
      },
      "source": [
        "#### Regular Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_hk_pcnXFbL",
      "metadata": {
        "id": "H_hk_pcnXFbL"
      },
      "source": [
        "In a typical [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layers), we have a set of $\\mathrm{N}$ kernels, each with a size of $\\mathrm{D}_{k} * \\mathrm{D}_{k}$. Each of these kernels convolves (\"slides over\") the entire input, which is a $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$ sized feature map (a tensor). When considering the computational cost of a convolution operation, we must consider the number of _parameters_ required for each convolutional layer. Generally, models with more convolutional layers have more parameters, and therefore require more computational resources to use. While this can lead to higher accuracy in image processing tasks, special attention needs to be paid to the computational cost of convolutional networks when utilising low-end hardware, such as with in-vehicle embedded devices which will be powering these type of networks with real-time inference demands.\n",
        "\n",
        "To understand the computational cost of a regular convolutional layer, we have the following cost:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N} * \\mathrm{D}_{k} * \\mathrm{D}_{k},\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $\\mathrm{D}_{g} * \\mathrm{D}_{g}$ is the size of the output feature map. A regular convolution takes in a $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$ input feature map and returns a $\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{N}$ feature map as output.\n",
        "\n",
        "This is illustrated below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008yIebXacAE",
      "metadata": {
        "id": "008yIebXacAE"
      },
      "source": [
        "<img src=\"figures/2023-01-26-Figure-1-Standard-Convolution-Filters.png\" alt=\"Figure 1. Filters in a regular convolutional layer.\">\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\textrm{Figure 1. Filters in a regular convolutional layer (credit: Howard et al., 2017).}\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_C3E7_bbOyD",
      "metadata": {
        "id": "H_C3E7_bbOyD"
      },
      "source": [
        "#### Depth-wise Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pXNWwOkWbPSL",
      "metadata": {
        "id": "pXNWwOkWbPSL"
      },
      "source": [
        "[Depth-wise convolution](https://tvm.d2l.ai/chapter_common_operators/depthwise_conv.html) and depth-wise separable convolution are two atypical convolution operations that have less parameters and therefore require less computational power to compute. Depth-wise convolutions are used in the [MobileNets](https://arxiv.org/abs/1704.04861) [1] architecture designed for mobile and embedded applications.\n",
        "\n",
        "Depth-wise convolution acts on each input channel separately with a different kernel for each. With a number of input channels $\\mathrm{M}$ we have $\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}$ kernels. Since depth-wise convolution only acts on a single input channel at a time, the kernel depth $\\mathrm{N}$ is set equal to $1$. Therefore, the computational cost of the depth-wise convolution is:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "To produce the same effect with regular convolution, each channel of the input requires its own kernel. To compute the convolution, each channel is selected individually, and all elements in the kernel are set to zero except for those corresponding to the respective input channel. The final output of the stacked convolutions is one $\\mathrm{M}$-channel output feature map. As shown above, depth-wise convolution reduces the number of parameters required by a factor $\\mathrm{N}$, i.e., the number of filters required _per input channel_. For a three-channel input, we require _three times less_ number of parameters with the depth-wise convolution approach.   \n",
        "\n",
        "This is illustrated below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d857dff0",
      "metadata": {
        "id": "d857dff0"
      },
      "source": [
        "<img src=\"figures/2023-01-26-Figure-2-Depthwise-Convolution-Filters.png\" alt=\"Figure 2. Filters in a depth-wise convolutional layer.\">\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\textrm{Figure 2. Filters in a depth-wise convolutional layer (credit: Howard et al., 2017).}\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2c2bbf",
      "metadata": {
        "id": "0a2c2bbf"
      },
      "source": [
        "In order to understand depth-wise separable convolution, we first introduce the point-wise convolution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a30c37",
      "metadata": {
        "id": "40a30c37"
      },
      "source": [
        "#### Point-wise Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e5837bf",
      "metadata": {
        "id": "0e5837bf"
      },
      "source": [
        "Point-wise convolution is a form of convolution that applies a $1\\times 1$ kernel across an input. Unlike the depth-wise convolution, the $1\\times 1$ kernel used here has a depth equal to the number of channels in the input. The computational complexity of the point-wise convolution is similar to the regular convolution, but instead the filter size $\\mathrm{D}_{k} * \\mathrm{D}_{k}$ is equal to $1 \\times 1$. Therefore, we have the following number of parameters:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This is illustrated below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32f85ba",
      "metadata": {
        "id": "d32f85ba"
      },
      "source": [
        "<img src=\"figures/2023-01-26-Figure-3-Pointwise-Convolution-Filters.png\" alt=\"Figure 3. Filters in a point-wise convolutional layer.\">\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\textrm{Figure 3. Filters in a point-wise convolutional layer (credit: Howard et al., 2017).}\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a13941d",
      "metadata": {
        "id": "7a13941d"
      },
      "source": [
        "#### Depth-wise Separable Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "942cb534",
      "metadata": {
        "id": "942cb534"
      },
      "source": [
        "Point-wise convolution is used in conjunction with depth-wise convolution in order to perform depth-wise separable convolution. Here, depth-wise separable convolution borrows the idea that the depth and the spatial dimension of a filter can be separated, as is the case with e.g., the [Sobel](https://en.wikipedia.org/wiki/Sobel_operator) filter for edge detection. The depth-wise separable convolution separates a kernel into two independent kernels, each of which performs two convolutions: the depth-wise and the point-wise convolution. Thus, the total computational cost for the depth-wise separable convolution is:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right) + \\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right),\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "which results in a $\\frac{1}{\\mathrm{N}} + \\frac{1}{\\mathrm{D}_{k} * \\mathrm{D}_{k}}$ reduction in total computation, which can be observed in the following:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right) + \\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right)}{\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N} * \\mathrm{D}_{k} * \\mathrm{D}_{k}} &= \\frac{\\mathrm{D}_{k}^{2} + \\mathrm{N}}{\\mathrm{D}_{k}^{2} * \\mathrm{N}} = \\frac{1}{\\mathrm{N}} + \\frac{1}{\\mathrm{D}_{k}^{2}}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "When $\\mathrm{N}$ is selected to be large enough, depth-wise separable convolution networks can be immensely more computationally efficient. For example, the MobileNet architecture uses a $3\\times 3$ kernel, which results in $\\sim 9\\mathrm{x}$ efficiency improvement over regular convolution layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff911d9",
      "metadata": {
        "id": "0ff911d9"
      },
      "source": [
        "#### Width Multiplier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b744d977",
      "metadata": {
        "id": "b744d977"
      },
      "source": [
        "In addition to utilising alternative convolution methods, scaling the number of input and output channels proportional to a _width multiplier_ is often performed. The width multiplier $\\alpha$ is a hyperparameter set to a value in the range $\\alpha \\in \\left[0, 1\\right]$. This results in a computational cost:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\left(\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\alpha\\mathrm{N}\\right).\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d363e25",
      "metadata": {
        "id": "4d363e25"
      },
      "source": [
        "#### Resolution Multiplier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9984092",
      "metadata": {
        "id": "e9984092"
      },
      "source": [
        "Similar to the width multiplier, a _resolution multiplier_ is used to scale the size of the input feature map. The resolution multiplier $\\rho$ is a hyperparameter set to a value in the range $\\rho \\in \\left[0, 1\\right]$. This results in a computational cost:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\mathrm{M} * \\mathrm{N}\\right).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Combining the width and resolution multipliers results in a scaled computational cost of:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\alpha\\mathrm{N}\\right).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "In the MobileNets architecture, for example, these values of $\\alpha$ and $\\rho$ are selected w.r.t. the speed versus accuracy versus size trade-off. In the paper by original author Howard et al., 2017, the authors found that the resolution multiplier has the effect of reducing computational cost by $\\rho^{2}$, whereas the width multiplier has the effect of reducing computational cost and number of parameters quadratically by roughly $\\alpha^{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cfaa112",
      "metadata": {
        "id": "7cfaa112"
      },
      "source": [
        "### 1.3. Convolutional Network Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d9769c0",
      "metadata": {
        "id": "4d9769c0"
      },
      "source": [
        "#### FCN-8 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066cc7a7",
      "metadata": {
        "id": "066cc7a7"
      },
      "source": [
        "The [FCN-8]() architecture by Long et al., 2014 [2] is an architecture that uses $1\\times 1$ convolutional layers to replace the fully-connecte—d layers of a standard neural network. As a result, the FCN-8 architecture is able to preserve spatial information of the input tensor and perform the down-sampling and feature extraction routines of a convolutional network.\n",
        "\n",
        "Fully-Convolutional Network (FCN) architectures have two primary components — an _encoder_ and a _decoder_. The encoder extracts features from an image using a series of sliding window convolution operations. The decoder in an FCN is used to up-scale the down-sampled intermediate feature maps generated from the encoder to a higher resolution — usually matching the original input dimensions. In the FCN-8 architecture, the encoder block is a set of $1\\times 1$ convolution layers. The decoder block of the FCN-8 is a set of transposed convolution layers which upsample the feature maps to the size of the original input. This process is usually referred to as \"reverse convolution\" or deconvolution since its effect is essentially reversing (with some loss) the downsampling of the input. \n",
        "\n",
        "In order to preserve fine-grained segmentation maps through the network to the decoder block, a set of skip _connections_ are used. Essentially, these \"connections\" between non-adjacent layers of differing resolutions help retain information from the original input by combining the output feature maps of each respective layer using an element-wise addition operation. As a result, the FCN-8 with skip connections is able to use information from multiple resolutions to make more precise segmentation decisions. Skip connections, along with the other advancements from the FCN-8 architecture, have proven successful in empirical studies between the FCN-8 and its \"sister\" networks — the FCN-16 and FCN-32. For more information on Fully-Convolutional Networks, see the [notebook from the previous lesson](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/1.7/1-Computer-Vision/Exercises/1-6-1-Fully-Convolutional-Networks/2023-01-23-Fully-Convolutional-Networks.ipynb).  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81567d69",
      "metadata": {
        "id": "81567d69"
      },
      "source": [
        "#### MobileNets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a4bcc5",
      "metadata": {
        "id": "38a4bcc5"
      },
      "source": [
        "The [MobileNets](https://arxiv.org/abs/1704.04861) architecture by Howard et al., 2017 [1] is an architecture that uses depth-wise separable convolutions to build light-weight deep neural networks. The MobileNets architecture, as the name suggests, is designed to run object detection and classification tasks efficiently (i.e., with high FPS and low memory footprint) on mobile and embedded devices. The MobileNets architecture achieves this in a three-part approach:\n",
        "1. **Depth-wise separable convolutions** — Perform a depth-wise convolution followed by a $1\\times 1$ convolution (instead of a standard convolution). The $1\\times 1$ convolution is called a point-wise convolution if it follows after a depth-wise convolution;\n",
        "2. **Width multipliers** — Reduces the size of the input / output channels using a scaling factor set to a value between $0.0$ and $1.0$;\n",
        "3. **Resolution multipliers** — Reduces the size of the original input using a scaling factor set to a value between $0.0$ and $1.0$.\n",
        "\n",
        "These three techniques reduce the cummulative number of parameters in the network and therefore the amount of computation required. The downside to models exploiting the parameter reduction approach is that accuracy is often the trade-off. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XGkWAi-UUik0",
      "metadata": {
        "id": "XGkWAi-UUik0"
      },
      "source": [
        "#### Single Shot Detector (SSD)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VPNl0saRUiVs",
      "metadata": {
        "id": "VPNl0saRUiVs"
      },
      "source": [
        "Many of the earlier deep neural network architectures involved networks with more than one training phase; the [Faster-RCNN](https://arxiv.org/abs/1506.01497) for example, first trains a Region Proposal Network (RPN) which is then merged with a pre-trained classification sub-network. The [Single Shot Detector](https://arxiv.org/abs/1512.02325) (SSD) by Liu et al., 2015 [3] combines these two sub-networks into a single-pass network that predicts bounding box locations and classifies the corresponding object classes. The major difference with single-shot networks is that they can be trained end-to-end, whereas architectures with multiple sub-networks, such as the Faster-RCNN, must train each module separately. The following is an outline of the original SSD architecture proposed by Liu et al., 2015 [3]:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xRRThpEkUiId",
      "metadata": {
        "id": "xRRThpEkUiId"
      },
      "source": [
        "<img src=\"figures/2023-01-26-Figure-4-Single-Shot-Detector-Network-Architecture.png\" alt=\"Figure 4. Architecture of the Single Shot Detector (SSD) proposed by Liu et al., 2015.\">\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\textrm{Figure 4. Architecture of the Single Shot Detector (SSD) proposed by Liu et al., 2015.}\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7uqbGoWrUh72",
      "metadata": {
        "id": "7uqbGoWrUh72"
      },
      "source": [
        "In the above architecture, we note the use of the VGG-16 pre-trained convolutional base. In this notebook, we will instead be using the MobileNet pre-trained base from Howard et al., 2017 [1]."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tK3_wxGcYeKm",
      "metadata": {
        "id": "tK3_wxGcYeKm"
      },
      "source": [
        "##### Bounding box detection with SSD"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5qCjGDPsYcEN",
      "metadata": {
        "id": "5qCjGDPsYcEN"
      },
      "source": [
        "SSD operates on feature maps to predict bounding box locations. Recall a feature map of $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$. For each feature map location, $k$ bounding boxes are predicted. Each bounding box carries with it the following information:\n",
        "* $\\left(\\mathrm{c}x, \\mathrm{c}y, w, h\\right)$ — Four bounding box corner offset locations;\n",
        "* $C = \\left(c_{1}, c_{2},\\ldots, c_{p}\\right)$ — class probabilities.\n",
        "\n",
        "The SSD does not predict the _shape_ of the box but rather the location of where the box is in the image. The $k$ bounding boxes each have a pre-determined shape (i.e., the anchors). This is illustrated in the figure below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf91e2b",
      "metadata": {
        "id": "eaf91e2b"
      },
      "source": [
        "<img src=\"figures/2023-01-26-Figure-5-Bounding-Box-Internal-Representation-with-SSD.png\" alt=\"Figure 5. Internal representation of bounding boxes using anchors with the Single Shot Detector (SSD) network.\">\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\textrm{Figure 5. Internal representation of bounding boxes using anchors in Single Shot Detector (SSD) network.}\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66029399",
      "metadata": {
        "id": "66029399"
      },
      "source": [
        "The anchor boxes used in the SSD have coordinates that are manually configured prior to training. Shown in Figure 5(c) is a set of $k = 4$ anchor boxes of varying size used to isolate an object for detection.\n",
        "\n",
        "In order to filter nonsensical bounding boxes, we use a loss function. For the final set of $N$ matched boxes, we compute the loss as:\n",
        "$$\n",
        "\\begin{align}\n",
        "L &= \\frac{1}{N}\\left(L_{\\textrm{class}} + L_{\\textrm{box}}\\right),\n",
        "\\end{align}\n",
        "$$\n",
        "where $L_{\\textrm{class}}$ is a softmax loss for classification, and $L_{\\textrm{box}}$ is an L1 smooth loss representing the error of the matched boxes to the ground-truth boxes. Note that L1 smooth loss is a modification of the standard L1 loss which is more robust to outliers. Also note that when $N = 0$, the loss is set to $0.0$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83266d21",
      "metadata": {
        "id": "83266d21"
      },
      "source": [
        "##### In Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dae7a50",
      "metadata": {
        "id": "0dae7a50"
      },
      "source": [
        "* A pre-trained convolutional base is used (e.g., VGG-16 or MobileNet);\n",
        "* The base model is extended with several convolutional blocks;\n",
        "* Each feature map is used to predict bounding boxes, and therefore diversity in feature map size allows for object detection at different resolutions;\n",
        "* Boxes are filtered by the Intersection over Union (IoU) metric and with hard negative mining (looking for difficult-to-detect examples);\n",
        "* The loss functions are softmax for classification and smooth L1 for detection;\n",
        "* The entire SSD network can be trained end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b337dc",
      "metadata": {
        "id": "19b337dc"
      },
      "source": [
        "## 2. Programming Task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d76f553",
      "metadata": {
        "id": "9d76f553"
      },
      "source": [
        "### 2.1. Intersection over Union (IoU)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0509c31",
      "metadata": {
        "id": "e0509c31"
      },
      "source": [
        "Here we use the TensorFlow [`tf.keras.metrics.MeanIoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU) function to compute the mean Intersection over Union (IoU) across all classes $i=0,\\ldots, n$.\n",
        "\n",
        "In order to use the metric as a standalone function, we have to first initialise the respective [`tf.keras.metrics.Metric`](https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/metrics/Metric) subclass instance (i.e., `MeanIoU`), then perform a single \"state update\" using the [`update_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#update_state) class method. As arguments to this function, we pass in the `y_true` and `y_pred` tensors that we wish to evaluate. Optionally, we can provide a `sample_weight` scalar value or vector of rank equal to `y_true`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1588d58",
      "metadata": {
        "id": "f1588d58"
      },
      "outputs": [],
      "source": [
        "### Defining the number of distinct class labels (i.e., classes)\n",
        "N_CLASSES = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89c5313",
      "metadata": {
        "id": "d89c5313"
      },
      "outputs": [],
      "source": [
        "### Initialising the `tf.keras.metrics.Metric` instance\n",
        "iou_mean = tf.keras.metrics.MeanIoU(\n",
        "    num_classes=N_CLASSES,\n",
        "    name='Mean IoU for multi-class object segmentation data',\n",
        "    dtype=tf.dtypes.float32,\n",
        "    ### Additional arguments for TF2.10+ API:\n",
        "    #ignore_class=None,\n",
        "    #sparse_y_true=True,    # `True` if class labels are integers, `False` if floating-point\n",
        "    #sparse_y_pred=True,\n",
        "    #axis=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aEPRhQllMLHG",
      "metadata": {
        "id": "aEPRhQllMLHG"
      },
      "source": [
        "#### Testing the `MeanIoU` metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JucYiLeoMMG1",
      "metadata": {
        "id": "JucYiLeoMMG1"
      },
      "outputs": [],
      "source": [
        "### Defining our prediction and ground-truth sets\n",
        "ground_truth_labels = [\n",
        "    [0, 0, 0, 0], \n",
        "    [1, 1, 1, 1],\n",
        "    [2, 2, 2, 2], \n",
        "    [3, 3, 3, 3],\n",
        "]\n",
        "predicted_labels = [\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 3, 0, 1],\n",
        "    [2, 2, 2, 3],\n",
        "    [3, 1, 0, 0],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54WZKwK2Mgb9",
      "metadata": {
        "id": "54WZKwK2Mgb9"
      },
      "outputs": [],
      "source": [
        "### Converting the matrices to n-rank tensors\n",
        "y_true = tf.convert_to_tensor(\n",
        "    np.array(\n",
        "        ground_truth_labels).reshape(1, -1, len(ground_truth_labels), N_CLASSES\n",
        "    ),\n",
        "    dtype=tf.float32\n",
        ")\n",
        "y_pred = tf.convert_to_tensor(\n",
        "    np.array(\n",
        "        predicted_labels).reshape(1, -1, len(predicted_labels), N_CLASSES\n",
        "    ),\n",
        "    dtype=tf.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t8Fwb-ZxNsaA",
      "metadata": {
        "id": "t8Fwb-ZxNsaA"
      },
      "outputs": [],
      "source": [
        "### Computing the mean IoU\n",
        "iou_mean.update_state(\n",
        "    y_true=y_true,\n",
        "    y_pred=y_pred\n",
        ")\n",
        "iou_mean.result().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SFL3s0eqMK9W",
      "metadata": {
        "id": "SFL3s0eqMK9W"
      },
      "source": [
        "As shown above, we obtain a mean IoU score for the set of predictions of $\\mathrm{IoU}_{\\textrm{mean}} \\ \\approx 0.420$, which matches our expected value for this test set.\n",
        "\n",
        "Now, we repeat the Mean IoU calculation using a second test set of predictions. Note that [`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) instances are stateful by default, meaning that each call to `update_state()` computes the Mean IoU for the input _mini-batch of predictions / ground-truth labels_. In other words, the default behaviour for this `Metric` instance is to accumulate the Mean IoU score across calls to `update_state()`. Since we are computing the Mean IoU score of the full batch (i.e., `batch_size=1`) manually, we must use the [`reset_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#reset_state) method before performing the IoU calculation with [`update_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#update_state)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V5vt1MGZSpbQ",
      "metadata": {
        "id": "V5vt1MGZSpbQ"
      },
      "outputs": [],
      "source": [
        "### Defining our new prediction set (assuming same `ground_truth_labels`)\n",
        "predicted_labels = [\n",
        "    [0, 0, 0, 0],\n",
        "    [1, 0, 0, 1],\n",
        "    [1, 2, 2, 1],\n",
        "    [3, 3, 0, 3],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KOVETdBRU0Iu",
      "metadata": {
        "id": "KOVETdBRU0Iu"
      },
      "outputs": [],
      "source": [
        "### Converting the matrix to an n-rank tensor\n",
        "y_pred = tf.convert_to_tensor(\n",
        "    np.array(\n",
        "        predicted_labels).reshape(1, -1, len(predicted_labels), N_CLASSES\n",
        "    ),\n",
        "    dtype=tf.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RmADU5HjSpTZ",
      "metadata": {
        "id": "RmADU5HjSpTZ"
      },
      "outputs": [],
      "source": [
        "### Resetting the state of the `MeanIoU` instance\n",
        "# i.e., current cumulative mean IoU becomes `0.0`\n",
        "iou_mean.reset_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QauTkcrRSpKm",
      "metadata": {
        "id": "QauTkcrRSpKm"
      },
      "outputs": [],
      "source": [
        "### Computing the new mean IoU\n",
        "iou_mean.update_state(\n",
        "    y_true=y_true,\n",
        "    y_pred=y_pred\n",
        ")\n",
        "iou_mean.result().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M9kVfx95VQ48",
      "metadata": {
        "id": "M9kVfx95VQ48"
      },
      "source": [
        "As shown above, we obtain a new mean IoU score for this second test of predictions of $\\mathrm{IoU}_{\\textrm{mean}} \\ \\approx 0.539$, which matches our expected value for this second test set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c0ea8b",
      "metadata": {
        "id": "b0c0ea8b"
      },
      "source": [
        "### 2.2. Separable Depthwise Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00310c4d",
      "metadata": {
        "id": "00310c4d"
      },
      "source": [
        "NOTE: the code provided here has been migrated to the TensorFlow 2.x API. Some functionality may differ from the original implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b7e989",
      "metadata": {
        "id": "d6b7e989"
      },
      "source": [
        "Here we implement the a MobileNets depth-wise separable convolution block using the following [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) components:\n",
        "* [`tf.nn.depthwise_conv2d`](https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d);\n",
        "* [`tf.keras.layers.BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization);\n",
        "* [`tf.keras.layers.ReLU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU);\n",
        "* [`tf.keras.layers.Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n",
        "\n",
        "\n",
        "We will then compare the number of parameters of the depth-wise separable convolution block to a regular convolution block, which is formed using the following [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) components:\n",
        "* [`tf.keras.layers.Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layer;\n",
        "* [`tf.keras.layers.BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e89e05aa",
      "metadata": {
        "id": "e89e05aa"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce9aba00",
      "metadata": {
        "id": "ce9aba00"
      },
      "outputs": [],
      "source": [
        "def vanilla_conv_block(\n",
        "        x: Union[np.ndarray, tf.Tensor], \n",
        "        kernel_size: Union[Tuple[int], List[int]],\n",
        "        output_channels: int\n",
        ") -> tf.keras.models.Sequential:\n",
        "    \"\"\"Implements a vanilla (regular) convolution block.\n",
        "    \n",
        "    A convolution block here is defined as the following:\n",
        "        Vanilla Conv -> Batch Norm -> ReLU,\n",
        "    where 'Vanilla Conv' corresponds to the `Conv2D` layer\n",
        "    provided in the TensorFlow 2.x API.\n",
        "    \n",
        "    :param x: Input tensor used to build the convolutional block,\n",
        "        i.e., input shape of `x` is provided for delayed-build pattern.\n",
        "    :param kernel_size: Kernel size to assign the convolutional layer.\n",
        "    :param output_channels: Depth of the output tensor.\n",
        "    :returns: The 'vanilla' convolutional block,\n",
        "        buit for input tensors of shape given by `x`.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### Build the convolutional block\n",
        "    conv = tf.keras.models.Sequential() \n",
        "    conv.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=output_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=(2, 2),\n",
        "            padding='SAME'\n",
        "        )\n",
        "    )\n",
        "    conv.add(\n",
        "        tf.keras.layers.BatchNormalization()\n",
        "    )\n",
        "    conv.add(\n",
        "        tf.keras.layers.ReLU()\n",
        "    )\n",
        "    ### Build the block by setting expected input shape to shape of tensor `x`\n",
        "    conv.build(x.shape)\n",
        "    ### Return the built convolutional block\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8849feff",
      "metadata": {
        "id": "8849feff"
      },
      "outputs": [],
      "source": [
        "def mobilenet_conv_block(\n",
        "        x: Union[np.ndarray, tf.Tensor], \n",
        "        kernel_size: Union[Tuple[int], List[int]], \n",
        "        output_channels: int\n",
        ") -> tf.keras.models.Sequential:\n",
        "    \"\"\"Implements a depth-wise separable convolution block (Howard, 2017).\n",
        "\n",
        "    A depth-wise separable convolution block is defined as the following:\n",
        "        Depth-wise -> Batch Norm -> ReLU -> Point-wise -> Batch Norm -> ReLU,\n",
        "    where the 'Point-wise' is implemented using the `Conv2D` layer provided\n",
        "    in the TensorFlow 2.x API.\n",
        "\n",
        "    :param x: Input tensor used to build the MobileNet convolutional block,\n",
        "        i.e., input shape of `x` is provided for delayed-build pattern.\n",
        "    :param kernel_size: Kernel size to assign the convolutional layers.\n",
        "    :param output_channels: Depth of the output tensor\n",
        "    :returns: The 'MobileNet' convolutional block,\n",
        "        built for input tensors of shape given by `x`.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### Build the MobileNet convolutional block\n",
        "    conv = tf.keras.models.Sequential()\n",
        "    conv.add(\n",
        "        # Should have 3x3 kernel\n",
        "        tf.keras.layers.DepthwiseConv2D(\n",
        "            kernel_size=KERNEL_SIZE,\n",
        "            strides=(1, 1),\n",
        "            padding='SAME'\n",
        "        )\n",
        "    )\n",
        "    conv.add(\n",
        "        tf.keras.layers.BatchNormalization()\n",
        "    )\n",
        "    conv.add(\n",
        "        tf.keras.layers.ReLU()\n",
        "    )\n",
        "    # Should have 1x1 kernel\n",
        "    conv.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=1,\n",
        "            kernel_size=(1, 1),\n",
        "            strides=(1, 1),\n",
        "            padding='SAME',\n",
        "        )\n",
        "    )\n",
        "    conv.add(\n",
        "        tf.keras.layers.BatchNormalization()\n",
        "    )\n",
        "    conv.add(\n",
        "        tf.keras.layers.ReLU()\n",
        "    )\n",
        "    ### Build the block by setting expected input shape to shape of tensor `x`\n",
        "    conv.build(x.shape)\n",
        "    ### Return the built MobileNet convolutional block\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ceae4f5",
      "metadata": {
        "id": "1ceae4f5"
      },
      "source": [
        "#### Testing the MobileNets convolutional block "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04345315",
      "metadata": {
        "id": "04345315"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2JH9neiom9",
      "metadata": {
        "id": "da2JH9neiom9"
      },
      "outputs": [],
      "source": [
        "### Setting the input parameters\n",
        "INPUT_CHANNELS = 32\n",
        "OUTPUT_CHANNELS = 512\n",
        "KERNEL_SIZE = 3\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GNrrlU1pioiv",
      "metadata": {
        "id": "GNrrlU1pioiv"
      },
      "outputs": [],
      "source": [
        "### Creating the input tensor\n",
        "x = tf.constant(\n",
        "    np.random.rand(1, IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS),\n",
        "    dtype=tf.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U3z6To3Ii7dl",
      "metadata": {
        "id": "U3z6To3Ii7dl"
      },
      "outputs": [],
      "source": [
        "### Testing the 'vanilla' convolutional block\n",
        "conv_vanilla = vanilla_conv_block(\n",
        "    x=x,\n",
        "    kernel_size=KERNEL_SIZE,\n",
        "    output_channels=OUTPUT_CHANNELS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oeycDPLPjwCo",
      "metadata": {
        "id": "oeycDPLPjwCo"
      },
      "outputs": [],
      "source": [
        "### Printing the 'vanilla' convolutional block summary (no. parameters)\n",
        "conv_vanilla.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2jL8ptgMr_FO",
      "metadata": {
        "id": "2jL8ptgMr_FO"
      },
      "outputs": [],
      "source": [
        "### Compute the filter of the Depth-wise Conv2D\n",
        "# Assuming the 'BHWC' format\n",
        "input_channel_dim = x.get_shape().as_list()[-1]\n",
        "tf.Variable(tf.random.truncated_normal(\n",
        "    shape=(KERNEL_SIZE, KERNEL_SIZE, input_channel_dim, 1)\n",
        ")).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t9Sz_AS3kK0E",
      "metadata": {
        "id": "t9Sz_AS3kK0E"
      },
      "outputs": [],
      "source": [
        "### Testing the 'MobileNet' depth-wise separable convolutional block\n",
        "conv_mobilenet = mobilenet_conv_block(\n",
        "    x=x,\n",
        "    kernel_size=KERNEL_SIZE,    # Should be (3, 3) for MobileNet\n",
        "    output_channels=OUTPUT_CHANNELS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wvhgq2sdkKnK",
      "metadata": {
        "id": "wvhgq2sdkKnK"
      },
      "outputs": [],
      "source": [
        "conv_mobilenet.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4RD5pZY87Ry8",
      "metadata": {
        "id": "4RD5pZY87Ry8"
      },
      "source": [
        "##### Comparing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CFMj6qce7iYN",
      "metadata": {
        "id": "CFMj6qce7iYN"
      },
      "outputs": [],
      "source": [
        "cv_params = conv_vanilla.count_params()\n",
        "cm_params = conv_mobilenet.count_params()\n",
        "diff_percent = (cv_params - cm_params) / cv_params * 100\n",
        "print(f\"Total parameters for 'vanilla' ConvNet block: {cv_params}\")\n",
        "print(f\"Total parameters for 'MobileNet' ConvNet block: {cm_params}\")\n",
        "print(f\"Reduction in parameters with 'MobileNet' ConvNet block: {diff_percent:.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dWQGgYPb7Rsr",
      "metadata": {
        "id": "dWQGgYPb7Rsr"
      },
      "source": [
        "With the MobileNet's depth-wise separable convolutional block, we have a $99.677\\%$ reduction in the total number of parameters in each block when compared to the standard \"vanilla\" convolutional block (i.e., the `Conv2D -> Batch Norm -> ReLU` block).\n",
        "\n",
        "If minimising the total number of parameters is your goal, then the MobileNet architecture is sure to suit your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3b6185",
      "metadata": {
        "id": "5e3b6185"
      },
      "source": [
        "### 2.3. Object Detection Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a4141f",
      "metadata": {
        "id": "64a4141f"
      },
      "source": [
        "NOTE: the code provided here has been migrated to the TensorFlow 2.x API based on the TensorFlow 2 Object Detection API [`inference_tf2_colab.ipynb`](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb) tutorial. Some functionality may differ from the original implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a38103",
      "metadata": {
        "id": "65a38103"
      },
      "source": [
        "In this section we will detect objects using an object detection model and its pre-trained weights made available at the [TensorFlow Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). All models and weights provided by the TensorFlow team have been pre-trained on the [COCO 2017](http://cocodataset.org/) dataset. The TensorFlow team even provides documentation regarding the use of the [Zoo models on mobile devices](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md).\n",
        "\n",
        "Here we will be experimenting with the following set of pre-trained models:\n",
        "* [SSD MobileNet V1 FPN 640x640](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz);\n",
        "* [RFCN ResNet101](http://download.tensorflow.org/models/object_detection/rfcn_resnet101_coco_11_06_2017.tar.gz) — DEPRECATED (archive [here](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/rfcn-resnet101-coco-tf));\n",
        "* [Faster R-CNN Inception ResNet V2 640x640](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017.tar.gz).\n",
        "\n",
        "Note that the above links are for the 11.6.17 versions and are intended for use with TensorFlow v1 models. Since we are instead going to be using the TensorFlow 2.x API, it's best to download these files instead from the [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
        "\n",
        "Each of these three models produces a set of regressed bounding box coordinates as outputs. We will define here a set of utility / helper functions which:\n",
        "1. Filter detected bounding boxes;\n",
        "2. Convert the detected bounding boxes to the original image coordinates;\n",
        "3. Render the converted bounding boxes onto the original image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "403ccb58",
      "metadata": {
        "id": "403ccb58"
      },
      "source": [
        "#### Setting up the TensorFlow Object Detection API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3784e99a",
      "metadata": {
        "id": "3784e99a"
      },
      "outputs": [],
      "source": [
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e69b43",
      "metadata": {
        "id": "86e69b43"
      },
      "outputs": [],
      "source": [
        "### Clone the `tensorflow/models` repository\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "    while \"models\" in pathlib.Path.cwd().parts:\n",
        "        os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "    !git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Patching the TF2.x install\n",
        "%%capture\n",
        "!pip install -U --pre tensorflow==\"2.2.0\""
      ],
      "metadata": {
        "id": "IKn0wGOYoPMz"
      },
      "id": "IKn0wGOYoPMz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02a1690",
      "metadata": {
        "id": "d02a1690"
      },
      "outputs": [],
      "source": [
        "### Install the TF Object Detection API\n",
        "%%bash\n",
        "cd models/research\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "817e5f14",
      "metadata": {
        "id": "817e5f14"
      },
      "outputs": [],
      "source": [
        "### Import required Python modules\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageColor, ImageDraw, ImageFont\n",
        "from scipy.stats import norm\n",
        "import scipy.misc\n",
        "from six import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852bd476",
      "metadata": {
        "id": "852bd476"
      },
      "outputs": [],
      "source": [
        "### Import TensorFlow modules\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from tensorflow import keras\n",
        "from object_detection.builders import model_builder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2ffd80",
      "metadata": {
        "id": "db2ffd80"
      },
      "source": [
        "#### Defining the utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb39bde6",
      "metadata": {
        "id": "cb39bde6"
      },
      "outputs": [],
      "source": [
        "### From TensorFlow's `inference_tf2_colab.ipynb`\n",
        "# Credit: https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8144df",
      "metadata": {
        "id": "4c8144df"
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(\n",
        "        path: str\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Loads an image from file into a NumPy `ndarray`.\n",
        "\n",
        "    The image located at `path` is read in using the TensorFlow\n",
        "    file reader. The image (assumed to be RGB) is converted from\n",
        "    raw bytes into a NumPy array of unsigned 8-bit values.\n",
        "    The corresponding image is then returned to be used in\n",
        "    the model data ingestion stage.\n",
        "\n",
        "    :param path: Absolute file path to the image to load.\n",
        "    :returns: Image loaded into a NumPy array with shape\n",
        "        (img_height, img_width, 3).\n",
        "    \"\"\"\n",
        "    \n",
        "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(img_data))\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)\n",
        "    ).astype(np.uint8)\n",
        "\n",
        "\n",
        "def get_keypoint_tuples(\n",
        "        eval_config\n",
        "):\n",
        "    \"\"\"Returns the keypoint edges from the given `eval_config`.\n",
        "    \n",
        "    The keypoint edges parsed from the `eval_config` are stored\n",
        "    in a list of tuple-formatted (start, end) keypoints\n",
        "\n",
        "    :param eval_config: Eval config containing the keypoint edges.\n",
        "    :returns: List of edge tuples.\n",
        "    \"\"\"\n",
        "    \n",
        "    tuple_list = []\n",
        "    kp_list = eval_config.keypoint_edge\n",
        "    for edge in kp_list:\n",
        "        tuple_list.append((edge.start, edge.end))\n",
        "    return tuple_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55b56cea",
      "metadata": {
        "id": "55b56cea"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `CarND-Object-Detection-Lab.ipynb`\n",
        "# NOTE: Modified for use with TF2.x API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd99a1f",
      "metadata": {
        "id": "dcd99a1f"
      },
      "outputs": [],
      "source": [
        "# Set colours to assign each class \n",
        "COLOUR_LIST = sorted([c for c in ImageColor.colormap.keys()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd86d5da",
      "metadata": {
        "id": "fd86d5da"
      },
      "outputs": [],
      "source": [
        "def filter_boxes(\n",
        "        min_score: float, \n",
        "        boxes: np.ndarray, \n",
        "        scores: np.ndarray, \n",
        "        classes: np.ndarray\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Returns boxes with a confidence above the `min_score` threshold.\n",
        "    \n",
        "    :param min_score: Minimum confidence score of bounding box to allow.\n",
        "    :param boxes: Set of predicted bounding boxes to threshold.\n",
        "    :param scores: Corresponding set of confidence (IoU) scores.\n",
        "    :param classes: Corresponding set of class labels for each bbox.\n",
        "    :returns: Tuple of filtered bounding boxes and their corresponding\n",
        "        IoU scores and class labels.\n",
        "    \"\"\"\n",
        "    n = len(classes)\n",
        "    idxs = []\n",
        "    for i in range(n):\n",
        "        if scores[i] >= min_score:\n",
        "            idxs.append(i)\n",
        "    filtered_boxes = boxes[idxs, ...]\n",
        "    filtered_scores = scores[idxs, ...]\n",
        "    filtered_classes = classes[idxs, ...]\n",
        "    return filtered_boxes, filtered_scores, filtered_classes\n",
        "\n",
        "\n",
        "def to_image_coords(\n",
        "        boxes: np.ndarray, \n",
        "        height: int, \n",
        "        width: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Converts bounding box coordinates to original image coordinates.\n",
        "    \n",
        "    De-normalises the coordinate predictions from range [0, 1] to the\n",
        "    original image range given by height x width.\n",
        "    :param boxes: Set of bounding box coordinates to de-normalise.\n",
        "    :param height: Height of the original image (in pixels).\n",
        "    :param width: Width of the original image (in pixels).\n",
        "    :returns: Set of bounding boxes with coordinates in original image frame.\n",
        "    \"\"\"\n",
        "    \n",
        "    box_coords = np.zeros_like(boxes)\n",
        "    box_coords[:, 0] = boxes[:, 0] * height\n",
        "    box_coords[:, 1] = boxes[:, 1] * width\n",
        "    box_coords[:, 2] = boxes[:, 2] * height\n",
        "    box_coords[:, 3] = boxes[:, 3] * width\n",
        "    return box_coords\n",
        "\n",
        "\n",
        "def draw_boxes(\n",
        "        image: PIL.Image, \n",
        "        boxes: np.ndarray, \n",
        "        classes: np.ndarray, \n",
        "        thickness: int=4\n",
        "):\n",
        "    \"\"\"Draws the bounding boxes onto the image.\n",
        "    \n",
        "    :param image: a `PIL.Image` instance to draw bboxes onto.\n",
        "    :param boxes: Set of bounding boxes to draw onto `image`.\n",
        "    :param classes: Corresponding set of class labels.\n",
        "    :param thickness: Line width to use for the bounding boxes.\n",
        "    \"\"\"\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for i in range(len(boxes)):\n",
        "        bot, left, top, right = boxes[i, ...]\n",
        "        class_id = int(classes[i])\n",
        "        color = COLOR_LIST[class_id]\n",
        "        draw.line(\n",
        "            [(left, top), \n",
        "             (left, bot), \n",
        "             (right, bot), \n",
        "             (right, top), \n",
        "             (left, top)\n",
        "            ], \n",
        "            width=thickness, \n",
        "            fill=color\n",
        "        )\n",
        "        \n",
        "        \n",
        "def load_graph(graph_file):\n",
        "    \"\"\"Loads a frozen inference graph\"\"\"\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        od_graph_def = tf.GraphDef()\n",
        "        with tf.gfile.GFile(graph_file, 'rb') as fid:\n",
        "            serialized_graph = fid.read()\n",
        "            od_graph_def.ParseFromString(serialized_graph)\n",
        "            tf.import_graph_def(od_graph_def, name='')\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb8978f",
      "metadata": {
        "id": "ffb8978f"
      },
      "source": [
        "#### Loading the model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1f6d82",
      "metadata": {
        "id": "ab1f6d82"
      },
      "outputs": [],
      "source": [
        "# Stores as keys the model name(s) from the TF2 Model Zoo\n",
        "# with corresponding values equal to the weights filename(s) \n",
        "MODELS = {\n",
        "    'ssd_mobilenet_v1_fpn_640x640': 'ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a8edac",
      "metadata": {
        "id": "91a8edac"
      },
      "outputs": [],
      "source": [
        "# Fetching the model name and filename of weights to use\n",
        "model_name = 'ssd_mobilenet_v1_fpn_640x640'\n",
        "model_weights_name = MODELS[model_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c5a8b3",
      "metadata": {
        "id": "06c5a8b3"
      },
      "outputs": [],
      "source": [
        "# Download the checkpoint (weights) and store into local directory\n",
        "DIR_WEIGHTS = os.path.join(\n",
        "    'models/research/object_detection', \n",
        "    'test_data'\n",
        ")\n",
        "if model_name == 'ssd_mobilenet_v1_fpn_640x640':\n",
        "    # Download the model checkpoint\n",
        "    !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "    # Untar the model checkpoint file\n",
        "    !tar -xf ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "    # Move the model `checkpoint` folder into `DIR_WEIGHTS`\n",
        "    !mv ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data\n",
        "    \n",
        "elif model_name == 'faster_rcnn_inception_resnet_v2_640x640':\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7bcea1",
      "metadata": {
        "id": "dc7bcea1"
      },
      "source": [
        "#### Loading model configs and checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f17f6be",
      "metadata": {
        "id": "6f17f6be"
      },
      "outputs": [],
      "source": [
        "# Set the model `pipeline.config` file path\n",
        "PATH_PIPELINE_CONFIG = os.path.join(\n",
        "    'models/research/object_detection/configs/tf2',\n",
        "    model_name + '.config'\n",
        ")\n",
        "# Set the model 'checkpoint' sub-directory path\n",
        "DIR_MODEL = os.path.join(\n",
        "    DIR_WEIGHTS,\n",
        "    'checkpoint'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041f3464",
      "metadata": {
        "id": "041f3464"
      },
      "outputs": [],
      "source": [
        "# Load the TF Object Detection API model config\n",
        "configs = config_util.get_configs_from_pipeline_file(\n",
        "    PATH_PIPELINE_CONFIG\n",
        ")\n",
        "model_config = configs['model']\n",
        "# Build the model for inference mode\n",
        "detection_model = model_builder.build(\n",
        "    model_config=model_config,\n",
        "    is_training=False    # Running pre-trained model in inference mode\n",
        ")\n",
        "# Restore the model checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(\n",
        "    model=detection_model\n",
        ")\n",
        "cpkt.restore(\n",
        "    os.path.join(DIR_MODEL, 'cpkt-0')\n",
        ").expect_partial()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc9f1f2",
      "metadata": {
        "id": "cdc9f1f2"
      },
      "outputs": [],
      "source": [
        "def get_model_detection_function(\n",
        "    model: tf\n",
        ") -> Callable[[tf.Tensor], \n",
        "              Tuple[dict, dict, tf.Tensor]\n",
        "]:\n",
        "    \"\"\"Creates the detection / inference loop `tf.function`.\"\"\"\n",
        "    \n",
        "    print(type(model))\n",
        "    @tf.function\n",
        "    def detect_fn(image):\n",
        "        '''Performs inference on the input image.'''\n",
        "        images, shapes = model.preprocess(image)\n",
        "        prediction_dict = model.predict(image, shapes)\n",
        "        detections = model.postprocess(prediction_dict, shapes)\n",
        "        return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
        "    return detect_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb8f717",
      "metadata": {
        "id": "4fb8f717"
      },
      "outputs": [],
      "source": [
        "# Initialise the detection function with the pre-trained model instance\n",
        "detect_fn = get_model_detection_function(detection_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "020d8bcf",
      "metadata": {
        "id": "020d8bcf"
      },
      "source": [
        "#### Loading the Label Map data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02eee9af",
      "metadata": {
        "id": "02eee9af"
      },
      "source": [
        "In order to properly map the class labels (strings, e.g., `'pedestrian'`) to class id integers (e.g., `0`), the TensorFlow 2 Object Detection API must be provided a `label_map.pbtxt` file. [Creating this file](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#create-label-map) can be done manually or using an [automated script](https://github.com/tensorflow/models/issues/1601#issuecomment-533659942). \n",
        "\n",
        "Since we are (for now) using the [`pets_example.record`](https://github.com/tensorflow/models/blob/master/research/object_detection/test_data/pets_examples.record) test data set provided in the TensorFlow [Object Detection](https://github.com/tensorflow/models/tree/master/research/object_detection) repository, we will just grab the corresponding [`pet_label_map.pbtxt`](https://github.com/tensorflow/models/blob/master/research/object_detection/data/pet_label_map.pbtxt) file.\n",
        "\n",
        "If you are attempting to use your own dataset with the TF2 Object Detection API, [check out my notebook](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-3-Creating-TF-Records/2022-08-03-Creating-TF-Records.ipynb) on doing just that. There we cover creating custom Label Map files using an automated script, serialising images and labels into TensorFlow-compatible [`.tfrecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) and [`TFRecordDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a450696",
      "metadata": {
        "id": "7a450696"
      },
      "outputs": [],
      "source": [
        "# Set the path to the Label Map\n",
        "PATH_LABEL_MAP = os.path.join(\n",
        "    'models/research/object_detection',\n",
        "    'data',\n",
        "    'pet_label_map.pbtxt'\n",
        ")\n",
        "# Create the Label Map instance\n",
        "label_map = label_map_util.load_labelmap(\n",
        "    PATH_LABEL_MAP\n",
        ")\n",
        "# Extract the Label Map data\n",
        "N_CLASSES = label_map_util.get_max_label_map_index(label_map)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map=label_map,\n",
        "    max_num_classes=N_CLASSES,\n",
        "    use_display_name=True\n",
        ")\n",
        "# Get the Label Map dict\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "label_map_dict = label_map_util.get_label_map_dict(\n",
        "    label_map,\n",
        "    use_display_name=True\n",
        ")\n",
        "label_map_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e26864ec",
      "metadata": {
        "id": "e26864ec"
      },
      "source": [
        "#### Running the inference loop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113de8e0",
      "metadata": {
        "id": "113de8e0"
      },
      "outputs": [],
      "source": [
        "# Set path to input data to run inference on\n",
        "# NOTE: since we loaded the `pet_label_map.pbtxt` earlier, we should\n",
        "# probably only run inference on images with dog breed classes\n",
        "DIR_DATA = 'models/research/object_detection/test_images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983a53a5",
      "metadata": {
        "id": "983a53a5"
      },
      "outputs": [],
      "source": [
        "# Get the first input image to load in as a NumPy array\n",
        "# Can alternatively load a TFRecord file and skip NumPy conversion\n",
        "# e.g., `.../test_data/pets_example.record`\n",
        "img_np = load_image_into_numpy_array(\n",
        "    os.path.join(DIR_DATA, \n",
        "                 'image1.jpg'  # Image with TPs (i.e., dogs to classify)\n",
        "    )\n",
        ")\n",
        "# Convert the NumPy array to TensorFlow `tf.Tensor`\n",
        "img_tensor = tf.convert_to_tensor(\n",
        "    np.expand_dims(img_np, 0),\n",
        "    dtype=tf.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7955b6",
      "metadata": {
        "id": "8b7955b6"
      },
      "outputs": [],
      "source": [
        "# Run inference over input image\n",
        "detections, predictions_dict, shapes = detect_fn(img_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cbce7cf",
      "metadata": {
        "id": "5cbce7cf"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the original image (for modification)\n",
        "img_np_with_detections = img_np.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9f1306",
      "metadata": {
        "id": "3c9f1306"
      },
      "outputs": [],
      "source": [
        "# Visualise the predicted bounding box(es) and class label(s)\n",
        "LABEL_ID_OFFSET = 1    # Index which the label map class ids start at\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image=img_np_with_detections,\n",
        "    boxes=detections['detection_boxes'][0].numpy(),\n",
        "    classes=(detections['detection_classes'][0].numpy() \n",
        "             + LABEL_ID_OFFSET).astype(int),\n",
        "    scores=detections['detection_scores'][0].numpy(),\n",
        "    category_index=category_index,\n",
        "    keypoints=None,\n",
        "    keypoint_scores=None,\n",
        "    track_ids=None, \n",
        "    use_normalized_coordinates=True,\n",
        "    max_boxes_to_draw=200,\n",
        "    min_score_thresh=0.30,\n",
        "    agnostic_mode=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4483e6e",
      "metadata": {
        "id": "a4483e6e"
      },
      "source": [
        "With the above utility function provided by TensorFlow, we no longer need to define our own IoU thresholding or coordinate conversion / de-normalisation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac72970d",
      "metadata": {
        "id": "ac72970d"
      },
      "source": [
        "### 2.4. Timing Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "403d2bae",
      "metadata": {
        "id": "403d2bae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc17783",
      "metadata": {
        "id": "6dc17783"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "23b7033d",
      "metadata": {
        "id": "23b7033d"
      },
      "source": [
        "### 2.5. Object Detection Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2846dd45",
      "metadata": {
        "id": "2846dd45"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d8cee3f",
      "metadata": {
        "id": "4d8cee3f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5c6e83cf",
      "metadata": {
        "id": "5c6e83cf"
      },
      "source": [
        "## 3. Closing Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138d7d2f",
      "metadata": {
        "id": "138d7d2f"
      },
      "source": [
        "##### Alternatives\n",
        "* TODO.\n",
        "##### Extensions of task\n",
        "* TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "550f9ba4",
      "metadata": {
        "id": "550f9ba4"
      },
      "source": [
        "## 4. Future Work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5728bd96",
      "metadata": {
        "id": "5728bd96"
      },
      "source": [
        "- ⬜️ TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6526376",
      "metadata": {
        "id": "f6526376"
      },
      "source": [
        "## Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a9606fb",
      "metadata": {
        "id": "9a9606fb"
      },
      "source": [
        "This assignment was prepared by Kelvin Lwin, Andrew Bauman, Dominique Luna et al., 2021 (link [here](https://github.com/udacity/CarND-Object-Detection-Lab)).\n",
        "\n",
        "References\n",
        "* [1] Howard, A. G. et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv. 2017. [doi:10.48550/arXiv.1704.04861](https://arxiv.org/abs/1704.04861).\n",
        "* [2] Shelhamer, E. et al. Fully Convolutional Networks for Semantic Segmentation. arXiv. 2016. [doi:10.48550/arXiv.1605.06211](https://arxiv.org/abs/1605.06211).\n",
        "* [3] Liu, W. et al. SSD: Single Shot MultiBox Detector. European Conference on Computer Vision, ECCV. Lecture Notes in Computer Science, 9905:21-37. 2016. [doi:10.1007/978-3-319-46448-0_2](https://doi.org/10.1007/978-3-319-46448-0_2).\n",
        "\n",
        "Helpful resources:\n",
        "* [`CarND-Object-Detection-Lab` by @udacity | GitHub](https://github.com/udacity/CarND-Object-Detection-Lab);\n",
        "* [3.4. Depthwise Convolution | Dive Into Deep Learning](https://tvm.d2l.ai/chapter_common_operators/depthwise_conv.html);\n",
        "* [Depth-wise Convolution and Depth-wise Separable Convolution by A. Pandey | Medium](https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec);\n",
        "* [Pointwise Convolution by A. Shrivastav | OpenGenus](https://iq.opengenus.org/pointwise-convolution/);\n",
        "* [Depthwise Separable Convolution - A FASTER CONVOLUTION! | YouTube](https://www.youtube.com/watch?v=T7o3xvJLuHk)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}