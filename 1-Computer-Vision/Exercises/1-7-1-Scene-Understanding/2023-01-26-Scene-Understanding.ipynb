{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d3c8fa",
   "metadata": {
    "id": "10d3c8fa"
   },
   "source": [
    "# Exercise 1.7.1 — Scene Understanding\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8342f66",
   "metadata": {
    "id": "a8342f66"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8a817",
   "metadata": {
    "id": "3ce8a817"
   },
   "source": [
    "* Compute the Mean Intersection over Union (IoU) of the multi-class segmentation label predictions;\n",
    "* Implement a standard convolution block and compare it to the MobileNet convolution block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a226974",
   "metadata": {
    "id": "9a226974"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e37f8f",
   "metadata": {
    "id": "36e37f8f"
   },
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040b9ad5",
   "metadata": {
    "id": "040b9ad5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3396c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "id": "6f3396c3",
    "outputId": "2f9e10d1-cbb7-46d6-c01e-6b8f7def1e3a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.9.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab345e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "id": "9ab345e3",
    "outputId": "f8ddd501-512e-461e-ecb9-4dd2b5c7baf4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b54c301",
   "metadata": {
    "id": "4b54c301"
   },
   "outputs": [],
   "source": [
    "### Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5279089e",
   "metadata": {
    "id": "5279089e"
   },
   "outputs": [],
   "source": [
    "ENV_COLAB = True                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda135f9",
   "metadata": {
    "id": "eda135f9"
   },
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8bc08a8",
   "metadata": {
    "id": "a8bc08a8"
   },
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c1e4cd",
   "metadata": {
    "id": "e7c1e4cd"
   },
   "outputs": [],
   "source": [
    "### Creating subdirectories (if not exists)\n",
    "os.makedirs(DIR_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e58534",
   "metadata": {
    "id": "21e58534"
   },
   "source": [
    "### 1.1. Scene Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d70ec3",
   "metadata": {
    "id": "73d70ec3"
   },
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd081af",
   "metadata": {
    "id": "3fd081af"
   },
   "source": [
    "TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34dd32",
   "metadata": {
    "id": "5f34dd32"
   },
   "source": [
    "#### Metrics — Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab143bf",
   "metadata": {
    "id": "3ab143bf"
   },
   "source": [
    "In the [very first exercise](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-1-Choosing-Metrics/2022-07-25-Choosing-Metrics-IoU.ipynb) of this course, we covered the Intersection over Union (IoU) metric and its application to the bounding box prediction task. Now, we use the IoU metric again but this time for semantic segmentation and scene understanding.\n",
    "\n",
    "We start with the same general formula for the IoU score given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU} &= \\frac{\\textrm{Area of Intersection}}{\\textrm{Area of Union}},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "but now we calculate the IoU score using the following binary classification metrics:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With this form of the IoU equation, all we need to do is compute the true positive ($\\mathrm{TP}$), true negative ($\\mathrm{TN}$), and the false positive ($\\mathrm{FN}$), false negative ($\\mathrm{FN}$) rates. For the image segmentation task, this boils down to the pixel-wise classification predictions. Thankfully, the algorithms we designed in [Sect. 2.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-1-Choosing-Metrics/2022-07-25-Choosing-Metrics-IoU.ipynb) of Exercise 1.1.1 hold; all we need to do is compute the pixel-wise classification metrics for each class using the same tabular approach as before. With these metrics, we evaluate the $\\mathrm{IoU}$ formula and obtain a score indicating the amount of \"overlap\" between the predicted region and the true region of each segmented object. \n",
    "\n",
    "Let's illustrate this with a simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54cb770",
   "metadata": {
    "id": "d54cb770"
   },
   "source": [
    "```python\n",
    "ground_truth_labels = [\n",
    "    [0, 0, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2], \n",
    "    [3, 3, 3, 3],\n",
    "]\n",
    "predicted_labels = [\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 3, 0, 1],\n",
    "    [2, 2, 2, 3],\n",
    "    [3, 1, 0, 0],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc4fe7",
   "metadata": {
    "id": "69dc4fe7"
   },
   "source": [
    "Above we define a set of _ground-truth_ and _predicted_ labels. Each row in the matrix corresponds to a class; looking at the first row of the `ground_truth_labels` (\"$\\mathrm{A}$\") matrix, we see that class `0` should appear at all four pixel locations. Looking at the first row of `predicted_labels` (\"$\\mathrm{B}$\" matrix), we see instead that only three of the four pixel locations were given a correct prediction of class `0`. In other words, we have in the first row a $\\mathrm{TP} = 3$. Now, we need to compute for class `0` the false positive ($\\mathrm{FP}$) rate. To do this, we examine the _other_ pixel locations (i.e., other rows of the `predicted_labels` matrix), and add up any occurrences of class label `0` where the corresponding entries in `ground_truth_labels` do not match. Since class label `0` was predicted _incorrectly_ at pixel locations $\\mathrm{B}_{2, 3}$, $\\mathrm{B}_{3, 3}$, and $\\mathrm{B}_{4, 4}$, we have a $\\mathrm{FP} = 3$. Now let's complete the calculations for the two other metrics: true negative ($\\mathrm{TN}$) and false negative ($\\mathrm{FN}$). The $\\mathrm{TN}$ value for this problem is easy to compute, since we assume all predictions here were valid (i.e., that we expected a class label to be predicted for every pixel in `predicted_labels`). That means our $\\mathrm{TN} = 0$. Lastly, our $\\mathrm{FN}$ rate is computed as the number of _incorrect_ predictions for class `0`. Looking at the first row of the `predicted_labels` matrix (i.e., the \"predictions\" for class `0`), we count the number of label predictions that are _not_ equal to class `0` to get our false negative rate. With _one_ incorrect class `0` prediction at the first index $\\mathrm{B}_{1, 1} = $ `1`, we have therefore a $\\mathrm{FN} = 1$. \n",
    "\n",
    "With these four classification metrics out of the way, we can obtain the $\\mathrm{IoU}$ score for class `0` as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU}_{0} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}} = \\frac{3}{3 + 3 + 1 + 0} = \\frac{3}{7} \\approx 0.4286.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now that we have the $\\mathrm{IoU}$ score for class `0` computed, we repeat the process for the other three rows (classes) in `predicted_labels` to obtain each classes' respective $\\mathrm{IoU}$ score. Once we have completed the calculations of all four classes, we can take the average to obtain the $\\mathrm{IoU}_{\\textrm{mean}}$, as simply:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU}_{\\textrm{mean}} &= \\frac{1}{n}\\sum_{i=0}^{n} \\mathrm{IoU}_{i},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is nothing but the sum of the per-class $\\mathrm{IoU}$ scores divided by the total number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ke35mrjgWjFL",
   "metadata": {
    "id": "ke35mrjgWjFL"
   },
   "source": [
    "### 1.2. Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e37fc7",
   "metadata": {
    "id": "55e37fc7"
   },
   "source": [
    "[Convolution](https://en.wikipedia.org/wiki/Convolution) is a measure of overlap between two functions as one slides over the other. Mathematically, it is a sum of products given by the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(f * g\\right)\\left(t\\right) := \\int_{-\\infty}^{\\infty} f\\left(\\tau\\right)\\cdot g\\left(t-\\tau\\right)d\\tau.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layers) in a neural network performs the convolution operation by applying a filter over the input tensor. After the same filter is repeatedly applied to the input, a feature map is created which shows the positions and intensity of a detected feature in an input. \n",
    "\n",
    "Convolutional layers are extremely useful in neural networks, but they often come with a high computational cost due to the number of parameters required for each input (and each channel of the input). In this section we will look at the basics of regular convolution and review the number of parameters required. Then, we will introduce several alternative convolution methods — namely, depth-wise, depth-wise separable and point-wise convolution, which help reduce this computational cost. These alternatives are especially useful for networks intended to be run on mobile devices and embedded hardware where computational resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B3WcU3QlWi5j",
   "metadata": {
    "id": "B3WcU3QlWi5j"
   },
   "source": [
    "#### Regular Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_hk_pcnXFbL",
   "metadata": {
    "id": "H_hk_pcnXFbL"
   },
   "source": [
    "In a typical [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layers), we have a set of $\\mathrm{N}$ kernels, each with a size of $\\mathrm{D}_{k} * \\mathrm{D}_{k}$. Each of these kernels convolves (\"slides over\") the entire input, which is a $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$ sized feature map (a tensor). When considering the computational cost of a convolution operation, we must consider the number of _parameters_ required for each convolutional layer. Generally, models with more convolutional layers have more parameters, and therefore require more computational resources to use. While this can lead to higher accuracy in image processing tasks, special attention needs to be paid to the computational cost of convolutional networks when utilising low-end hardware, such as with in-vehicle embedded devices which will be powering these type of networks with real-time inference demands.\n",
    "\n",
    "To understand the computational cost of a regular convolutional layer, we have the following cost:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N} * \\mathrm{D}_{k} * \\mathrm{D}_{k},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{D}_{g} * \\mathrm{D}_{g}$ is the size of the output feature map. A regular convolution takes in a $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$ input feature map and returns a $\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{N}$ feature map as output.\n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008yIebXacAE",
   "metadata": {
    "id": "008yIebXacAE"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-1-Standard-Convolution-Filters.png\" alt=\"Figure 1. Filters in a regular convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 1. Filters in a regular convolutional layer (credit: Howard et al., 2017).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_C3E7_bbOyD",
   "metadata": {
    "id": "H_C3E7_bbOyD"
   },
   "source": [
    "#### Depth-wise Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pXNWwOkWbPSL",
   "metadata": {
    "id": "pXNWwOkWbPSL"
   },
   "source": [
    "[Depth-wise convolution](https://tvm.d2l.ai/chapter_common_operators/depthwise_conv.html) and depth-wise separable convolution are two atypical convolution operations that have less parameters and therefore require less computational power to compute. Depth-wise convolutions are used in the [MobileNets](https://arxiv.org/abs/1704.04861) [1] architecture designed for mobile and embedded applications.\n",
    "\n",
    "Depth-wise convolution acts on each input channel separately with a different kernel for each. With a number of input channels $\\mathrm{M}$ we have $\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}$ kernels. Since depth-wise convolution only acts on a single input channel at a time, the kernel depth $\\mathrm{N}$ is set equal to $1$. Therefore, the computational cost of the depth-wise convolution is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To produce the same effect with regular convolution, each channel of the input requires its own kernel. To compute the convolution, each channel is selected individually, and all elements in the kernel are set to zero except for those corresponding to the respective input channel. The final output of the stacked convolutions is one $\\mathrm{M}$-channel output feature map. As shown above, depth-wise convolution reduces the number of parameters required by a factor $\\mathrm{N}$, i.e., the number of filters required _per input channel_. For a three-channel input, we require _three times less_ number of parameters with the depth-wise convolution approach.   \n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857dff0",
   "metadata": {
    "id": "d857dff0"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-2-Depthwise-Convolution-Filters.png\" alt=\"Figure 2. Filters in a depth-wise convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 2. Filters in a depth-wise convolutional layer (credit: Howard et al., 2017).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c2bbf",
   "metadata": {
    "id": "0a2c2bbf"
   },
   "source": [
    "In order to understand depth-wise separable convolution, we first introduce the point-wise convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a30c37",
   "metadata": {
    "id": "40a30c37"
   },
   "source": [
    "#### Point-wise Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5837bf",
   "metadata": {
    "id": "0e5837bf"
   },
   "source": [
    "Point-wise convolution is a form of convolution that applies a $1\\times 1$ kernel across an input. Unlike the depth-wise convolution, the $1\\times 1$ kernel used here has a depth equal to the number of channels in the input. The computational complexity of the point-wise convolution is similar to the regular convolution, but instead the filter size $\\mathrm{D}_{k} * \\mathrm{D}_{k}$ is equal to $1 \\times 1$. Therefore, we have the following number of parameters:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f85ba",
   "metadata": {
    "id": "d32f85ba"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-3-Pointwise-Convolution-Filters.png\" alt=\"Figure 3. Filters in a point-wise convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 3. Filters in a point-wise convolutional layer (credit: Howard et al., 2017).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13941d",
   "metadata": {
    "id": "7a13941d"
   },
   "source": [
    "#### Depth-wise Separable Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cb534",
   "metadata": {
    "id": "942cb534"
   },
   "source": [
    "Point-wise convolution is used in conjunction with depth-wise convolution in order to perform depth-wise separable convolution. Here, depth-wise separable convolution borrows the idea that the depth and the spatial dimension of a filter can be separated, as is the case with e.g., the [Sobel](https://en.wikipedia.org/wiki/Sobel_operator) filter for edge detection. The depth-wise separable convolution separates a kernel into two independent kernels, each of which performs two convolutions: the depth-wise and the point-wise convolution. Thus, the total computational cost for the depth-wise separable convolution is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right) + \\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which results in a $\\frac{1}{\\mathrm{N}} + \\frac{1}{\\mathrm{D}_{k} * \\mathrm{D}_{k}}$ reduction in total computation, which can be observed in the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right) + \\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right)}{\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N} * \\mathrm{D}_{k} * \\mathrm{D}_{k}} &= \\frac{\\mathrm{D}_{k}^{2} + \\mathrm{N}}{\\mathrm{D}_{k}^{2} * \\mathrm{N}} = \\frac{1}{\\mathrm{N}} + \\frac{1}{\\mathrm{D}_{k}^{2}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $\\mathrm{N}$ is selected to be large enough, depth-wise separable convolution networks can be immensely more computationally efficient. For example, the MobileNet architecture uses a $3\\times 3$ kernel, which results in $\\sim 9\\mathrm{x}$ efficiency improvement over regular convolution layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff911d9",
   "metadata": {
    "id": "0ff911d9"
   },
   "source": [
    "#### Width Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744d977",
   "metadata": {
    "id": "b744d977"
   },
   "source": [
    "In addition to utilising alternative convolution methods, scaling the number of input and output channels proportional to a _width multiplier_ is often performed. The width multiplier $\\alpha$ is a hyperparameter set to a value in the range $\\alpha \\in \\left[0, 1\\right]$. This results in a computational cost:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\alpha\\mathrm{N}\\right).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d363e25",
   "metadata": {
    "id": "4d363e25"
   },
   "source": [
    "#### Resolution Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9984092",
   "metadata": {
    "id": "e9984092"
   },
   "source": [
    "Similar to the width multiplier, a _resolution multiplier_ is used to scale the size of the input feature map. The resolution multiplier $\\rho$ is a hyperparameter set to a value in the range $\\rho \\in \\left[0, 1\\right]$. This results in a computational cost:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\mathrm{M} * \\mathrm{N}\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Combining the width and resolution multipliers results in a scaled computational cost of:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\alpha\\mathrm{N}\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the MobileNets architecture, for example, these values of $\\alpha$ and $\\rho$ are selected w.r.t. the speed versus accuracy versus size trade-off. In the paper by original author Howard et al., 2017, the authors found that the resolution multiplier has the effect of reducing computational cost by $\\rho^{2}$, whereas the width multiplier has the effect of reducing computational cost and number of parameters quadratically by roughly $\\alpha^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaa112",
   "metadata": {
    "id": "7cfaa112"
   },
   "source": [
    "### 1.3. Convolutional Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9769c0",
   "metadata": {
    "id": "4d9769c0"
   },
   "source": [
    "#### FCN-8 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cc7a7",
   "metadata": {
    "id": "066cc7a7"
   },
   "source": [
    "The [FCN-8]() architecture by Long et al., 2014 [2] is an architecture that uses $1\\times 1$ convolutional layers to replace the fully-connecte—d layers of a standard neural network. As a result, the FCN-8 architecture is able to preserve spatial information of the input tensor and perform the down-sampling and feature extraction routines of a convolutional network.\n",
    "\n",
    "Fully-Convolutional Network (FCN) architectures have two primary components — an _encoder_ and a _decoder_. The encoder extracts features from an image using a series of sliding window convolution operations. The decoder in an FCN is used to up-scale the down-sampled intermediate feature maps generated from the encoder to a higher resolution — usually matching the original input dimensions. In the FCN-8 architecture, the encoder block is a set of $1\\times 1$ convolution layers. The decoder block of the FCN-8 is a set of transposed convolution layers which upsample the feature maps to the size of the original input. This process is usually referred to as \"reverse convolution\" or deconvolution since its effect is essentially reversing (with some loss) the downsampling of the input. \n",
    "\n",
    "In order to preserve fine-grained segmentation maps through the network to the decoder block, a set of skip _connections_ are used. Essentially, these \"connections\" between non-adjacent layers of differing resolutions help retain information from the original input by combining the output feature maps of each respective layer using an element-wise addition operation. As a result, the FCN-8 with skip connections is able to use information from multiple resolutions to make more precise segmentation decisions. Skip connections, along with the other advancements from the FCN-8 architecture, have proven successful in empirical studies between the FCN-8 and its \"sister\" networks — the FCN-16 and FCN-32. For more information on Fully-Convolutional Networks, see the [notebook from the previous lesson](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/1.7/1-Computer-Vision/Exercises/1-6-1-Fully-Convolutional-Networks/2023-01-23-Fully-Convolutional-Networks.ipynb).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81567d69",
   "metadata": {
    "id": "81567d69"
   },
   "source": [
    "#### MobileNets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4bcc5",
   "metadata": {
    "id": "38a4bcc5"
   },
   "source": [
    "The [MobileNets](https://arxiv.org/abs/1704.04861) architecture by Howard et al., 2017 [1] is an architecture that uses depth-wise separable convolutions to build light-weight deep neural networks. The MobileNets architecture, as the name suggests, is designed to run object detection and classification tasks efficiently (i.e., with high FPS and low memory footprint) on mobile and embedded devices. The MobileNets architecture achieves this in a three-part approach:\n",
    "1. **Depth-wise separable convolutions** — Perform a depth-wise convolution followed by a $1\\times 1$ convolution (instead of a standard convolution). The $1\\times 1$ convolution is called a point-wise convolution if it follows after a depth-wise convolution;\n",
    "2. **Width multipliers** — Reduces the size of the input / output channels using a scaling factor set to a value between $0.0$ and $1.0$;\n",
    "3. **Resolution multipliers** — Reduces the size of the original input using a scaling factor set to a value between $0.0$ and $1.0$.\n",
    "\n",
    "These three techniques reduce the cummulative number of parameters in the network and therefore the amount of computation required. The downside to models exploiting the parameter reduction approach is that accuracy is often the trade-off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XGkWAi-UUik0",
   "metadata": {
    "id": "XGkWAi-UUik0"
   },
   "source": [
    "#### Single Shot Detector (SSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VPNl0saRUiVs",
   "metadata": {
    "id": "VPNl0saRUiVs"
   },
   "source": [
    "Many of the earlier deep neural network architectures involved networks with more than one training phase; the [Faster-RCNN](https://arxiv.org/abs/1506.01497) for example, first trains a Region Proposal Network (RPN) which is then merged with a pre-trained classification sub-network. The [Single Shot Detector](https://arxiv.org/abs/1512.02325) (SSD) by Liu et al., 2015 [3] combines these two sub-networks into a single-pass network that predicts bounding box locations and classifies the corresponding object classes. The major difference with single-shot networks is that they can be trained end-to-end, whereas architectures with multiple sub-networks, such as the Faster-RCNN, must train each module separately. The following is an outline of the original SSD architecture proposed by Liu et al., 2015 [3]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xRRThpEkUiId",
   "metadata": {
    "id": "xRRThpEkUiId"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-4-Single-Shot-Detector-Network-Architecture.png\" alt=\"Figure 4. Architecture of the Single Shot Detector (SSD) proposed by Liu et al., 2015.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 4. Architecture of the Single Shot Detector (SSD) proposed by Liu et al., 2015.}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7uqbGoWrUh72",
   "metadata": {
    "id": "7uqbGoWrUh72"
   },
   "source": [
    "In the above architecture, we note the use of the VGG-16 pre-trained convolutional base. In this notebook, we will instead be using the MobileNet pre-trained base from Howard et al., 2017 [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tK3_wxGcYeKm",
   "metadata": {
    "id": "tK3_wxGcYeKm"
   },
   "source": [
    "##### Bounding box detection with SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qCjGDPsYcEN",
   "metadata": {
    "id": "5qCjGDPsYcEN"
   },
   "source": [
    "SSD operates on feature maps to predict bounding box locations. Recall a feature map of $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$. For each feature map location, $k$ bounding boxes are predicted. Each bounding box carries with it the following information:\n",
    "* $\\left(\\mathrm{c}x, \\mathrm{c}y, w, h\\right)$ — Four bounding box corner offset locations;\n",
    "* $C = \\left(c_{1}, c_{2},\\ldots, c_{p}\\right)$ — class probabilities.\n",
    "\n",
    "The SSD does not predict the _shape_ of the box but rather the location of where the box is in the image. The $k$ bounding boxes each have a pre-determined shape (i.e., the anchors). This is illustrated in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce723a1b",
   "metadata": {},
   "source": [
    "<img src=\"figures/2023-01-26-Figure-5-Bounding-Box-Internal-Representation-with-SSD.png\" alt=\"Figure 5. Internal representation of bounding boxes using anchors with the Single Shot Detector (SSD) network.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 5. Internal representation of bounding boxes using anchors in Single Shot Detector (SSD) network.}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddfdf9e",
   "metadata": {},
   "source": [
    "The anchor boxes used in the SSD have coordinates that are manually configured prior to training. Shown in Figure 5(c) is a set of $k = 4$ anchor boxes of varying size used to isolate an object for detection.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b337dc",
   "metadata": {
    "id": "19b337dc"
   },
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76f553",
   "metadata": {
    "id": "9d76f553"
   },
   "source": [
    "### 2.1. Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0509c31",
   "metadata": {
    "id": "e0509c31"
   },
   "source": [
    "Here we use the TensorFlow [`tf.keras.metrics.MeanIoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU) function to compute the mean Intersection over Union (IoU) across all classes $i=0,\\ldots, n$.\n",
    "\n",
    "In order to use the metric as a standalone function, we have to first initialise the respective [`tf.keras.metrics.Metric`](https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/metrics/Metric) subclass instance (i.e., `MeanIoU`), then perform a single \"state update\" using the [`update_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#update_state) class method. As arguments to this function, we pass in the `y_true` and `y_pred` tensors that we wish to evaluate. Optionally, we can provide a `sample_weight` scalar value or vector of rank equal to `y_true`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1588d58",
   "metadata": {
    "id": "f1588d58"
   },
   "outputs": [],
   "source": [
    "### Defining the number of distinct class labels (i.e., classes)\n",
    "N_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89c5313",
   "metadata": {
    "id": "d89c5313"
   },
   "outputs": [],
   "source": [
    "### Initialising the `tf.keras.metrics.Metric` instance\n",
    "iou_mean = tf.keras.metrics.MeanIoU(\n",
    "    num_classes=N_CLASSES,\n",
    "    name='Mean IoU for multi-class object segmentation data',\n",
    "    dtype=tf.dtypes.float32,\n",
    "    ### Additional arguments for TF2.10+ API:\n",
    "    #ignore_class=None,\n",
    "    #sparse_y_true=True,    # `True` if class labels are integers, `False` if floating-point\n",
    "    #sparse_y_pred=True,\n",
    "    #axis=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aEPRhQllMLHG",
   "metadata": {
    "id": "aEPRhQllMLHG"
   },
   "source": [
    "#### Testing the `MeanIoU` metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "JucYiLeoMMG1",
   "metadata": {
    "id": "JucYiLeoMMG1"
   },
   "outputs": [],
   "source": [
    "### Defining our prediction and ground-truth sets\n",
    "ground_truth_labels = [\n",
    "    [0, 0, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2], \n",
    "    [3, 3, 3, 3],\n",
    "]\n",
    "predicted_labels = [\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 3, 0, 1],\n",
    "    [2, 2, 2, 3],\n",
    "    [3, 1, 0, 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54WZKwK2Mgb9",
   "metadata": {
    "id": "54WZKwK2Mgb9"
   },
   "outputs": [],
   "source": [
    "### Converting the matrices to n-rank tensors\n",
    "y_true = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        ground_truth_labels).reshape(1, -1, len(ground_truth_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "y_pred = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        predicted_labels).reshape(1, -1, len(predicted_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "t8Fwb-ZxNsaA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8Fwb-ZxNsaA",
    "outputId": "e9ec41b3-fcea-47f7-dd1c-def61600b5af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41964284"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Computing the mean IoU\n",
    "iou_mean.update_state(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred\n",
    ")\n",
    "iou_mean.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFL3s0eqMK9W",
   "metadata": {
    "id": "SFL3s0eqMK9W"
   },
   "source": [
    "As shown above, we obtain a mean IoU score for the set of predictions of $\\mathrm{IoU}_{\\textrm{mean}} \\ \\approx 0.420$, which matches our expected value for this test set.\n",
    "\n",
    "Now, we repeat the Mean IoU calculation using a second test set of predictions. Note that [`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) instances are stateful by default, meaning that each call to `update_state()` computes the Mean IoU for the input _mini-batch of predictions / ground-truth labels_. In other words, the default behaviour for this `Metric` instance is to accumulate the Mean IoU score across calls to `update_state()`. Since we are computing the Mean IoU score of the full batch (i.e., `batch_size=1`) manually, we must use the [`reset_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#reset_state) method before performing the IoU calculation with [`update_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#update_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "V5vt1MGZSpbQ",
   "metadata": {
    "id": "V5vt1MGZSpbQ"
   },
   "outputs": [],
   "source": [
    "### Defining our new prediction set (assuming same `ground_truth_labels`)\n",
    "predicted_labels = [\n",
    "    [0, 0, 0, 0],\n",
    "    [1, 0, 0, 1],\n",
    "    [1, 2, 2, 1],\n",
    "    [3, 3, 0, 3],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "KOVETdBRU0Iu",
   "metadata": {
    "id": "KOVETdBRU0Iu"
   },
   "outputs": [],
   "source": [
    "### Converting the matrix to an n-rank tensor\n",
    "y_pred = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        predicted_labels).reshape(1, -1, len(predicted_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "RmADU5HjSpTZ",
   "metadata": {
    "id": "RmADU5HjSpTZ"
   },
   "outputs": [],
   "source": [
    "### Resetting the state of the `MeanIoU` instance\n",
    "# i.e., current cumulative mean IoU becomes `0.0`\n",
    "iou_mean.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "QauTkcrRSpKm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QauTkcrRSpKm",
    "outputId": "2f7b4a58-8e5c-4d4b-e070-8ad4a2533d24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53869045"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Computing the new mean IoU\n",
    "iou_mean.update_state(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred\n",
    ")\n",
    "iou_mean.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M9kVfx95VQ48",
   "metadata": {
    "id": "M9kVfx95VQ48"
   },
   "source": [
    "As shown above, we obtain a new mean IoU score for this second test of predictions of $\\mathrm{IoU}_{\\textrm{mean}} \\ \\approx 0.539$, which matches our expected value for this second test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0ea8b",
   "metadata": {
    "id": "b0c0ea8b"
   },
   "source": [
    "### 2.2. Separable Depthwise Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00310c4d",
   "metadata": {
    "id": "00310c4d"
   },
   "source": [
    "NOTE: the code provided here has been migrated to the TensorFlow 2.x API. Some functionality may differ from the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7e989",
   "metadata": {
    "id": "d6b7e989"
   },
   "source": [
    "Here we implement the a MobileNets depth-wise separable convolution block using the following [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) components:\n",
    "* [`tf.nn.depthwise_conv2d`](https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d);\n",
    "* [`tf.keras.layers.BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization);\n",
    "* [`tf.keras.layers.ReLU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU);\n",
    "* [`tf.keras.layers.Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n",
    "\n",
    "\n",
    "We will then compare the number of parameters of the depth-wise separable convolution block to a regular convolution block, which is formed using the following [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) components:\n",
    "* [`tf.keras.layers.Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layer;\n",
    "* [`tf.keras.layers.BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e89e05aa",
   "metadata": {
    "id": "e89e05aa"
   },
   "outputs": [],
   "source": [
    "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce9aba00",
   "metadata": {
    "id": "ce9aba00"
   },
   "outputs": [],
   "source": [
    "def vanilla_conv_block(\n",
    "        x: Union[np.ndarray, tf.Tensor], \n",
    "        kernel_size: Union[Tuple[int], List[int]],\n",
    "        output_channels: int\n",
    ") -> tf.keras.models.Sequential:\n",
    "    \"\"\"Implements a vanilla (regular) convolution block.\n",
    "    \n",
    "    A convolution block here is defined as the following:\n",
    "        Vanilla Conv -> Batch Norm -> ReLU,\n",
    "    where 'Vanilla Conv' corresponds to the `Conv2D` layer\n",
    "    provided in the TensorFlow 2.x API.\n",
    "    \n",
    "    :param x: Input tensor used to build the convolutional block,\n",
    "        i.e., input shape of `x` is provided for delayed-build pattern.\n",
    "    :param kernel_size: Kernel size to assign the convolutional layer.\n",
    "    :param output_channels: Depth of the output tensor.\n",
    "    :returns: The 'vanilla' convolutional block,\n",
    "        buit for input tensors of shape given by `x`.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Build the convolutional block\n",
    "    conv = tf.keras.models.Sequential() \n",
    "    conv.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=output_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=(2, 2),\n",
    "            padding='SAME'\n",
    "        )\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.ReLU()\n",
    "    )\n",
    "    ### Build the block by setting expected input shape to shape of tensor `x`\n",
    "    conv.build(x.shape)\n",
    "    ### Return the built convolutional block\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8849feff",
   "metadata": {
    "id": "8849feff"
   },
   "outputs": [],
   "source": [
    "# TODO: implement MobileNet conv block\n",
    "def mobilenet_conv_block(\n",
    "        x: Union[np.ndarray, tf.Tensor], \n",
    "        kernel_size: Union[Tuple[int], List[int]], \n",
    "        output_channels: int\n",
    ") -> tf.keras.models.Sequential:\n",
    "    \"\"\"Implements a depth-wise separable convolution block (Howard, 2017).\n",
    "\n",
    "    A depth-wise separable convolution block is defined as the following:\n",
    "        Depth-wise -> Batch Norm -> ReLU -> Point-wise -> Batch Norm -> ReLU,\n",
    "    where the 'Point-wise' is implemented using the `Conv2D` layer provided\n",
    "    in the TensorFlow 2.x API.\n",
    "\n",
    "    :param x: Input tensor used to build the MobileNet convolutional block,\n",
    "        i.e., input shape of `x` is provided for delayed-build pattern.\n",
    "    :param kernel_size: Kernel size to assign the convolutional layers.\n",
    "    :param output_channels: Depth of the output tensor\n",
    "    :returns: The 'MobileNet' convolutional block,\n",
    "        built for input tensors of shape given by `x`.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Build the MobileNet convolutional block\n",
    "    conv = tf.keras.models.Sequential()\n",
    "    conv.add(\n",
    "        # Should have 3x3 kernel\n",
    "        tf.keras.layers.DepthwiseConv2D(\n",
    "            kernel_size=KERNEL_SIZE,\n",
    "            strides=(1, 1),\n",
    "            padding='SAME'\n",
    "        )\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.ReLU()\n",
    "    )\n",
    "    # Should have 1x1 kernel\n",
    "    conv.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "        )\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.ReLU()\n",
    "    )\n",
    "    ### Build the block by setting expected input shape to shape of tensor `x`\n",
    "    conv.build(x.shape)\n",
    "    ### Return the built MobileNet convolutional block\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceae4f5",
   "metadata": {
    "id": "1ceae4f5"
   },
   "source": [
    "#### Testing the MobileNets convolutional block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04345315",
   "metadata": {
    "id": "04345315"
   },
   "outputs": [],
   "source": [
    "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da2JH9neiom9",
   "metadata": {
    "id": "da2JH9neiom9"
   },
   "outputs": [],
   "source": [
    "### Setting the input parameters\n",
    "INPUT_CHANNELS = 32\n",
    "OUTPUT_CHANNELS = 512\n",
    "KERNEL_SIZE = 3\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "GNrrlU1pioiv",
   "metadata": {
    "id": "GNrrlU1pioiv"
   },
   "outputs": [],
   "source": [
    "### Creating the input tensor\n",
    "x = tf.constant(\n",
    "    np.random.rand(1, IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS),\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "U3z6To3Ii7dl",
   "metadata": {
    "id": "U3z6To3Ii7dl"
   },
   "outputs": [],
   "source": [
    "### Testing the 'vanilla' convolutional block\n",
    "conv_vanilla = vanilla_conv_block(\n",
    "    x=x,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    output_channels=OUTPUT_CHANNELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "oeycDPLPjwCo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeycDPLPjwCo",
    "outputId": "f8fb1997-e2d8-45aa-e228-52f7ddda6b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (1, 128, 128, 512)        147968    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (1, 128, 128, 512)       2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (1, 128, 128, 512)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 150,016\n",
      "Trainable params: 148,992\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Printing the 'vanilla' convolutional block summary (no. parameters)\n",
    "conv_vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2jL8ptgMr_FO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jL8ptgMr_FO",
    "outputId": "af33d285-e80c-4fa2-af22-4822f2dc98fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 3, 32, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute the filter of the Depth-wise Conv2D\n",
    "# Assuming the 'BHWC' format\n",
    "input_channel_dim = x.get_shape().as_list()[-1]\n",
    "tf.Variable(tf.random.truncated_normal(\n",
    "    shape=(KERNEL_SIZE, KERNEL_SIZE, input_channel_dim, 1)\n",
    ")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "t9Sz_AS3kK0E",
   "metadata": {
    "id": "t9Sz_AS3kK0E"
   },
   "outputs": [],
   "source": [
    "### Testing the 'MobileNet' depth-wise separable convolutional block\n",
    "conv_mobilenet = mobilenet_conv_block(\n",
    "    x=x,\n",
    "    kernel_size=KERNEL_SIZE,    # Should be (3, 3) for MobileNet\n",
    "    output_channels=OUTPUT_CHANNELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wvhgq2sdkKnK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvhgq2sdkKnK",
    "outputId": "1121839a-77f1-478f-c8c4-e9277d499752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " depthwise_conv2d (Depthwise  (1, 256, 256, 32)        320       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (1, 256, 256, 32)        128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (1, 256, 256, 32)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (1, 256, 256, 1)          33        \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (1, 256, 256, 1)         4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (1, 256, 256, 1)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 485\n",
      "Trainable params: 419\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_mobilenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4RD5pZY87Ry8",
   "metadata": {
    "id": "4RD5pZY87Ry8"
   },
   "source": [
    "##### Comparing the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "CFMj6qce7iYN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFMj6qce7iYN",
    "outputId": "48c080e3-6beb-4efb-e869-41e6256d15be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters for 'vanilla' ConvNet block: 150016\n",
      "Total parameters for 'MobileNet' ConvNet block: 485\n",
      "Reduction in parameters with 'MobileNet' ConvNet block: 99.677%\n"
     ]
    }
   ],
   "source": [
    "cv_params = conv_vanilla.count_params()\n",
    "cm_params = conv_mobilenet.count_params()\n",
    "diff_percent = (cv_params - cm_params) / cv_params * 100\n",
    "print(f\"Total parameters for 'vanilla' ConvNet block: {cv_params}\")\n",
    "print(f\"Total parameters for 'MobileNet' ConvNet block: {cm_params}\")\n",
    "print(f\"Reduction in parameters with 'MobileNet' ConvNet block: {diff_percent:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dWQGgYPb7Rsr",
   "metadata": {
    "id": "dWQGgYPb7Rsr"
   },
   "source": [
    "With the MobileNet's depth-wise separable convolutional block, we have a $99.677\\%$ reduction in the total number of parameters in each block when compared to the standard \"vanilla\" convolutional block (i.e., the `Conv2D -> Batch Norm -> ReLU` block).\n",
    "\n",
    "If minimising the total number of parameters is your goal, then the MobileNet architecture is sure to suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db9973",
   "metadata": {
    "id": "24db9973"
   },
   "source": [
    "### 2.3. SSD Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b1ab4e8",
   "metadata": {
    "id": "6b1ab4e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3955c0f",
   "metadata": {
    "id": "b3955c0f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f22d31",
   "metadata": {
    "id": "e6f22d31"
   },
   "source": [
    "### 2.4. Filtering Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b9fcb11",
   "metadata": {
    "id": "0b9fcb11"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a3affc3",
   "metadata": {
    "id": "5a3affc3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e3b6185",
   "metadata": {
    "id": "5e3b6185"
   },
   "source": [
    "### 2.5. Object Detection Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "921de691",
   "metadata": {
    "id": "921de691"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24c320b4",
   "metadata": {
    "id": "24c320b4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac72970d",
   "metadata": {
    "id": "ac72970d"
   },
   "source": [
    "### 2.6. Timing Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "403d2bae",
   "metadata": {
    "id": "403d2bae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dc17783",
   "metadata": {
    "id": "6dc17783"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23b7033d",
   "metadata": {
    "id": "23b7033d"
   },
   "source": [
    "### 2.7. Object Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2846dd45",
   "metadata": {
    "id": "2846dd45"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d8cee3f",
   "metadata": {
    "id": "4d8cee3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c6e83cf",
   "metadata": {
    "id": "5c6e83cf"
   },
   "source": [
    "## 3. Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d7d2f",
   "metadata": {
    "id": "138d7d2f"
   },
   "source": [
    "##### Alternatives\n",
    "* TODO.\n",
    "##### Extensions of task\n",
    "* TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f9ba4",
   "metadata": {
    "id": "550f9ba4"
   },
   "source": [
    "## 4. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728bd96",
   "metadata": {
    "id": "5728bd96"
   },
   "source": [
    "- ⬜️ TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6526376",
   "metadata": {
    "id": "f6526376"
   },
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9606fb",
   "metadata": {
    "id": "9a9606fb"
   },
   "source": [
    "This assignment was prepared by Kelvin Lwin, Andrew Bauman, Dominique Luna et al., 2021 (link [here](https://github.com/udacity/CarND-Object-Detection-Lab)).\n",
    "\n",
    "References\n",
    "* [1] Howard, A. G. et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv. 2017. [doi:10.48550/arXiv.1704.04861](https://arxiv.org/abs/1704.04861).\n",
    "* [2] Shelhamer, E. et al. Fully Convolutional Networks for Semantic Segmentation. arXiv. 2016. [doi:10.48550/arXiv.1605.06211](https://arxiv.org/abs/1605.06211).\n",
    "* [3] Liu, W. et al. SSD: Single Shot MultiBox Detector. European Conference on Computer Vision, ECCV. Lecture Notes in Computer Science, 9905:21-37. 2016. [doi:10.1007/978-3-319-46448-0_2](https://doi.org/10.1007/978-3-319-46448-0_2).\n",
    "\n",
    "Helpful resources:\n",
    "* [`CarND-Object-Detection-Lab` by @udacity | GitHub](https://github.com/udacity/CarND-Object-Detection-Lab);\n",
    "* [3.4. Depthwise Convolution | Dive Into Deep Learning](https://tvm.d2l.ai/chapter_common_operators/depthwise_conv.html);\n",
    "* [Depth-wise Convolution and Depth-wise Separable Convolution by A. Pandey | Medium](https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec);\n",
    "* [Pointwise Convolution by A. Shrivastav | OpenGenus](https://iq.opengenus.org/pointwise-convolution/);\n",
    "* [Depthwise Separable Convolution - A FASTER CONVOLUTION! | YouTube](https://www.youtube.com/watch?v=T7o3xvJLuHk)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
