{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d3c8fa",
   "metadata": {
    "id": "10d3c8fa"
   },
   "source": [
    "# Exercise 1.7.1 — Scene Understanding\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8342f66",
   "metadata": {
    "id": "a8342f66"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8a817",
   "metadata": {
    "id": "3ce8a817"
   },
   "source": [
    "* Compute the Mean Intersection over Union (IoU) of the multi-class segmentation label predictions;\n",
    "* Implement a standard convolution block and compare it to the MobileNet convolution block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a226974",
   "metadata": {
    "id": "9a226974"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e37f8f",
   "metadata": {
    "id": "36e37f8f"
   },
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040b9ad5",
   "metadata": {
    "id": "040b9ad5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3396c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "id": "6f3396c3",
    "outputId": "2f9e10d1-cbb7-46d6-c01e-6b8f7def1e3a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.9.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab345e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "id": "9ab345e3",
    "outputId": "f8ddd501-512e-461e-ecb9-4dd2b5c7baf4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b54c301",
   "metadata": {
    "id": "4b54c301"
   },
   "outputs": [],
   "source": [
    "### Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5279089e",
   "metadata": {
    "id": "5279089e"
   },
   "outputs": [],
   "source": [
    "ENV_COLAB = True                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda135f9",
   "metadata": {
    "id": "eda135f9"
   },
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8bc08a8",
   "metadata": {
    "id": "a8bc08a8"
   },
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c1e4cd",
   "metadata": {
    "id": "e7c1e4cd"
   },
   "outputs": [],
   "source": [
    "### Creating subdirectories (if not exists)\n",
    "os.makedirs(DIR_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e58534",
   "metadata": {
    "id": "21e58534"
   },
   "source": [
    "### 1.1. Scene Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d70ec3",
   "metadata": {
    "id": "73d70ec3"
   },
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd081af",
   "metadata": {
    "id": "3fd081af"
   },
   "source": [
    "TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34dd32",
   "metadata": {
    "id": "5f34dd32"
   },
   "source": [
    "#### Metrics — Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab143bf",
   "metadata": {
    "id": "3ab143bf"
   },
   "source": [
    "In the [very first exercise](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-1-Choosing-Metrics/2022-07-25-Choosing-Metrics-IoU.ipynb) of this course, we covered the Intersection over Union (IoU) metric and its application to the bounding box prediction task. Now, we use the IoU metric again but this time for semantic segmentation and scene understanding.\n",
    "\n",
    "We start with the same general formula for the IoU score given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU} &= \\frac{\\textrm{Area of Intersection}}{\\textrm{Area of Union}},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "but now we calculate the IoU score using the following binary classification metrics:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With this form of the IoU equation, all we need to do is compute the true positive ($\\mathrm{TP}$), true negative ($\\mathrm{TN}$), and the false positive ($\\mathrm{FN}$), false negative ($\\mathrm{FN}$) rates. For the image segmentation task, this boils down to the pixel-wise classification predictions. Thankfully, the algorithms we designed in [Sect. 2.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-1-Choosing-Metrics/2022-07-25-Choosing-Metrics-IoU.ipynb) of Exercise 1.1.1 hold; all we need to do is compute the pixel-wise classification metrics for each class using the same tabular approach as before. With these metrics, we evaluate the $\\mathrm{IoU}$ formula and obtain a score indicating the amount of \"overlap\" between the predicted region and the true region of each segmented object. \n",
    "\n",
    "Let's illustrate this with a simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54cb770",
   "metadata": {
    "id": "d54cb770"
   },
   "source": [
    "```python\n",
    "ground_truth_labels = [\n",
    "    [0, 0, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2], \n",
    "    [3, 3, 3, 3],\n",
    "]\n",
    "predicted_labels = [\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 3, 0, 1],\n",
    "    [2, 2, 2, 3],\n",
    "    [3, 1, 0, 0],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc4fe7",
   "metadata": {
    "id": "69dc4fe7"
   },
   "source": [
    "Above we define a set of _ground-truth_ and _predicted_ labels. Each row in the matrix corresponds to a class; looking at the first row of the `ground_truth_labels` (\"$\\mathrm{A}$\") matrix, we see that class `0` should appear at all four pixel locations. Looking at the first row of `predicted_labels` (\"$\\mathrm{B}$\" matrix), we see instead that only three of the four pixel locations were given a correct prediction of class `0`. In other words, we have in the first row a $\\mathrm{TP} = 3$. Now, we need to compute for class `0` the false positive ($\\mathrm{FP}$) rate. To do this, we examine the _other_ pixel locations (i.e., other rows of the `predicted_labels` matrix), and add up any occurrences of class label `0` where the corresponding entries in `ground_truth_labels` do not match. Since class label `0` was predicted _incorrectly_ at pixel locations $\\mathrm{B}_{2, 3}$, $\\mathrm{B}_{3, 3}$, and $\\mathrm{B}_{4, 4}$, we have a $\\mathrm{FP} = 3$. Now let's complete the calculations for the two other metrics: true negative ($\\mathrm{TN}$) and false negative ($\\mathrm{FN}$). The $\\mathrm{TN}$ value for this problem is easy to compute, since we assume all predictions here were valid (i.e., that we expected a class label to be predicted for every pixel in `predicted_labels`). That means our $\\mathrm{TN} = 0$. Lastly, our $\\mathrm{FN}$ rate is computed as the number of _incorrect_ predictions for class `0`. Looking at the first row of the `predicted_labels` matrix (i.e., the \"predictions\" for class `0`), we count the number of label predictions that are _not_ equal to class `0` to get our false negative rate. With _one_ incorrect class `0` prediction at the first index $\\mathrm{B}_{1, 1} = $ `1`, we have therefore a $\\mathrm{FN} = 1$. \n",
    "\n",
    "With these four classification metrics out of the way, we can obtain the $\\mathrm{IoU}$ score for class `0` as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU}_{0} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}} = \\frac{3}{3 + 3 + 1 + 0} = \\frac{3}{7} \\approx 0.4286.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now that we have the $\\mathrm{IoU}$ score for class `0` computed, we repeat the process for the other three rows (classes) in `predicted_labels` to obtain each classes' respective $\\mathrm{IoU}$ score. Once we have completed the calculations of all four classes, we can take the average to obtain the $\\mathrm{IoU}_{\\textrm{mean}}$, as simply:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU}_{\\textrm{mean}} &= \\frac{1}{n}\\sum_{i=0}^{n} \\mathrm{IoU}_{i},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is nothing but the sum of the per-class $\\mathrm{IoU}$ scores divided by the total number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ke35mrjgWjFL",
   "metadata": {
    "id": "ke35mrjgWjFL"
   },
   "source": [
    "### 1.2. Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e37fc7",
   "metadata": {
    "id": "55e37fc7"
   },
   "source": [
    "[Convolution](https://en.wikipedia.org/wiki/Convolution) is a measure of overlap between two functions as one slides over the other. Mathematically, it is a sum of products given by the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(f * g\\right)\\left(t\\right) := \\int_{-\\infty}^{\\infty} f\\left(\\tau\\right)\\cdot g\\left(t-\\tau\\right)d\\tau.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layers) in a neural network performs the convolution operation by applying a filter over the input tensor. After the same filter is repeatedly applied to the input, a feature map is created which shows the positions and intensity of a detected feature in an input. \n",
    "\n",
    "Convolutional layers are extremely useful in neural networks, but they often come with a high computational cost due to the number of parameters required for each input (and each channel of the input). In this section we will look at the basics of regular convolution and review the number of parameters required. Then, we will introduce several alternative convolution methods — namely, depth-wise, depth-wise separable and point-wise convolution, which help reduce this computational cost. These alternatives are especially useful for networks intended to be run on mobile devices and embedded hardware where computational resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B3WcU3QlWi5j",
   "metadata": {
    "id": "B3WcU3QlWi5j"
   },
   "source": [
    "#### Regular Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_hk_pcnXFbL",
   "metadata": {
    "id": "H_hk_pcnXFbL"
   },
   "source": [
    "In a typical [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layers), we have a set of $\\mathrm{N}$ kernels, each with a size of $\\mathrm{D}_{k} * \\mathrm{D}_{k}$. Each of these kernels convolves (\"slides over\") the entire input, which is a $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$ sized feature map (a tensor). When considering the computational cost of a convolution operation, we must consider the number of _parameters_ required for each convolutional layer. Generally, models with more convolutional layers have more parameters, and therefore require more computational resources to use. While this can lead to higher accuracy in image processing tasks, special attention needs to be paid to the computational cost of convolutional networks when utilising low-end hardware, such as with in-vehicle embedded devices which will be powering these type of networks with real-time inference demands.\n",
    "\n",
    "To understand the computational cost of a regular convolutional layer, we have the following cost:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N} * \\mathrm{D}_{k} * \\mathrm{D}_{k},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{D}_{g} * \\mathrm{D}_{g}$ is the size of the output feature map. A regular convolution takes in a $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$ input feature map and returns a $\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{N}$ feature map as output.\n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008yIebXacAE",
   "metadata": {
    "id": "008yIebXacAE"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-1-Standard-Convolution-Filters.png\" alt=\"Figure 1. Filters in a regular convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 1. Filters in a regular convolutional layer (credit: Howard et al., 2017).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_C3E7_bbOyD",
   "metadata": {
    "id": "H_C3E7_bbOyD"
   },
   "source": [
    "#### Depth-wise Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pXNWwOkWbPSL",
   "metadata": {
    "id": "pXNWwOkWbPSL"
   },
   "source": [
    "[Depth-wise convolution](https://tvm.d2l.ai/chapter_common_operators/depthwise_conv.html) and depth-wise separable convolution are two atypical convolution operations that have less parameters and therefore require less computational power to compute. Depth-wise convolutions are used in the [MobileNets](https://arxiv.org/abs/1704.04861) [1] architecture designed for mobile and embedded applications.\n",
    "\n",
    "Depth-wise convolution acts on each input channel separately with a different kernel for each. With a number of input channels $\\mathrm{M}$ we have $\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}$ kernels. Since depth-wise convolution only acts on a single input channel at a time, the kernel depth $\\mathrm{N}$ is set equal to $1$. Therefore, the computational cost of the depth-wise convolution is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To produce the same effect with regular convolution, each channel of the input requires its own kernel. To compute the convolution, each channel is selected individually, and all elements in the kernel are set to zero except for those corresponding to the respective input channel. The final output of the stacked convolutions is one $\\mathrm{M}$-channel output feature map. As shown above, depth-wise convolution reduces the number of parameters required by a factor $\\mathrm{N}$, i.e., the number of filters required _per input channel_. For a three-channel input, we require _three times less_ number of parameters with the depth-wise convolution approach.   \n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857dff0",
   "metadata": {
    "id": "d857dff0"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-2-Depthwise-Convolution-Filters.png\" alt=\"Figure 2. Filters in a depth-wise convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 2. Filters in a depth-wise convolutional layer (credit: Howard et al., 2017).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c2bbf",
   "metadata": {
    "id": "0a2c2bbf"
   },
   "source": [
    "In order to understand depth-wise separable convolution, we first introduce the point-wise convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a30c37",
   "metadata": {
    "id": "40a30c37"
   },
   "source": [
    "#### Point-wise Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5837bf",
   "metadata": {
    "id": "0e5837bf"
   },
   "source": [
    "Point-wise convolution is a form of convolution that applies a $1\\times 1$ kernel across an input. Unlike the depth-wise convolution, the $1\\times 1$ kernel used here has a depth equal to the number of channels in the input. The computational complexity of the point-wise convolution is similar to the regular convolution, but instead the filter size $\\mathrm{D}_{k} * \\mathrm{D}_{k}$ is equal to $1 \\times 1$. Therefore, we have the following number of parameters:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f85ba",
   "metadata": {
    "id": "d32f85ba"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-3-Pointwise-Convolution-Filters.png\" alt=\"Figure 3. Filters in a point-wise convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 3. Filters in a point-wise convolutional layer (credit: Howard et al., 2017).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13941d",
   "metadata": {
    "id": "7a13941d"
   },
   "source": [
    "#### Depth-wise Separable Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cb534",
   "metadata": {
    "id": "942cb534"
   },
   "source": [
    "Point-wise convolution is used in conjunction with depth-wise convolution in order to perform depth-wise separable convolution. Here, depth-wise separable convolution borrows the idea that the depth and the spatial dimension of a filter can be separated, as is the case with e.g., the [Sobel](https://en.wikipedia.org/wiki/Sobel_operator) filter for edge detection. The depth-wise separable convolution separates a kernel into two independent kernels, each of which performs two convolutions: the depth-wise and the point-wise convolution. Thus, the total computational cost for the depth-wise separable convolution is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right) + \\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which results in a $\\frac{1}{\\mathrm{N}} + \\frac{1}{\\mathrm{D}_{k} * \\mathrm{D}_{k}}$ reduction in total computation, which can be observed in the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right) + \\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right)}{\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N} * \\mathrm{D}_{k} * \\mathrm{D}_{k}} &= \\frac{\\mathrm{D}_{k}^{2} + \\mathrm{N}}{\\mathrm{D}_{k}^{2} * \\mathrm{N}} = \\frac{1}{\\mathrm{N}} + \\frac{1}{\\mathrm{D}_{k}^{2}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $\\mathrm{N}$ is selected to be large enough, depth-wise separable convolution networks can be immensely more computationally efficient. For example, the MobileNet architecture uses a $3\\times 3$ kernel, which results in $\\sim 9\\mathrm{x}$ efficiency improvement over regular convolution layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff911d9",
   "metadata": {
    "id": "0ff911d9"
   },
   "source": [
    "#### Width Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744d977",
   "metadata": {
    "id": "b744d977"
   },
   "source": [
    "In addition to utilising alternative convolution methods, scaling the number of input and output channels proportional to a _width multiplier_ is often performed. The width multiplier $\\alpha$ is a hyperparameter set to a value in the range $\\alpha \\in \\left[0, 1\\right]$. This results in a computational cost:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\alpha\\mathrm{N}\\right).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d363e25",
   "metadata": {
    "id": "4d363e25"
   },
   "source": [
    "#### Resolution Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9984092",
   "metadata": {
    "id": "e9984092"
   },
   "source": [
    "Similar to the width multiplier, a _resolution multiplier_ is used to scale the size of the input feature map. The resolution multiplier $\\rho$ is a hyperparameter set to a value in the range $\\rho \\in \\left[0, 1\\right]$. This results in a computational cost:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\mathrm{M} * \\mathrm{N}\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Combining the width and resolution multipliers results in a scaled computational cost of:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}\\right) + \\left(\\rho\\mathrm{D}_{f} * \\rho\\mathrm{D}_{f} * \\alpha\\mathrm{M} * \\alpha\\mathrm{N}\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the MobileNets architecture, for example, these values of $\\alpha$ and $\\rho$ are selected w.r.t. the speed versus accuracy versus size trade-off. In the paper by original author Howard et al., 2017, the authors found that the resolution multiplier has the effect of reducing computational cost by $\\rho^{2}$, whereas the width multiplier has the effect of reducing computational cost and number of parameters quadratically by roughly $\\alpha^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaa112",
   "metadata": {
    "id": "7cfaa112"
   },
   "source": [
    "### 1.3. Convolutional Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9769c0",
   "metadata": {
    "id": "4d9769c0"
   },
   "source": [
    "#### FCN-8 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cc7a7",
   "metadata": {
    "id": "066cc7a7"
   },
   "source": [
    "The [FCN-8]() architecture by Long et al., 2014 [2] is an architecture that uses $1\\times 1$ convolutional layers to replace the fully-connecte—d layers of a standard neural network. As a result, the FCN-8 architecture is able to preserve spatial information of the input tensor and perform the down-sampling and feature extraction routines of a convolutional network.\n",
    "\n",
    "Fully-Convolutional Network (FCN) architectures have two primary components — an _encoder_ and a _decoder_. The encoder extracts features from an image using a series of sliding window convolution operations. The decoder in an FCN is used to up-scale the down-sampled intermediate feature maps generated from the encoder to a higher resolution — usually matching the original input dimensions. In the FCN-8 architecture, the encoder block is a set of $1\\times 1$ convolution layers. The decoder block of the FCN-8 is a set of transposed convolution layers which upsample the feature maps to the size of the original input. This process is usually referred to as \"reverse convolution\" or deconvolution since its effect is essentially reversing (with some loss) the downsampling of the input. \n",
    "\n",
    "In order to preserve fine-grained segmentation maps through the network to the decoder block, a set of skip _connections_ are used. Essentially, these \"connections\" between non-adjacent layers of differing resolutions help retain information from the original input by combining the output feature maps of each respective layer using an element-wise addition operation. As a result, the FCN-8 with skip connections is able to use information from multiple resolutions to make more precise segmentation decisions. Skip connections, along with the other advancements from the FCN-8 architecture, have proven successful in empirical studies between the FCN-8 and its \"sister\" networks — the FCN-16 and FCN-32. For more information on Fully-Convolutional Networks, see the [notebook from the previous lesson](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/1.7/1-Computer-Vision/Exercises/1-6-1-Fully-Convolutional-Networks/2023-01-23-Fully-Convolutional-Networks.ipynb).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81567d69",
   "metadata": {
    "id": "81567d69"
   },
   "source": [
    "#### MobileNets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4bcc5",
   "metadata": {
    "id": "38a4bcc5"
   },
   "source": [
    "The [MobileNets](https://arxiv.org/abs/1704.04861) architecture by Howard et al., 2017 [1] is an architecture that uses depth-wise separable convolutions to build light-weight deep neural networks. The MobileNets architecture, as the name suggests, is designed to run object detection and classification tasks efficiently (i.e., with high FPS and low memory footprint) on mobile and embedded devices. The MobileNets architecture achieves this in a three-part approach:\n",
    "1. **Depth-wise separable convolutions** — Perform a depth-wise convolution followed by a $1\\times 1$ convolution (instead of a standard convolution). The $1\\times 1$ convolution is called a point-wise convolution if it follows after a depth-wise convolution;\n",
    "2. **Width multipliers** — Reduces the size of the input / output channels using a scaling factor set to a value between $0.0$ and $1.0$;\n",
    "3. **Resolution multipliers** — Reduces the size of the original input using a scaling factor set to a value between $0.0$ and $1.0$.\n",
    "\n",
    "These three techniques reduce the cummulative number of parameters in the network and therefore the amount of computation required. The downside to models exploiting the parameter reduction approach is that accuracy is often the trade-off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XGkWAi-UUik0",
   "metadata": {
    "id": "XGkWAi-UUik0"
   },
   "source": [
    "#### Single Shot Detector (SSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VPNl0saRUiVs",
   "metadata": {
    "id": "VPNl0saRUiVs"
   },
   "source": [
    "Many of the earlier deep neural network architectures involved networks with more than one training phase; the [Faster-RCNN](https://arxiv.org/abs/1506.01497) for example, first trains a Region Proposal Network (RPN) which is then merged with a pre-trained classification sub-network. The [Single Shot Detector](https://arxiv.org/abs/1512.02325) (SSD) by Liu et al., 2015 [3] combines these two sub-networks into a single-pass network that predicts bounding box locations and classifies the corresponding object classes. The major difference with single-shot networks is that they can be trained end-to-end, whereas architectures with multiple sub-networks, such as the Faster-RCNN, must train each module separately. The following is an outline of the original SSD architecture proposed by Liu et al., 2015 [3]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xRRThpEkUiId",
   "metadata": {
    "id": "xRRThpEkUiId"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-4-Single-Shot-Detector-Network-Architecture.png\" alt=\"Figure 4. Architecture of the Single Shot Detector (SSD) proposed by Liu et al., 2015.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 4. Architecture of the Single Shot Detector (SSD) proposed by Liu et al., 2015.}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7uqbGoWrUh72",
   "metadata": {
    "id": "7uqbGoWrUh72"
   },
   "source": [
    "In the above architecture, we note the use of the VGG-16 pre-trained convolutional base. In this notebook, we will instead be using the MobileNet pre-trained base from Howard et al., 2017 [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tK3_wxGcYeKm",
   "metadata": {
    "id": "tK3_wxGcYeKm"
   },
   "source": [
    "##### Bounding box detection with SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qCjGDPsYcEN",
   "metadata": {
    "id": "5qCjGDPsYcEN"
   },
   "source": [
    "SSD operates on feature maps to predict bounding box locations. Recall a feature map of $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$. For each feature map location, $k$ bounding boxes are predicted. Each bounding box carries with it the following information:\n",
    "* $\\left(\\mathrm{c}x, \\mathrm{c}y, w, h\\right)$ — Four bounding box corner offset locations;\n",
    "* $C = \\left(c_{1}, c_{2},\\ldots, c_{p}\\right)$ — class probabilities.\n",
    "\n",
    "The SSD does not predict the _shape_ of the box but rather the location of where the box is in the image. The $k$ bounding boxes each have a pre-determined shape (i.e., the anchors). This is illustrated in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f6960",
   "metadata": {},
   "source": [
    "<img src=\"figures/2023-01-26-Figure-5-Bounding-Box-Internal-Representation-with-SSD.png\" alt=\"Figure 5. Internal representation of bounding boxes using anchors with the Single Shot Detector (SSD) network.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 5. Internal representation of bounding boxes using anchors in Single Shot Detector (SSD) network.}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac4505",
   "metadata": {},
   "source": [
    "The anchor boxes used in the SSD have coordinates that are manually configured prior to training. Shown in Figure 5(c) is a set of $k = 4$ anchor boxes of varying size used to isolate an object for detection.\n",
    "\n",
    "In order to filter nonsensical bounding boxes, we use a loss function. For the final set of $N$ matched boxes, we compute the loss as:\n",
    "$$\n",
    "\\begin{align}\n",
    "L &= \\frac{1}{N}\\left(L_{\\textrm{class}} + L_{\\textrm{box}}\\right),\n",
    "\\end{align}\n",
    "$$\n",
    "where $L_{\\textrm{class}}$ is a softmax loss for classification, and $L_{\\textrm{box}}$ is an L1 smooth loss representing the error of the matched boxes to the ground-truth boxes. Note that L1 smooth loss is a modification of the standard L1 loss which is more robust to outliers. Also note that when $N = 0$, the loss is set to $0.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d3623",
   "metadata": {},
   "source": [
    "##### In Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29986c7e",
   "metadata": {},
   "source": [
    "* A pre-trained convolutional base is used (e.g., VGG-16 or MobileNet);\n",
    "* The base model is extended with several convolutional blocks;\n",
    "* Each feature map is used to predict bounding boxes, and therefore diversity in feature map size allows for object detection at different resolutions;\n",
    "* Boxes are filtered by the Intersection over Union (IoU) metric and with hard negative mining (looking for difficult-to-detect examples);\n",
    "* The loss functions are softmax for classification and smooth L1 for detection;\n",
    "* The entire SSD network can be trained end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b337dc",
   "metadata": {
    "id": "19b337dc"
   },
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76f553",
   "metadata": {
    "id": "9d76f553"
   },
   "source": [
    "### 2.1. Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0509c31",
   "metadata": {
    "id": "e0509c31"
   },
   "source": [
    "Here we use the TensorFlow [`tf.keras.metrics.MeanIoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU) function to compute the mean Intersection over Union (IoU) across all classes $i=0,\\ldots, n$.\n",
    "\n",
    "In order to use the metric as a standalone function, we have to first initialise the respective [`tf.keras.metrics.Metric`](https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/metrics/Metric) subclass instance (i.e., `MeanIoU`), then perform a single \"state update\" using the [`update_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#update_state) class method. As arguments to this function, we pass in the `y_true` and `y_pred` tensors that we wish to evaluate. Optionally, we can provide a `sample_weight` scalar value or vector of rank equal to `y_true`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1588d58",
   "metadata": {
    "id": "f1588d58"
   },
   "outputs": [],
   "source": [
    "### Defining the number of distinct class labels (i.e., classes)\n",
    "N_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89c5313",
   "metadata": {
    "id": "d89c5313"
   },
   "outputs": [],
   "source": [
    "### Initialising the `tf.keras.metrics.Metric` instance\n",
    "iou_mean = tf.keras.metrics.MeanIoU(\n",
    "    num_classes=N_CLASSES,\n",
    "    name='Mean IoU for multi-class object segmentation data',\n",
    "    dtype=tf.dtypes.float32,\n",
    "    ### Additional arguments for TF2.10+ API:\n",
    "    #ignore_class=None,\n",
    "    #sparse_y_true=True,    # `True` if class labels are integers, `False` if floating-point\n",
    "    #sparse_y_pred=True,\n",
    "    #axis=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aEPRhQllMLHG",
   "metadata": {
    "id": "aEPRhQllMLHG"
   },
   "source": [
    "#### Testing the `MeanIoU` metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "JucYiLeoMMG1",
   "metadata": {
    "id": "JucYiLeoMMG1"
   },
   "outputs": [],
   "source": [
    "### Defining our prediction and ground-truth sets\n",
    "ground_truth_labels = [\n",
    "    [0, 0, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2], \n",
    "    [3, 3, 3, 3],\n",
    "]\n",
    "predicted_labels = [\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 3, 0, 1],\n",
    "    [2, 2, 2, 3],\n",
    "    [3, 1, 0, 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54WZKwK2Mgb9",
   "metadata": {
    "id": "54WZKwK2Mgb9"
   },
   "outputs": [],
   "source": [
    "### Converting the matrices to n-rank tensors\n",
    "y_true = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        ground_truth_labels).reshape(1, -1, len(ground_truth_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "y_pred = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        predicted_labels).reshape(1, -1, len(predicted_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "t8Fwb-ZxNsaA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8Fwb-ZxNsaA",
    "outputId": "e9ec41b3-fcea-47f7-dd1c-def61600b5af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41964284"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Computing the mean IoU\n",
    "iou_mean.update_state(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred\n",
    ")\n",
    "iou_mean.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFL3s0eqMK9W",
   "metadata": {
    "id": "SFL3s0eqMK9W"
   },
   "source": [
    "As shown above, we obtain a mean IoU score for the set of predictions of $\\mathrm{IoU}_{\\textrm{mean}} \\ \\approx 0.420$, which matches our expected value for this test set.\n",
    "\n",
    "Now, we repeat the Mean IoU calculation using a second test set of predictions. Note that [`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) instances are stateful by default, meaning that each call to `update_state()` computes the Mean IoU for the input _mini-batch of predictions / ground-truth labels_. In other words, the default behaviour for this `Metric` instance is to accumulate the Mean IoU score across calls to `update_state()`. Since we are computing the Mean IoU score of the full batch (i.e., `batch_size=1`) manually, we must use the [`reset_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#reset_state) method before performing the IoU calculation with [`update_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#update_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "V5vt1MGZSpbQ",
   "metadata": {
    "id": "V5vt1MGZSpbQ"
   },
   "outputs": [],
   "source": [
    "### Defining our new prediction set (assuming same `ground_truth_labels`)\n",
    "predicted_labels = [\n",
    "    [0, 0, 0, 0],\n",
    "    [1, 0, 0, 1],\n",
    "    [1, 2, 2, 1],\n",
    "    [3, 3, 0, 3],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "KOVETdBRU0Iu",
   "metadata": {
    "id": "KOVETdBRU0Iu"
   },
   "outputs": [],
   "source": [
    "### Converting the matrix to an n-rank tensor\n",
    "y_pred = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        predicted_labels).reshape(1, -1, len(predicted_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "RmADU5HjSpTZ",
   "metadata": {
    "id": "RmADU5HjSpTZ"
   },
   "outputs": [],
   "source": [
    "### Resetting the state of the `MeanIoU` instance\n",
    "# i.e., current cumulative mean IoU becomes `0.0`\n",
    "iou_mean.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "QauTkcrRSpKm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QauTkcrRSpKm",
    "outputId": "2f7b4a58-8e5c-4d4b-e070-8ad4a2533d24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53869045"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Computing the new mean IoU\n",
    "iou_mean.update_state(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred\n",
    ")\n",
    "iou_mean.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M9kVfx95VQ48",
   "metadata": {
    "id": "M9kVfx95VQ48"
   },
   "source": [
    "As shown above, we obtain a new mean IoU score for this second test of predictions of $\\mathrm{IoU}_{\\textrm{mean}} \\ \\approx 0.539$, which matches our expected value for this second test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0ea8b",
   "metadata": {
    "id": "b0c0ea8b"
   },
   "source": [
    "### 2.2. Separable Depthwise Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00310c4d",
   "metadata": {
    "id": "00310c4d"
   },
   "source": [
    "NOTE: the code provided here has been migrated to the TensorFlow 2.x API. Some functionality may differ from the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7e989",
   "metadata": {
    "id": "d6b7e989"
   },
   "source": [
    "Here we implement the a MobileNets depth-wise separable convolution block using the following [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) components:\n",
    "* [`tf.nn.depthwise_conv2d`](https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d);\n",
    "* [`tf.keras.layers.BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization);\n",
    "* [`tf.keras.layers.ReLU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU);\n",
    "* [`tf.keras.layers.Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\n",
    "\n",
    "\n",
    "We will then compare the number of parameters of the depth-wise separable convolution block to a regular convolution block, which is formed using the following [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) components:\n",
    "* [`tf.keras.layers.Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layer;\n",
    "* [`tf.keras.layers.BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e89e05aa",
   "metadata": {
    "id": "e89e05aa"
   },
   "outputs": [],
   "source": [
    "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce9aba00",
   "metadata": {
    "id": "ce9aba00"
   },
   "outputs": [],
   "source": [
    "def vanilla_conv_block(\n",
    "        x: Union[np.ndarray, tf.Tensor], \n",
    "        kernel_size: Union[Tuple[int], List[int]],\n",
    "        output_channels: int\n",
    ") -> tf.keras.models.Sequential:\n",
    "    \"\"\"Implements a vanilla (regular) convolution block.\n",
    "    \n",
    "    A convolution block here is defined as the following:\n",
    "        Vanilla Conv -> Batch Norm -> ReLU,\n",
    "    where 'Vanilla Conv' corresponds to the `Conv2D` layer\n",
    "    provided in the TensorFlow 2.x API.\n",
    "    \n",
    "    :param x: Input tensor used to build the convolutional block,\n",
    "        i.e., input shape of `x` is provided for delayed-build pattern.\n",
    "    :param kernel_size: Kernel size to assign the convolutional layer.\n",
    "    :param output_channels: Depth of the output tensor.\n",
    "    :returns: The 'vanilla' convolutional block,\n",
    "        buit for input tensors of shape given by `x`.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Build the convolutional block\n",
    "    conv = tf.keras.models.Sequential() \n",
    "    conv.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=output_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=(2, 2),\n",
    "            padding='SAME'\n",
    "        )\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.ReLU()\n",
    "    )\n",
    "    ### Build the block by setting expected input shape to shape of tensor `x`\n",
    "    conv.build(x.shape)\n",
    "    ### Return the built convolutional block\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8849feff",
   "metadata": {
    "id": "8849feff"
   },
   "outputs": [],
   "source": [
    "def mobilenet_conv_block(\n",
    "        x: Union[np.ndarray, tf.Tensor], \n",
    "        kernel_size: Union[Tuple[int], List[int]], \n",
    "        output_channels: int\n",
    ") -> tf.keras.models.Sequential:\n",
    "    \"\"\"Implements a depth-wise separable convolution block (Howard, 2017).\n",
    "\n",
    "    A depth-wise separable convolution block is defined as the following:\n",
    "        Depth-wise -> Batch Norm -> ReLU -> Point-wise -> Batch Norm -> ReLU,\n",
    "    where the 'Point-wise' is implemented using the `Conv2D` layer provided\n",
    "    in the TensorFlow 2.x API.\n",
    "\n",
    "    :param x: Input tensor used to build the MobileNet convolutional block,\n",
    "        i.e., input shape of `x` is provided for delayed-build pattern.\n",
    "    :param kernel_size: Kernel size to assign the convolutional layers.\n",
    "    :param output_channels: Depth of the output tensor\n",
    "    :returns: The 'MobileNet' convolutional block,\n",
    "        built for input tensors of shape given by `x`.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Build the MobileNet convolutional block\n",
    "    conv = tf.keras.models.Sequential()\n",
    "    conv.add(\n",
    "        # Should have 3x3 kernel\n",
    "        tf.keras.layers.DepthwiseConv2D(\n",
    "            kernel_size=KERNEL_SIZE,\n",
    "            strides=(1, 1),\n",
    "            padding='SAME'\n",
    "        )\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.ReLU()\n",
    "    )\n",
    "    # Should have 1x1 kernel\n",
    "    conv.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "        )\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "    )\n",
    "    conv.add(\n",
    "        tf.keras.layers.ReLU()\n",
    "    )\n",
    "    ### Build the block by setting expected input shape to shape of tensor `x`\n",
    "    conv.build(x.shape)\n",
    "    ### Return the built MobileNet convolutional block\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceae4f5",
   "metadata": {
    "id": "1ceae4f5"
   },
   "source": [
    "#### Testing the MobileNets convolutional block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04345315",
   "metadata": {
    "id": "04345315"
   },
   "outputs": [],
   "source": [
    "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da2JH9neiom9",
   "metadata": {
    "id": "da2JH9neiom9"
   },
   "outputs": [],
   "source": [
    "### Setting the input parameters\n",
    "INPUT_CHANNELS = 32\n",
    "OUTPUT_CHANNELS = 512\n",
    "KERNEL_SIZE = 3\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "GNrrlU1pioiv",
   "metadata": {
    "id": "GNrrlU1pioiv"
   },
   "outputs": [],
   "source": [
    "### Creating the input tensor\n",
    "x = tf.constant(\n",
    "    np.random.rand(1, IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS),\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "U3z6To3Ii7dl",
   "metadata": {
    "id": "U3z6To3Ii7dl"
   },
   "outputs": [],
   "source": [
    "### Testing the 'vanilla' convolutional block\n",
    "conv_vanilla = vanilla_conv_block(\n",
    "    x=x,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    output_channels=OUTPUT_CHANNELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "oeycDPLPjwCo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeycDPLPjwCo",
    "outputId": "f8fb1997-e2d8-45aa-e228-52f7ddda6b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (1, 128, 128, 512)        147968    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (1, 128, 128, 512)       2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (1, 128, 128, 512)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 150,016\n",
      "Trainable params: 148,992\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Printing the 'vanilla' convolutional block summary (no. parameters)\n",
    "conv_vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2jL8ptgMr_FO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jL8ptgMr_FO",
    "outputId": "af33d285-e80c-4fa2-af22-4822f2dc98fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 3, 32, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute the filter of the Depth-wise Conv2D\n",
    "# Assuming the 'BHWC' format\n",
    "input_channel_dim = x.get_shape().as_list()[-1]\n",
    "tf.Variable(tf.random.truncated_normal(\n",
    "    shape=(KERNEL_SIZE, KERNEL_SIZE, input_channel_dim, 1)\n",
    ")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "t9Sz_AS3kK0E",
   "metadata": {
    "id": "t9Sz_AS3kK0E"
   },
   "outputs": [],
   "source": [
    "### Testing the 'MobileNet' depth-wise separable convolutional block\n",
    "conv_mobilenet = mobilenet_conv_block(\n",
    "    x=x,\n",
    "    kernel_size=KERNEL_SIZE,    # Should be (3, 3) for MobileNet\n",
    "    output_channels=OUTPUT_CHANNELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wvhgq2sdkKnK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvhgq2sdkKnK",
    "outputId": "1121839a-77f1-478f-c8c4-e9277d499752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " depthwise_conv2d (Depthwise  (1, 256, 256, 32)        320       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (1, 256, 256, 32)        128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (1, 256, 256, 32)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (1, 256, 256, 1)          33        \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (1, 256, 256, 1)         4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (1, 256, 256, 1)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 485\n",
      "Trainable params: 419\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_mobilenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4RD5pZY87Ry8",
   "metadata": {
    "id": "4RD5pZY87Ry8"
   },
   "source": [
    "##### Comparing the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "CFMj6qce7iYN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFMj6qce7iYN",
    "outputId": "48c080e3-6beb-4efb-e869-41e6256d15be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters for 'vanilla' ConvNet block: 150016\n",
      "Total parameters for 'MobileNet' ConvNet block: 485\n",
      "Reduction in parameters with 'MobileNet' ConvNet block: 99.677%\n"
     ]
    }
   ],
   "source": [
    "cv_params = conv_vanilla.count_params()\n",
    "cm_params = conv_mobilenet.count_params()\n",
    "diff_percent = (cv_params - cm_params) / cv_params * 100\n",
    "print(f\"Total parameters for 'vanilla' ConvNet block: {cv_params}\")\n",
    "print(f\"Total parameters for 'MobileNet' ConvNet block: {cm_params}\")\n",
    "print(f\"Reduction in parameters with 'MobileNet' ConvNet block: {diff_percent:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dWQGgYPb7Rsr",
   "metadata": {
    "id": "dWQGgYPb7Rsr"
   },
   "source": [
    "With the MobileNet's depth-wise separable convolutional block, we have a $99.677\\%$ reduction in the total number of parameters in each block when compared to the standard \"vanilla\" convolutional block (i.e., the `Conv2D -> Batch Norm -> ReLU` block).\n",
    "\n",
    "If minimising the total number of parameters is your goal, then the MobileNet architecture is sure to suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b6185",
   "metadata": {
    "id": "5e3b6185"
   },
   "source": [
    "### 2.3. Object Detection Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d412a0c",
   "metadata": {
    "id": "921de691"
   },
   "source": [
    "In this section we will detect objects using an object detection model and its pre-trained weights made available at the [TensorFlow Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). All models and weights provided by the TensorFlow team have been pre-trained on the [COCO 2017](http://cocodataset.org/) dataset. The TensorFlow team even provides documentation regarding the use of the [Zoo models on mobile devices](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md).\n",
    "\n",
    "Here we will be experimenting with the following set of pre-trained models:\n",
    "* [SSD MobileNet V1 FPN 640x640](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz);\n",
    "* [RFCN ResNet101](http://download.tensorflow.org/models/object_detection/rfcn_resnet101_coco_11_06_2017.tar.gz) — DEPRECATED (archive [here](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/rfcn-resnet101-coco-tf));\n",
    "* [Faster R-CNN Inception ResNet V2 640x640](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017.tar.gz).\n",
    "\n",
    "Note that the above links are for the 11.6.17 versions and are intended for use with TensorFlow v1 models. Since we are instead going to be using the TensorFlow 2.x API, it's best to download these files instead from the [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
    "\n",
    "Each of these three models produces a set of regressed bounding box coordinates as outputs. We will define here a set of utility / helper functions which:\n",
    "1. Filter detected bounding boxes;\n",
    "2. Convert the detected bounding boxes to the original image coordinates;\n",
    "3. Render the converted bounding boxes onto the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24171f4e",
   "metadata": {
    "id": "24c320b4"
   },
   "source": [
    "#### Setting up the TensorFlow Object Detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06470d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clone the `tensorflow/models` repository\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "    while \"models\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "    !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e448166",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install the TF Object Detection API\n",
    "%%bash\n",
    "cd models/research\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required Python modules\n",
    "import io\n",
    "import maptlotlib.pyplot as plt\n",
    "from PIL import Image, ImageColor, ImageDraw, ImageFront\n",
    "from scipy.stats import norm\n",
    "import scipy.misc\n",
    "from six import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dff3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import TensorFlow modules\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f1658",
   "metadata": {},
   "source": [
    "#### Defining the utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From TensorFlow's `inference_tf2_colab.ipynb`\n",
    "# Credit: https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "  Args:\n",
    "    path: the file path to the image\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "  image = Image.open(BytesIO(img_data))\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def get_keypoint_tuples(eval_config):\n",
    "  \"\"\"Return a tuple list of keypoint edges from the eval config.\n",
    "  \n",
    "  Args:\n",
    "    eval_config: an eval config containing the keypoint edges\n",
    "  \n",
    "  Returns:\n",
    "    a list of edge tuples, each in the format (start, end)\n",
    "  \"\"\"\n",
    "  tuple_list = []\n",
    "  kp_list = eval_config.keypoint_edge\n",
    "  for edge in kp_list:\n",
    "    tuple_list.append((edge.start, edge.end))\n",
    "  return tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786fe594",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOUR_LIST = sorted([c for c in ImageColor.colormap.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Colors (one for each class)\n",
    "cmap = ImageColor.colormap\n",
    "print(\"Number of colors =\", len(cmap))\n",
    "COLOR_LIST = sorted([c for c in cmap.keys()])\n",
    "\n",
    "#\n",
    "# Utility funcs\n",
    "#\n",
    "\n",
    "def filter_boxes(min_score, boxes, scores, classes):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "    n = len(classes)\n",
    "    idxs = []\n",
    "    for i in range(n):\n",
    "        if scores[i] >= min_score:\n",
    "            idxs.append(i)\n",
    "    \n",
    "    filtered_boxes = boxes[idxs, ...]\n",
    "    filtered_scores = scores[idxs, ...]\n",
    "    filtered_classes = classes[idxs, ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes\n",
    "\n",
    "def to_image_coords(boxes, height, width):\n",
    "    \"\"\"\n",
    "    The original box coordinate output is normalized, i.e [0, 1].\n",
    "    \n",
    "    This converts it back to the original coordinate based on the image\n",
    "    size.\n",
    "    \"\"\"\n",
    "    box_coords = np.zeros_like(boxes)\n",
    "    box_coords[:, 0] = boxes[:, 0] * height\n",
    "    box_coords[:, 1] = boxes[:, 1] * width\n",
    "    box_coords[:, 2] = boxes[:, 2] * height\n",
    "    box_coords[:, 3] = boxes[:, 3] * width\n",
    "    \n",
    "    return box_coords\n",
    "\n",
    "def draw_boxes(image, boxes, classes, thickness=4):\n",
    "    \"\"\"Draw bounding boxes on the image\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for i in range(len(boxes)):\n",
    "        bot, left, top, right = boxes[i, ...]\n",
    "        class_id = int(classes[i])\n",
    "        color = COLOR_LIST[class_id]\n",
    "        draw.line([(left, top), (left, bot), (right, bot), (right, top), (left, top)], width=thickness, fill=color)\n",
    "        \n",
    "def load_graph(graph_file):\n",
    "    \"\"\"Loads a frozen inference graph\"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(graph_file, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8e503",
   "metadata": {},
   "source": [
    "#### Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detection_graph = load_graph(SSD_GRAPH_FILE)\n",
    "# detection_graph = load_graph(RFCN_GRAPH_FILE)\n",
    "# detection_graph = load_graph(FASTER_RCNN_GRAPH_FILE)\n",
    "\n",
    "# The input placeholder for the image.\n",
    "# `get_tensor_by_name` returns the Tensor with the associated name in the Graph.\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Each box represents a part of the image where a particular object was detected.\n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represent how level of confidence for each of the objects.\n",
    "# Score is shown on the result image, together with the class label.\n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "\n",
    "# The classification of the object (integer id).\n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa4a1b",
   "metadata": {},
   "source": [
    "#### Running inference on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `CarND-Object-Detection-Lab.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49218c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image.\n",
    "image = Image.open('./assets/sample1.jpg')\n",
    "image_np = np.expand_dims(np.asarray(image, dtype=np.uint8), 0)\n",
    "\n",
    "with tf.Session(graph=detection_graph) as sess:                \n",
    "    # Actual detection.\n",
    "    (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], \n",
    "                                        feed_dict={image_tensor: image_np})\n",
    "\n",
    "    # Remove unnecessary dimensions\n",
    "    boxes = np.squeeze(boxes)\n",
    "    scores = np.squeeze(scores)\n",
    "    classes = np.squeeze(classes)\n",
    "\n",
    "    confidence_cutoff = 0.8\n",
    "    # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "    boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes)\n",
    "\n",
    "    # The current box coordinates are normalized to a range between 0 and 1.\n",
    "    # This converts the coordinates actual location on the image.\n",
    "    width, height = image.size\n",
    "    box_coords = to_image_coords(boxes, height, width)\n",
    "\n",
    "    # Each class with be represented by a differently colored box\n",
    "    draw_boxes(image, box_coords, classes)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72970d",
   "metadata": {
    "id": "ac72970d"
   },
   "source": [
    "### 2.4. Timing Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "403d2bae",
   "metadata": {
    "id": "403d2bae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dc17783",
   "metadata": {
    "id": "6dc17783"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23b7033d",
   "metadata": {
    "id": "23b7033d"
   },
   "source": [
    "### 2.5. Object Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2846dd45",
   "metadata": {
    "id": "2846dd45"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d8cee3f",
   "metadata": {
    "id": "4d8cee3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c6e83cf",
   "metadata": {
    "id": "5c6e83cf"
   },
   "source": [
    "## 3. Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d7d2f",
   "metadata": {
    "id": "138d7d2f"
   },
   "source": [
    "##### Alternatives\n",
    "* TODO.\n",
    "##### Extensions of task\n",
    "* TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f9ba4",
   "metadata": {
    "id": "550f9ba4"
   },
   "source": [
    "## 4. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728bd96",
   "metadata": {
    "id": "5728bd96"
   },
   "source": [
    "- ⬜️ TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6526376",
   "metadata": {
    "id": "f6526376"
   },
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9606fb",
   "metadata": {
    "id": "9a9606fb"
   },
   "source": [
    "This assignment was prepared by Kelvin Lwin, Andrew Bauman, Dominique Luna et al., 2021 (link [here](https://github.com/udacity/CarND-Object-Detection-Lab)).\n",
    "\n",
    "References\n",
    "* [1] Howard, A. G. et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv. 2017. [doi:10.48550/arXiv.1704.04861](https://arxiv.org/abs/1704.04861).\n",
    "* [2] Shelhamer, E. et al. Fully Convolutional Networks for Semantic Segmentation. arXiv. 2016. [doi:10.48550/arXiv.1605.06211](https://arxiv.org/abs/1605.06211).\n",
    "* [3] Liu, W. et al. SSD: Single Shot MultiBox Detector. European Conference on Computer Vision, ECCV. Lecture Notes in Computer Science, 9905:21-37. 2016. [doi:10.1007/978-3-319-46448-0_2](https://doi.org/10.1007/978-3-319-46448-0_2).\n",
    "\n",
    "Helpful resources:\n",
    "* [`CarND-Object-Detection-Lab` by @udacity | GitHub](https://github.com/udacity/CarND-Object-Detection-Lab);\n",
    "* [3.4. Depthwise Convolution | Dive Into Deep Learning](https://tvm.d2l.ai/chapter_common_operators/depthwise_conv.html);\n",
    "* [Depth-wise Convolution and Depth-wise Separable Convolution by A. Pandey | Medium](https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec);\n",
    "* [Pointwise Convolution by A. Shrivastav | OpenGenus](https://iq.opengenus.org/pointwise-convolution/);\n",
    "* [Depthwise Separable Convolution - A FASTER CONVOLUTION! | YouTube](https://www.youtube.com/watch?v=T7o3xvJLuHk)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
