{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d3c8fa",
   "metadata": {
    "id": "10d3c8fa"
   },
   "source": [
    "# Exercise 1.7.1 — Scene Understanding\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8342f66",
   "metadata": {
    "id": "a8342f66"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8a817",
   "metadata": {
    "id": "3ce8a817"
   },
   "source": [
    "* Compute the Mean Intersection over Union (IoU) of the multi-class segmentation label predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a226974",
   "metadata": {
    "id": "9a226974"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e37f8f",
   "metadata": {
    "id": "36e37f8f"
   },
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040b9ad5",
   "metadata": {
    "id": "040b9ad5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3396c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "id": "6f3396c3",
    "outputId": "476d9084-9756-4377-b7a1-d062a45dbc0a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.9.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab345e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "id": "9ab345e3",
    "outputId": "a3f988cf-4d79-40df-ed7d-507fe5f48272"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b54c301",
   "metadata": {
    "id": "4b54c301"
   },
   "outputs": [],
   "source": [
    "### Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5279089e",
   "metadata": {
    "id": "5279089e"
   },
   "outputs": [],
   "source": [
    "ENV_COLAB = True                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eda135f9",
   "metadata": {
    "id": "eda135f9"
   },
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8bc08a8",
   "metadata": {
    "id": "a8bc08a8"
   },
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c1e4cd",
   "metadata": {
    "id": "e7c1e4cd"
   },
   "outputs": [],
   "source": [
    "### Creating subdirectories (if not exists)\n",
    "os.makedirs(DIR_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e58534",
   "metadata": {
    "id": "21e58534"
   },
   "source": [
    "### 1.1. Scene Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d70ec3",
   "metadata": {
    "id": "73d70ec3"
   },
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd081af",
   "metadata": {
    "id": "3fd081af"
   },
   "source": [
    "TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34dd32",
   "metadata": {
    "id": "5f34dd32"
   },
   "source": [
    "#### Metrics — Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab143bf",
   "metadata": {
    "id": "3ab143bf"
   },
   "source": [
    "In the [very first exercise](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-1-Choosing-Metrics/2022-07-25-Choosing-Metrics-IoU.ipynb) of this course, we covered the Intersection over Union (IoU) metric and its application to the bounding box prediction task. Now, we use the IoU metric again but this time for semantic segmentation and scene understanding.\n",
    "\n",
    "We start with the same general formula for the IoU score given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU} &= \\frac{\\textrm{Area of Intersection}}{\\textrm{Area of Union}},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "but now we calculate the IoU score using the following binary classification metrics:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With this form of the IoU equation, all we need to do is compute the true positive ($\\mathrm{TP}$), true negative ($\\mathrm{TN}$), and the false positive ($\\mathrm{FN}$), false negative ($\\mathrm{FN}$) rates. For the image segmentation task, this boils down to the pixel-wise classification predictions. Thankfully, the algorithms we designed in [Sect. 2.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Computer-Vision/Exercises/1-1-1-Choosing-Metrics/2022-07-25-Choosing-Metrics-IoU.ipynb) of Exercise 1.1.1 hold; all we need to do is compute the pixel-wise classification metrics for each class using the same tabular approach as before. With these metrics, we evaluate the $\\mathrm{IoU}$ formula and obtain a score indicating the amount of \"overlap\" between the predicted region and the true region of each segmented object. \n",
    "\n",
    "Let's illustrate this with a simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54cb770",
   "metadata": {
    "id": "d54cb770"
   },
   "source": [
    "```python\n",
    "ground_truth_labels = [\n",
    "    [0, 0, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2], \n",
    "    [3, 3, 3, 3],\n",
    "]\n",
    "predicted_labels = [\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 3, 0, 1],\n",
    "    [2, 2, 2, 3],\n",
    "    [3, 1, 0, 0],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc4fe7",
   "metadata": {
    "id": "69dc4fe7"
   },
   "source": [
    "Above we define a set of _ground-truth_ and _predicted_ labels. Each row in the matrix corresponds to a class; looking at the first row of the `ground_truth_labels` (\"$\\mathrm{A}$\") matrix, we see that class `0` should appear at all four pixel locations. Looking at the first row of `predicted_labels` (\"$\\mathrm{B}$\" matrix), we see instead that only three of the four pixel locations were given a correct prediction of class `0`. In other words, we have in the first row a $\\mathrm{TP} = 3$. Now, we need to compute for class `0` the false positive ($\\mathrm{FP}$) rate. To do this, we examine the _other_ pixel locations (i.e., other rows of the `predicted_labels` matrix), and add up any occurrences of class label `0` where the corresponding entries in `ground_truth_labels` do not match. Since class label `0` was predicted _incorrectly_ at pixel locations $\\mathrm{B}_{2, 3}$, $\\mathrm{B}_{3, 3}$, and $\\mathrm{B}_{4, 4}$, we have a $\\mathrm{FP} = 3$. Now let's complete the calculations for the two other metrics: true negative ($\\mathrm{TN}$) and false negative ($\\mathrm{FN}$). The $\\mathrm{TN}$ value for this problem is easy to compute, since we assume all predictions here were valid (i.e., that we expected a class label to be predicted for every pixel in `predicted_labels`). That means our $\\mathrm{TN} = 0$. Lastly, our $\\mathrm{FN}$ rate is computed as the number of _incorrect_ predictions for class `0`. Looking at the first row of the `predicted_labels` matrix (i.e., the \"predictions\" for class `0`), we count the number of label predictions that are _not_ equal to class `0` to get our false negative rate. With _one_ incorrect class `0` prediction at the first index $\\mathrm{B}_{1, 1} = $ `1`, we have therefore a $\\mathrm{FN} = 1$. \n",
    "\n",
    "With these four classification metrics out of the way, we can obtain the $\\mathrm{IoU}$ score for class `0` as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU}_{0} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}} = \\frac{3}{3 + 3 + 1 + 0} = \\frac{3}{7} \\approx 0.4286.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now that we have the $\\mathrm{IoU}$ score for class `0` computed, we repeat the process for the other three rows (classes) in `predicted_labels` to obtain each classes' respective $\\mathrm{IoU}$ score. Once we have completed the calculations of all four classes, we can take the average to obtain the $\\mathrm{IoU}_{\\textrm{mean}}$, as simply:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{IoU}_{\\textrm{mean}} &= \\frac{1}{n}\\sum_{i=0}^{n} \\mathrm{IoU}_{i},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is nothing but the sum of the per-class $\\mathrm{IoU}$ scores divided by the total number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ke35mrjgWjFL",
   "metadata": {
    "id": "ke35mrjgWjFL"
   },
   "source": [
    "### 1.2. Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1f517",
   "metadata": {},
   "source": [
    "[Convolution](https://en.wikipedia.org/wiki/Convolution) is a measure of overlap between two functions as one slides over the other. Mathematically, it is a sum of products given by the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(f * g\\right)\\left(t\\right) := \\int_{-\\infty}^{\\infty} f\\left(\\tau\\right)\\cdot g\\left(t-\\tau\\right)d\\tau.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layers) in a neural network performs the convolution operation by applying a filter over the input tensor. After the same filter is repeatedly applied to the input, a feature map is created which shows the positions and intensity of a detected feature in an input. \n",
    "\n",
    "Convolutional layers are extremely useful in neural networks, but they often come with a high computational cost due to the number of parameters required for each input (and each channel of the input). In this section we will look at the basics of regular convolution and review the number of parameters required. Then, we will introduce several alternative convolution methods — namely, depth-wise, depth-wise separable and point-wise convolution, which help reduce this computational cost. These alternatives are especially useful for networks intended to be run on mobile devices and embedded hardware where computational resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B3WcU3QlWi5j",
   "metadata": {
    "id": "B3WcU3QlWi5j"
   },
   "source": [
    "#### Regular Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_hk_pcnXFbL",
   "metadata": {
    "id": "H_hk_pcnXFbL"
   },
   "source": [
    "In a typical [convolutional layer](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layers), we have a set of $\\mathrm{N}$ kernels, each with a size of $\\mathrm{D}_{k} * \\mathrm{D}_{k}$. Each of these kernels convolves (\"slides over\") the entire input, which is a $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$ sized feature map (a tensor). When considering the computational cost of a convolution operation, we must consider the number of _parameters_ required for each convolutional layer. Generally, models with more convolutional layers have more parameters, and therefore require more computational resources to use. While this can lead to higher accuracy in image processing tasks, special attention needs to be paid to the computational cost of convolutional networks when utilising low-end hardware, such as with in-vehicle embedded devices which will be powering these type of networks with real-time inference demands.\n",
    "\n",
    "To understand the computational cost of a regular convolutional layer, we have the following cost:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N} * \\mathrm{D}_{k} * \\mathrm{D}_{k},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{D}_{g} * \\mathrm{D}_{g}$ is the size of the output feature map. A regular convolution takes in a $\\mathrm{D}_{f} * \\mathrm{D}_{f} * \\mathrm{M}$ input feature map and returns a $\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{N}$ feature map as output.\n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008yIebXacAE",
   "metadata": {
    "id": "008yIebXacAE"
   },
   "source": [
    "<img src=\"figures/2023-01-26-Figure-1-Standard-Convolution-Filters.png\" alt=\"Figure 1. Filters in a regular convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 1. Filters in a regular convolutional layer (credit: Udacity).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_C3E7_bbOyD",
   "metadata": {
    "id": "H_C3E7_bbOyD"
   },
   "source": [
    "#### Depth-wise Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pXNWwOkWbPSL",
   "metadata": {
    "id": "pXNWwOkWbPSL"
   },
   "source": [
    "[Depth-wise convolution](https://tvm.d2l.ai/chapter_common_operators/depthwise_conv.html) and depth-wise separable convolution are two atypical convolution operations that have less parameters and therefore require less computational power to compute. Depth-wise convolutions are used in the [MobileNet]() [1] architecture designed for mobile and embedded applications.\n",
    "\n",
    "Depth-wise convolution acts on each input channel separately with a different kernel for each. With a number of input channels $\\mathrm{M}$ we have $\\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}$ kernels. Since depth-wise convolution only acts on a single input channel at a time, the kernel depth $\\mathrm{N}$ is set equal to $1$. Therefore, the computational cost of the depth-wise convolution is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{D}_{k} * \\mathrm{D}_{k}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To produce the same effect with regular convolution, each channel of the input requires its own kernel. To compute the convolution, each channel is selected individually, and all elements in the kernel are set to zero except for those corresponding to the respective input channel. The final output of the stacked convolutions is one $\\mathrm{M}$-channel output feature map. As shown above, depth-wise convolution reduces the number of parameters required by a factor $\\mathrm{N}$, i.e., the number of filters required _per input channel_. For a three-channel input, we require _three times less_ number of parameters with the depth-wise convolution approach.   \n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af4401f",
   "metadata": {},
   "source": [
    "<img src=\"figures/2023-01-26-Figure-2-Depthwise-Convolution-Filters.png\" alt=\"Figure 2. Filters in a depth-wise convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 2. Filters in a depth-wise convolutional layer (credit: Udacity).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92934cdb",
   "metadata": {},
   "source": [
    "In order to understand depth-wise separable convolution, we first introduce the point-wise convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8eebde",
   "metadata": {},
   "source": [
    "#### Point-wise Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b1fea",
   "metadata": {},
   "source": [
    "Point-wise convolution is a form of convolution that applies a $1\\times 1$ kernel across an input. Unlike the depth-wise convolution, the $1\\times 1$ kernel used here has a depth equal to the number of channels in the input. The computational complexity of the point-wise convolution is similar to the regular convolution, but instead the filter size $\\mathrm{D}_{k} * \\mathrm{D}_{k}$ is equal to $1 \\times 1$. Therefore, we have the following number of parameters:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e88b8",
   "metadata": {},
   "source": [
    "<img src=\"figures/2023-01-26-Figure-3-Pointwise-Convolution-Filters.png\" alt=\"Figure 3. Filters in a point-wise convolutional layer.\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{Figure 3. Filters in a point-wise convolutional layer (credit: Udacity).}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a112e",
   "metadata": {},
   "source": [
    "#### Depth-wise Separable Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31ba99",
   "metadata": {},
   "source": [
    "Point-wise convolution is used in conjunction with depth-wise convolution in order to perform depth-wise separable convolution. Here, depth-wise separable convolution borrows the idea that the depth and the spatial dimension of a filter can be separated, as is the case with e.g., the [Sobel](https://en.wikipedia.org/wiki/Sobel_operator) filter. The depth-wise separable convolution separates a kernel into two independent kernels, each of which performs two convolutions: the depth-wise and the point-wise convolution. Thus, the total computational cost for the depth-wise separable convolution is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right) + \\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which results in a $\\frac{1}{\\mathrm{N}} + \\frac{1}{\\mathrm{D}_{k} * \\mathrm{D}_{k}}$ reduction in total computation, which can be observed in the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right) + \\left(\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N}\\right)}{\\mathrm{D}_{g} * \\mathrm{D}_{g} * \\mathrm{M} * \\mathrm{N} * \\mathrm{D}_{k} * \\mathrm{D}_{k}} &= \\frac{\\mathrm{D}_{k}^{2} + \\mathrm{N}}{\\mathrm{D}_{k}^{2} * \\mathrm{N}} = \\frac{1}{\\mathrm{N}} + \\frac{1}{\\mathrm{D}_{k}^{2}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $\\mathrm{N}$ is selected to be large enough, depth-wise separable convolution networks can be immensely more computationally efficient. For example, the MobileNet architecture uses a $3\\times 3$ kernel, which results in $\\sim 9\\mathrm{x}$ efficiency improvement over regular convolution layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaa112",
   "metadata": {
    "id": "7cfaa112"
   },
   "source": [
    "### 1.3. Convolutional Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9769c0",
   "metadata": {
    "id": "4d9769c0"
   },
   "source": [
    "#### FCN-8 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cc7a7",
   "metadata": {
    "id": "066cc7a7"
   },
   "source": [
    "TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81567d69",
   "metadata": {
    "id": "81567d69"
   },
   "source": [
    "#### MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4bcc5",
   "metadata": {
    "id": "38a4bcc5"
   },
   "source": [
    "TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b337dc",
   "metadata": {
    "id": "19b337dc"
   },
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76f553",
   "metadata": {
    "id": "9d76f553"
   },
   "source": [
    "### 2.1. Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0509c31",
   "metadata": {
    "id": "e0509c31"
   },
   "source": [
    "Here we use the TensorFlow [`tf.keras.metrics.MeanIoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU) function to compute the mean Intersection over Union (IoU) across all classes $i=0,\\ldots, n$.\n",
    "\n",
    "In order to use the metric as a standalone function, we have to first initialise the respective [`tf.keras.metrics.Metric`](https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/metrics/Metric) subclass instance (i.e., `MeanIoU`), then perform a single \"state update\" using the [`update_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#update_state) class method. As arguments to this function, we pass in the `y_true` and `y_pred` tensors that we wish to evaluate. Optionally, we can provide a `sample_weight` scalar value or vector of rank equal to `y_true`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1588d58",
   "metadata": {
    "id": "f1588d58"
   },
   "outputs": [],
   "source": [
    "### Defining the number of distinct class labels (i.e., classes)\n",
    "N_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89c5313",
   "metadata": {
    "id": "d89c5313"
   },
   "outputs": [],
   "source": [
    "### Initialising the `tf.keras.metrics.Metric` instance\n",
    "iou_mean = tf.keras.metrics.MeanIoU(\n",
    "    num_classes=N_CLASSES,\n",
    "    name='Mean IoU for multi-class object segmentation data',\n",
    "    dtype=tf.dtypes.float32,\n",
    "    ### Additional arguments for TF2.10+ API:\n",
    "    #ignore_class=None,\n",
    "    #sparse_y_true=True,    # `True` if class labels are integers, `False` if floating-point\n",
    "    #sparse_y_pred=True,\n",
    "    #axis=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aEPRhQllMLHG",
   "metadata": {
    "id": "aEPRhQllMLHG"
   },
   "source": [
    "#### Testing the `MeanIoU` metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "JucYiLeoMMG1",
   "metadata": {
    "id": "JucYiLeoMMG1"
   },
   "outputs": [],
   "source": [
    "### Defining our prediction and ground-truth sets\n",
    "ground_truth_labels = [\n",
    "    [0, 0, 0, 0], \n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2], \n",
    "    [3, 3, 3, 3],\n",
    "]\n",
    "predicted_labels = [\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 3, 0, 1],\n",
    "    [2, 2, 2, 3],\n",
    "    [3, 1, 0, 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54WZKwK2Mgb9",
   "metadata": {
    "id": "54WZKwK2Mgb9"
   },
   "outputs": [],
   "source": [
    "### Converting the matrices to n-rank tensors\n",
    "y_true = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        ground_truth_labels).reshape(1, -1, len(ground_truth_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "y_pred = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        predicted_labels).reshape(1, -1, len(predicted_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "t8Fwb-ZxNsaA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8Fwb-ZxNsaA",
    "outputId": "641bc6df-05af-4b77-8feb-d9dc0d0f0304"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41964284"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Computing the mean IoU\n",
    "iou_mean.update_state(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred\n",
    ")\n",
    "iou_mean.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFL3s0eqMK9W",
   "metadata": {
    "id": "SFL3s0eqMK9W"
   },
   "source": [
    "As shown above, we obtain a mean IoU score for the set of predictions of $\\mathrm{IoU}_{\\textrm{mean}} \\ \\approx 0.420$, which matches our expected value for this test set.\n",
    "\n",
    "Now, we repeat the Mean IoU calculation using a second test set of predictions. Note that [`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) instances are stateful by default, meaning that each call to `update_state()` computes the Mean IoU for the input _mini-batch of predictions / ground-truth labels_. In other words, the default behaviour for this `Metric` instance is to accumulate the Mean IoU score across calls to `update_state()`. Since we are computing the Mean IoU score of the full batch (i.e., `batch_size=1`) manually, we must use the [`reset_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#reset_state) method before performing the IoU calculation with [`update_state()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU#update_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "V5vt1MGZSpbQ",
   "metadata": {
    "id": "V5vt1MGZSpbQ"
   },
   "outputs": [],
   "source": [
    "### Defining our new prediction set (assuming same `ground_truth_labels`)\n",
    "predicted_labels = [\n",
    "    [0, 0, 0, 0],\n",
    "    [1, 0, 0, 1],\n",
    "    [1, 2, 2, 1],\n",
    "    [3, 3, 0, 3],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "KOVETdBRU0Iu",
   "metadata": {
    "id": "KOVETdBRU0Iu"
   },
   "outputs": [],
   "source": [
    "### Converting the matrix to an n-rank tensor\n",
    "y_pred = tf.convert_to_tensor(\n",
    "    np.array(\n",
    "        predicted_labels).reshape(1, -1, len(predicted_labels), N_CLASSES\n",
    "    ),\n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "RmADU5HjSpTZ",
   "metadata": {
    "id": "RmADU5HjSpTZ"
   },
   "outputs": [],
   "source": [
    "### Resetting the state of the `MeanIoU` instance\n",
    "# i.e., current cumulative mean IoU becomes `0.0`\n",
    "iou_mean.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "QauTkcrRSpKm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QauTkcrRSpKm",
    "outputId": "e3ce0425-9239-4c3b-f597-9f542cb26905"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53869045"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Computing the new mean IoU\n",
    "iou_mean.update_state(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred\n",
    ")\n",
    "iou_mean.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M9kVfx95VQ48",
   "metadata": {
    "id": "M9kVfx95VQ48"
   },
   "source": [
    "As shown above, we obtain a new mean IoU score for this second test of predictions of $\\mathrm{IoU}_{\\textrm{mean}} \\ \\approx 0.539$, which matches our expected value for this second test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0ea8b",
   "metadata": {
    "id": "b0c0ea8b"
   },
   "source": [
    "### 2.2. Separable Depthwise Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e89e05aa",
   "metadata": {
    "id": "e89e05aa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce9aba00",
   "metadata": {
    "id": "ce9aba00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24db9973",
   "metadata": {
    "id": "24db9973"
   },
   "source": [
    "### 2.3. SSD Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b1ab4e8",
   "metadata": {
    "id": "6b1ab4e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3955c0f",
   "metadata": {
    "id": "b3955c0f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f22d31",
   "metadata": {
    "id": "e6f22d31"
   },
   "source": [
    "### 2.4. Filtering Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b9fcb11",
   "metadata": {
    "id": "0b9fcb11"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3affc3",
   "metadata": {
    "id": "5a3affc3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e3b6185",
   "metadata": {
    "id": "5e3b6185"
   },
   "source": [
    "#### 2.5. Object Detection Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "921de691",
   "metadata": {
    "id": "921de691"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24c320b4",
   "metadata": {
    "id": "24c320b4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac72970d",
   "metadata": {
    "id": "ac72970d"
   },
   "source": [
    "### 2.6. Timing Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "403d2bae",
   "metadata": {
    "id": "403d2bae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dc17783",
   "metadata": {
    "id": "6dc17783"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23b7033d",
   "metadata": {
    "id": "23b7033d"
   },
   "source": [
    "### 2.7. Object Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2846dd45",
   "metadata": {
    "id": "2846dd45"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d8cee3f",
   "metadata": {
    "id": "4d8cee3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c6e83cf",
   "metadata": {
    "id": "5c6e83cf"
   },
   "source": [
    "## 3. Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d7d2f",
   "metadata": {
    "id": "138d7d2f"
   },
   "source": [
    "##### Alternatives\n",
    "* TODO.\n",
    "##### Extensions of task\n",
    "* TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f9ba4",
   "metadata": {
    "id": "550f9ba4"
   },
   "source": [
    "## 4. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728bd96",
   "metadata": {
    "id": "5728bd96"
   },
   "source": [
    "- ⬜️ TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6526376",
   "metadata": {
    "id": "f6526376"
   },
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9606fb",
   "metadata": {
    "id": "9a9606fb"
   },
   "source": [
    "This assignment was prepared by Kelvin Lwin, Andrew Bauman, Dominique Luna et al., 2021 (link [here](https://github.com/udacity/CarND-Object-Detection-Lab)).\n",
    "\n",
    "References\n",
    "* [1] Howard, A. G. et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv. 2017. [doi:10.48550/arXiv.1704.04861](https://arxiv.org/abs/1704.04861).\n",
    "\n",
    "Helpful resources:\n",
    "* [`CarND-Object-Detection-Lab` by @udacity | GitHub](https://github.com/udacity/CarND-Object-Detection-Lab);\n",
    "* [3.4. Depthwise Convolution | Dive Into Deep Learning](https://tvm.d2l.ai/chapter_common_operators/depthwise_conv.html);\n",
    "* [Depth-wise Convolution and Depth-wise Separable Convolution by A. Pandey | Medium](https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec);\n",
    "* [Pointwise Convolution by A. Shrivastav | OpenGenus](https://iq.opengenus.org/pointwise-convolution/);\n",
    "* [Depthwise Separable Convolution - A FASTER CONVOLUTION! | YouTube](https://www.youtube.com/watch?v=T7o3xvJLuHk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7623421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
