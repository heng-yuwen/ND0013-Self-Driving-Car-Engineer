{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b73b05",
   "metadata": {},
   "source": [
    "# Exercise 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65fb0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bfcb4",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1501ac3",
   "metadata": {},
   "source": [
    "In this exercise we will implement the following functions:\n",
    "* `softmax`: computes the softmax (normalised exponential function) of a input tensor;\n",
    "* `cross_entropy`: calculates the cross-entropy loss between one-hot encoded prediction and ground truth vectors;\n",
    "* `model`: logistic regression algorithm;\n",
    "* `accuracy`: calculates the accuracy between a set of predictions and corresponding ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dcb91",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2983a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f0b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d88c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27fa02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbfa20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05aa651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba03402",
   "metadata": {},
   "source": [
    "### 1.1. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be5228",
   "metadata": {},
   "source": [
    "The [softmax function](https://en.wikipedia.org/wiki/Softmax_function) is a generalisation of the sigmoid [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to multiple dimensions. In machine learning, particularly for logistic regression, the softmax function $\\phi$ acts as a decision boundary applied to multi-class datasets, computing the probability of each observation $x_{i}$ belonging to one of $j = i,...,k$ class labels (assuming an independent relationship between the classes),\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P\\left(y=j \\ \\vert \\ z_{i}\\right) = \\phi_{softmax}\\left(z_{i}\\right) \n",
    "    = \\frac{\\mathcal{e}^{z_{i}}}{\\sum_{j=0}^{k}\\mathcal{e}^{z_{k}^{i}}}.\n",
    "    \\end{align}\n",
    "$$\n",
    "The input $z$ is defined to be\n",
    "$$\n",
    "\\begin{align}\n",
    "    z &= w_{0}x_{0} + w_{1}x_{1} + \\ldots + w_{m}x_{m} = \\sum_{i=0}^{m} w_{i}x_{i} = \\mathrm{w}^{\\top}\\mathrm{x}.\n",
    "    \\end{align}\n",
    "$$\n",
    "such that $\\mathrm{w}$ is the weight vector, $\\mathrm{x}$ is the feature vector belonging to a single training observation, and $w_{0}$ is the bias unit.\n",
    "\n",
    "The softmax function computes the probability for each class $P\\left(y=j \\vert x_{i}; w_{j}\\right)$, then a correction step is applied to the predictions during training using a cost function that minimises the cross-entropy over the training set observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9476d5d",
   "metadata": {},
   "source": [
    "### 1.2. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb2f35",
   "metadata": {},
   "source": [
    "In machine learning, [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) is often used as a loss function computed between two probability distributions. Given a set of predictions $q_{i}$ and corresponding true probability values $p_{i}$ we can compute the cross-entropy loss, i.e., _log loss_ [1],\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H\\left(p,q\\right) = -\\sum_{i}{p_i}\\mathrm{log}q_i = -ylog\\hat{y} - \\left(1-y\\right)\\mathrm{log}\\left(1-\\hat{y}\\right)\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "which serves as a measure of dissimilarity between $p$ and $q$.\n",
    "\n",
    "For a set of $n = 1,\\ldots,N$ training observations, we can compute the average of the loss function over all observations such that $H\\left(p,q\\right)$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    J(\\mathrm{w}) = \\frac{1}{N}\\sum_{n=1}^{N}H\\left(p_{n},q_{n}\\right) \n",
    "    = -\\frac{1}{N}\\sum_{n=1}^{N} \\begin{bmatrix} y_{n}\\mathrm{log}\\hat{y}_{n} + \\left(1-y_{n}\\right)\\mathrm{log}\\left(1-\\hat{y}_{n}\\right) \\end{bmatrix}.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e097e",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84b851",
   "metadata": {},
   "source": [
    "The cross-entropy loss function is defined for a training sample $x_{i}$ belonging to class $j$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(x, y; w\\right) &= H\\left(y, \\hat{y}\\right) = \\sum_{j} y_{j}\\mathrm{log}\\hat{y}_{j} = -\\mathrm{log}\\frac{\\mathcal{e}^{w_{j}^{\\top}x_{i}}}{\\sum_{j=1}^{k} \\mathcal{e}^{w_{j}^{\\top}x_{i}}}\n",
    "    \\end{align}\n",
    "$$\n",
    "where $y$ denotes the [one-hot](https://en.wikipedia.org/wiki/One-hot) encoded vector and $\\hat{y}$ denotes the probability distribution $h\\left(x_{i}\\right)$.\n",
    "\n",
    "The cross-entropy loss function for all observations $\\left(\\mathrm{X}_{i}, \\mathrm{Y}_{i}\\right)_{i=1}^{N}$ is then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(\\mathrm{X}, \\mathrm{Y}; \\mathrm{w}\\right) = -\\sum_{i=1}^{N}\\sum_{j=1}^{k} I\\left[y_{i} = j\\right]\\mathrm{log}\\frac{\\mathcal{e}^{w_{j}^{\\top}x_{i}}}{\\sum_{j=1}^{k}\\mathcal{e}^{w_{j}^{\\top} x_{i}}}\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b777c1",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae821c84",
   "metadata": {},
   "source": [
    "### 2.1. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5d459",
   "metadata": {},
   "source": [
    "In this exercise, you have to implement 4 different functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8128d23",
   "metadata": {},
   "source": [
    "* `softmax`: compute the softmax of a vector. This function takes as input a tensor and outputs a discrete probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b539cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d144aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    softmax implementation\n",
    "    args:\n",
    "    - logits [tensor]: 1xN logits tensor\n",
    "    returns:\n",
    "    - soft_logits [tensor]: softmax of logits\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    \n",
    "    soft_logits = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    return soft_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d81e48d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "       0.10902364, 0.29635698])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=7 predictions\n",
    "x = [1.0, 2.0, 3.0, 1.0, 2.0, 2.0, 3.0]\n",
    "x_scaled = softmax(x)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7968a04",
   "metadata": {},
   "source": [
    "### 2.2. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ba4a0",
   "metadata": {},
   "source": [
    "* `cross_entropy`: calculate the cross entropy loss given a vector of predictions (after softmax) and a vector of ground truth (one-hot vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae0b187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "340e7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(scaled_logits, one_hot):\n",
    "    \"\"\"\n",
    "    Cross entropy loss implementation\n",
    "    args:\n",
    "    - scaled_logits [tensor]: NxC tensor where N batch size / C number of classes\n",
    "    - one_hot [tensor]: one hot tensor\n",
    "    returns:\n",
    "    - loss [tensor]: cross entropy \n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    \n",
    "    n_classes = one_hot.shape[0]\n",
    "    class_label = one_hot.argmax(axis=1)\n",
    "    log_likelihood = -np.log(scaled_logits[:n_classes, class_label])\n",
    "    loss = np.sum(log_likelihood) / n_classes\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1af1a4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our ground-truth labels and using one-hot encoding\n",
    "y = np.array([2.0, 2.0, 3.0, 0.0, 2.0, 1.0, 3.0])\n",
    "y_one_hot = utils.to_categorical(y, num_classes=np.unique(y).shape[0])\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "133c50ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "        0.10902364, 0.29635698],\n",
       "       [0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "        0.10902364, 0.29635698],\n",
       "       [0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "        0.10902364, 0.29635698]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating pseudo-batched prediction data by repeating 'predictions'\n",
    "X_scaled = np.stack([x_scaled] * 3)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e26ed492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.648571590055648"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the cross-entropy loss function with N=1 batch\n",
    "loss = cross_entropy(X_scaled, y_one_hot)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d190cd80",
   "metadata": {},
   "source": [
    "### 2.3. Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa6d04",
   "metadata": {},
   "source": [
    "* `model`: takes a batch of images (stack of images along the first dimensions) and feeds it through the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92083e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "    \"\"\"\n",
    "    logistic regression model\n",
    "    args:\n",
    "    - X [tensor]: input HxWx3\n",
    "    - W [tensor]: weights\n",
    "    - b [tensor]: bias\n",
    "    returns:\n",
    "    - output [tensor]\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca9b93",
   "metadata": {},
   "source": [
    "### 2.4. Prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aca75b",
   "metadata": {},
   "source": [
    "* `accuracy`: given a vector of predictions and a vector of ground truth, calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06181bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, Y):\n",
    "    \"\"\"\n",
    "    calculate accuracy\n",
    "    args:\n",
    "    - y_hat [tensor]: NxC tensor of models predictions\n",
    "    - y [tensor]: N tensor of ground truth classes\n",
    "    returns:\n",
    "    - acc [tensor]: accuracy\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8b2e1",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be159df3",
   "metadata": {},
   "source": [
    "You can leverage the `tf.boolean_mask` function to calculate the cross entropy. Keep in mind\n",
    "that most elements of the ground truth vector are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051aadeb",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This assignment was prepared by Thomas Hossler and Michael Virgo et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "References\n",
    "* [1] Ji, S. Xie, Y. Logistic Regression: From Binary to Multi-Class. http://people.tamu.edu/~sji/classes/LR.pdf\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [Softmax Regression and How is it Related to Logistic Regression? | KDnuggets](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html)\n",
    "* [Softmax and Cross Entropy Loss | P. Dahal](https://deepnotes.io/softmax-crossentropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
