{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b73b05",
   "metadata": {},
   "source": [
    "# Exercise 1.3.1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d04ed",
   "metadata": {},
   "source": [
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979664f",
   "metadata": {},
   "source": [
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bfcb4",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1be90b",
   "metadata": {},
   "source": [
    "In this exercise we will implement the following functions:\n",
    "* `softmax`: computes the softmax (normalised exponential function) of a input tensor;\n",
    "* `cross_entropy`: calculates the cross-entropy loss between one-hot encoded prediction and ground truth vectors;\n",
    "* `model`: logistic regression algorithm;\n",
    "* `accuracy`: calculates the accuracy between a set of predictions and corresponding ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24afe77",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5605c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f55668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de001b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70fb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9de4350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f651f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ea3f8",
   "metadata": {},
   "source": [
    "### 1.1. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee36bd",
   "metadata": {},
   "source": [
    "The [softmax function](https://en.wikipedia.org/wiki/Softmax_function) is a generalisation of the sigmoid [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to multiple dimensions. In machine learning, particularly for logistic regression, the softmax function $\\phi$ acts as a decision boundary applied to multi-class datasets, computing the probability of each observation $x_{i}$ belonging to one of $j = i,...,k$ class labels (assuming an independent relationship between the classes),\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P\\left(y=j \\ \\vert \\ z_{i}\\right) = \\phi_{softmax}\\left(z_{i}\\right) \n",
    "    = \\frac{\\mathcal{e}^{z_{i}}}{\\sum_{j=0}^{k}\\mathcal{e}^{z_{k}^{i}}}.\n",
    "    \\end{align}\n",
    "$$\n",
    "The input $z$ is defined to be\n",
    "$$\n",
    "\\begin{align}\n",
    "    z &= w_{0}x_{0} + w_{1}x_{1} + \\ldots + w_{m}x_{m} = \\sum_{i=0}^{m} w_{i}x_{i} = \\mathrm{w}^{\\top}\\mathrm{x}.\n",
    "    \\end{align}\n",
    "$$\n",
    "such that $\\mathrm{w}$ is the weight vector, $\\mathrm{x}$ is the feature vector belonging to a single training observation, and $w_{0}$ is the bias unit.\n",
    "\n",
    "The softmax function computes the probability for each class $P\\left(y=j \\vert x_{i}; w_{j}\\right)$, then a correction step is applied to the predictions during training using a cost function that minimises the cross-entropy over the training set observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43814d3",
   "metadata": {},
   "source": [
    "##### Note on numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abe1c2",
   "metadata": {},
   "source": [
    "Exponentiation in Python can be a problem for larger numbers. A Numpy `float64` value can represent a maximal number on the order of $10^{308}$, but with exponentiation in the softmax function it is possible to overshoot this number, even for fairly modest-sized inputs (as pointed out in [this](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) post by E. Bendersky).\n",
    "\n",
    "To handle this, we can normalise the inputs using an arbitrary constant $C$ and moving it into the exponent to obtain\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\phi_{softmax}\\left(z_{i}\\right) \n",
    "    = \\frac{C\\mathcal{e}^{z_{i}}}{\\sum_{j=0}^{k}C\\mathcal{e}^{z_{k}^{i}}} = \\frac{\\mathcal{e}^{z_{i} + \\mathrm{log}\\left(C\\right)}}{\\sum_{j=0}^{k}C\\mathcal{e}^{z_{k}^{i} + \\mathrm{log}\\left(C\\right)}}.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Replacing $\\mathrm{log}\\left(C\\right)$ with another arbitrary constant $D$, we can then select a value for $D$ as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    D = -max\\left(x_{1}, x_{2},\\ldots,x_{N}\\right) \n",
    "    \\end{align}\n",
    "$$\n",
    "such that all input observations $x$ will be shifted towards zero with _negative_ values. Because of this, we can better avoid NaNs as negatives with large exponents \"saturate\" to zero rather than infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183d4c8",
   "metadata": {},
   "source": [
    "### 1.2. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1b9984",
   "metadata": {},
   "source": [
    "In machine learning, [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) is often used as a loss function computed between two probability distributions. Given a set of predictions $q_{i}$ and corresponding true probability values $p_{i}$ we can compute the cross-entropy loss, i.e., _log loss_ [1],\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H\\left(p,q\\right) = -\\sum_{i}{p_i}\\mathrm{log}q_i = -ylog\\hat{y} - \\left(1-y\\right)\\mathrm{log}\\left(1-\\hat{y}\\right)\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "which serves as a measure of dissimilarity between $p$ and $q$.\n",
    "\n",
    "For a set of $n = 1,\\ldots,N$ training observations, we can compute the average of the loss function over all observations such that $H\\left(p,q\\right)$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    J(\\mathrm{w}) = \\frac{1}{N}\\sum_{n=1}^{N}H\\left(p_{n},q_{n}\\right) \n",
    "    = -\\frac{1}{N}\\sum_{n=1}^{N} \\begin{bmatrix} y_{n}\\mathrm{log}\\hat{y}_{n} + \\left(1-y_{n}\\right)\\mathrm{log}\\left(1-\\hat{y}_{n}\\right) \\end{bmatrix}.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9057c4",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f03b25",
   "metadata": {},
   "source": [
    "The cross-entropy loss function is defined for a training sample $x_{i}$ belonging to class $j$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(x, y; w\\right) &= H\\left(y, \\hat{y}\\right) = \\sum_{j} y_{j}\\mathrm{log}\\hat{y}_{j} = -\\mathrm{log}\\frac{\\mathcal{e}^{w_{j}^{\\top}x_{i}}}{\\sum_{j=1}^{k} \\mathcal{e}^{w_{j}^{\\top}x_{i}}}\n",
    "    \\end{align}\n",
    "$$\n",
    "where $y$ denotes the [one-hot](https://en.wikipedia.org/wiki/One-hot) encoded vector and $\\hat{y}$ denotes the probability distribution $h\\left(x_{i}\\right)$.\n",
    "\n",
    "The cross-entropy loss function for all observations $\\left(\\mathrm{X}_{i}, \\mathrm{Y}_{i}\\right)_{i=1}^{N}$ is then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(\\mathrm{X}, \\mathrm{Y}; \\mathrm{w}\\right) = -\\sum_{i=1}^{N}\\sum_{j=1}^{k} I\\left[y_{i} = j\\right]\\mathrm{log}\\frac{\\mathcal{e}^{w_{j}^{\\top}x_{i}}}{\\sum_{j=1}^{k}\\mathcal{e}^{w_{j}^{\\top} x_{i}}}.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfc7f1",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d3dfa",
   "metadata": {},
   "source": [
    "### 2.1. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5d459",
   "metadata": {},
   "source": [
    "In this exercise, you have to implement 4 different functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8128d23",
   "metadata": {},
   "source": [
    "* `softmax`: compute the softmax of a vector. This function takes as input a tensor and outputs a discrete probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b539cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d144aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits, stable=False):\n",
    "    \"\"\"Returns the softmax probability distribution.\n",
    "    \n",
    "    :param logits: a 1xN tf.Tensor of logits.\n",
    "    :param stable: optional, flag indicating whether\n",
    "        or not to normalise the input data.\n",
    "    returns: soft_logits, a 1xN tf.Tensor of real \n",
    "        values in range (0,1) that sum up to 1.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(logits, tf.Tensor)\n",
    "    if stable:\n",
    "        logits = tf.subtract(logits, tf.reduce_max(logits))\n",
    "    soft_logits = tf.math.exp(logits)\n",
    "    soft_logits /= tf.math.reduce_sum(soft_logits)\n",
    "    return soft_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22ac0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 19:41:36.465409: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-08-28 19:41:36.466833: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "       0.10902364, 0.29635698])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=7 predictions\n",
    "x = [1.0, 2.0, 3.0, 1.0, 2.0, 2.0, 3.0]\n",
    "### Converting to tf.Tensor object\n",
    "x = tf.constant(x, dtype=tf.float64)\n",
    "### Computing the softmax function and printing results as Numpy array\n",
    "x_scaled = softmax(x)\n",
    "x_scaled.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b62cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([nan, nan, nan])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=3 large values (without normalising)\n",
    "x_large = tf.constant([1000, 2000, 3000], dtype=tf.float64)\n",
    "softmax(x_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "307037d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=3 large values (with normalising)\n",
    "x_large = tf.constant([1000, 2000, 3000], dtype=tf.float64)\n",
    "softmax(x_large, stable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca89c15",
   "metadata": {},
   "source": [
    "**Note**: this output isn't very ideal either, since the softmax function does not typically result in a zero value. However, for very large numbers, we are expecting a result extremely close to zero anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b098e5",
   "metadata": {},
   "source": [
    "### 2.2. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ba4a0",
   "metadata": {},
   "source": [
    "* `cross_entropy`: calculate the cross entropy loss given a vector of predictions (after softmax) and a vector of ground truth (one-hot vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0b187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "340e7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(scaled_logits, one_hot):\n",
    "    \"\"\"Returns the cross-entropy loss.\n",
    "    \n",
    "    :param scaled_logits: an NxC tf.Tensor of scaled softmax\n",
    "        distribution values, [n_samples x n_classes].\n",
    "    :param one_hot: an NxC tf.Tensor of one-hot encoded \n",
    "        ground truth labels, [n_samples x n_classes].\n",
    "    :returns: loss, a 1x1 tf.Tensor with cross-entropy loss. \n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(scaled_logits, tf.Tensor)\n",
    "    assert isinstance(one_hot, tf.Tensor)\n",
    "    assert scaled_logits.shape == y_one_hot.shape\n",
    "    n_samples = one_hot.shape[0]\n",
    "    class_labels = tf.math.argmax(one_hot, axis=1)\n",
    "    # For each sample, pick the probability value from the distribution\n",
    "    # that corresponds to the true class label\n",
    "    preds = tnp.asarray(scaled_logits)[range(n_samples), class_labels]\n",
    "    log_likelihood = -tf.math.log(preds)\n",
    "    loss = tf.math.reduce_sum(log_likelihood) / n_samples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17922ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our ground-truth labels and using one-hot encoding\n",
    "y = tf.constant([2.0, 2.0, 3.0, 0.0, 2.0, 1.0, 3.0])\n",
    "y_one_hot = tf.constant(tf.keras.utils.to_categorical(y))\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7e9170a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 4), dtype=float64, numpy=\n",
       "array([[0.04010756, 0.04010756, 0.04010756, 0.04010756],\n",
       "       [0.10902364, 0.10902364, 0.10902364, 0.10902364],\n",
       "       [0.29635698, 0.29635698, 0.29635698, 0.29635698],\n",
       "       [0.04010756, 0.04010756, 0.04010756, 0.04010756],\n",
       "       [0.10902364, 0.10902364, 0.10902364, 0.10902364],\n",
       "       [0.10902364, 0.10902364, 0.10902364, 0.10902364],\n",
       "       [0.29635698, 0.29635698, 0.29635698, 0.29635698]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating pseudo-batched data by repeating 'predictions'\n",
    "X_scaled = tf.stack([x_scaled] * 4, axis=1)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479b9123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=2.216190530018549>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the cross-entropy loss function with N=1 batch\n",
    "loss = cross_entropy(X_scaled, y_one_hot)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327561a",
   "metadata": {},
   "source": [
    "### 2.3. Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa6d04",
   "metadata": {},
   "source": [
    "* `model`: takes a batch of images (stack of images along the first dimensions) and feeds it through the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b8cf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92083e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "    \"\"\"\n",
    "    logistic regression model\n",
    "    args:\n",
    "    - X [tensor]: input HxWx3\n",
    "    - W [tensor]: weights\n",
    "    - b [tensor]: bias\n",
    "    returns:\n",
    "    - output [tensor]\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28285f",
   "metadata": {},
   "source": [
    "### 2.4. Prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aca75b",
   "metadata": {},
   "source": [
    "* `accuracy`: given a vector of predictions and a vector of ground truth, calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06181bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bf1feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, Y):\n",
    "    \"\"\"\n",
    "    calculate accuracy\n",
    "    args:\n",
    "    - y_hat [tensor]: NxC tensor of models predictions\n",
    "    - y [tensor]: N tensor of ground truth classes\n",
    "    returns:\n",
    "    - acc [tensor]: accuracy\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8b2e1",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be159df3",
   "metadata": {},
   "source": [
    "You can leverage the `tf.boolean_mask` function to calculate the cross entropy. Keep in mind\n",
    "that most elements of the ground truth vector are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90acbad",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This assignment was prepared by Thomas Hossler and Michael Virgo et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "References\n",
    "* [1] Ji, S. Xie, Y. Logistic Regression: From Binary to Multi-Class. http://people.tamu.edu/~sji/classes/LR.pdf\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [Softmax Regression and How is it Related to Logistic Regression? | KDnuggets](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html)\n",
    "* [The Softmax function and its derivative | E. Bendersky](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\n",
    "* [Softmax and Cross Entropy Loss | P. Dahal](https://deepnotes.io/softmax-crossentropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
