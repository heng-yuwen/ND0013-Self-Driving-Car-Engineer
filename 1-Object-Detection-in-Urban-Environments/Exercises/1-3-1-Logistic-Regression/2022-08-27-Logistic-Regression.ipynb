{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b73b05",
   "metadata": {},
   "source": [
    "# Exercise 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65fb0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bfcb4",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a320e",
   "metadata": {},
   "source": [
    "In this exercise we will implement the following functions:\n",
    "* `softmax`: computes the softmax (normalised exponential function) of a input tensor;\n",
    "* `cross_entropy`: calculates the cross-entropy loss between one-hot encoded prediction and ground truth vectors;\n",
    "* `model`: logistic regression algorithm;\n",
    "* `accuracy`: calculates the accuracy between a set of predictions and corresponding ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153113ba",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2308d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9188c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38607790",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02bfb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6db820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb7c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ffc8e",
   "metadata": {},
   "source": [
    "### 1.1. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483020bd",
   "metadata": {},
   "source": [
    "The [softmax function](https://en.wikipedia.org/wiki/Softmax_function) is a generalisation of the sigmoid [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to multiple dimensions. In machine learning, particularly for logistic regression, the softmax function $\\phi$ acts as a decision boundary applied to multi-class datasets, computing the probability of each observation $x_{i}$ belonging to one of $j = i,...,k$ class labels (assuming an independent relationship between the classes),\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P\\left(y=j \\ \\vert \\ z_{i}\\right) = \\phi_{softmax}\\left(z_{i}\\right) \n",
    "    = \\frac{\\mathcal{e}^{z_{i}}}{\\sum_{j=0}^{k}\\mathcal{e}^{z_{k}^{i}}}.\n",
    "    \\end{align}\n",
    "$$\n",
    "The input $z$ is defined to be\n",
    "$$\n",
    "\\begin{align}\n",
    "    z &= w_{0}x_{0} + w_{1}x_{1} + \\ldots + w_{m}x_{m} = \\sum_{i=0}^{m} w_{i}x_{i} = \\mathrm{w}^{\\top}\\mathrm{x}.\n",
    "    \\end{align}\n",
    "$$\n",
    "such that $\\mathrm{w}$ is the weight vector, $\\mathrm{x}$ is the feature vector belonging to a single training observation, and $w_{0}$ is the bias unit.\n",
    "\n",
    "The softmax function computes the probability for each class $P\\left(y=j \\vert x_{i}; w_{j}\\right)$, then a correction step is applied to the predictions during training using a cost function that minimises the cross-entropy over the training set observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1f8ed",
   "metadata": {},
   "source": [
    "##### Note on numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce3acf",
   "metadata": {},
   "source": [
    "Exponentiation in Python can be a problem for larger numbers. A Numpy `float64` value can represent a maximal number on the order of $10^{308}$, but with exponentiation in the softmax function it is possible to overshoot this number, even for fairly modest-sized inputs (as pointed out in [this](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) post by E. Bendersky).\n",
    "\n",
    "To handle this, we can normalise the inputs using an arbitrary constant $C$ and moving it into the exponent to obtain\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\phi_{softmax}\\left(z_{i}\\right) \n",
    "    = \\frac{C\\mathcal{e}^{z_{i}}}{\\sum_{j=0}^{k}C\\mathcal{e}^{z_{k}^{i}}} = \\frac{\\mathcal{e}^{z_{i} + \\mathrm{log}\\left(C\\right)}}{\\sum_{j=0}^{k}C\\mathcal{e}^{z_{k}^{i} + \\mathrm{log}\\left(C\\right)}}.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Replacing $\\mathrm{log}\\left(C\\right)$ with another arbitrary constant $D$, we can then select a value for $D$ as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    D = -max\\left(x_{1}, x_{2},\\ldots,x_{N}\\right) \n",
    "    \\end{align}\n",
    "$$\n",
    "such that all input observations $x$ will be shifted towards zero with _negative_ values. Because of this, we can better avoid NaNs as negatives with large exponents \"saturate\" to zero rather than infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe82f54",
   "metadata": {},
   "source": [
    "### 1.2. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309acfc",
   "metadata": {},
   "source": [
    "In machine learning, [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) is often used as a loss function computed between two probability distributions. Given a set of predictions $q_{i}$ and corresponding true probability values $p_{i}$ we can compute the cross-entropy loss, i.e., _log loss_ [1],\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H\\left(p,q\\right) = -\\sum_{i}{p_i}\\mathrm{log}q_i = -ylog\\hat{y} - \\left(1-y\\right)\\mathrm{log}\\left(1-\\hat{y}\\right)\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "which serves as a measure of dissimilarity between $p$ and $q$.\n",
    "\n",
    "For a set of $n = 1,\\ldots,N$ training observations, we can compute the average of the loss function over all observations such that $H\\left(p,q\\right)$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    J(\\mathrm{w}) = \\frac{1}{N}\\sum_{n=1}^{N}H\\left(p_{n},q_{n}\\right) \n",
    "    = -\\frac{1}{N}\\sum_{n=1}^{N} \\begin{bmatrix} y_{n}\\mathrm{log}\\hat{y}_{n} + \\left(1-y_{n}\\right)\\mathrm{log}\\left(1-\\hat{y}_{n}\\right) \\end{bmatrix}.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ca1fb",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e32f4",
   "metadata": {},
   "source": [
    "The cross-entropy loss function is defined for a training sample $x_{i}$ belonging to class $j$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(x, y; w\\right) &= H\\left(y, \\hat{y}\\right) = \\sum_{j} y_{j}\\mathrm{log}\\hat{y}_{j} = -\\mathrm{log}\\frac{\\mathcal{e}^{w_{j}^{\\top}x_{i}}}{\\sum_{j=1}^{k} \\mathcal{e}^{w_{j}^{\\top}x_{i}}}\n",
    "    \\end{align}\n",
    "$$\n",
    "where $y$ denotes the [one-hot](https://en.wikipedia.org/wiki/One-hot) encoded vector and $\\hat{y}$ denotes the probability distribution $h\\left(x_{i}\\right)$.\n",
    "\n",
    "The cross-entropy loss function for all observations $\\left(\\mathrm{X}_{i}, \\mathrm{Y}_{i}\\right)_{i=1}^{N}$ is then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(\\mathrm{X}, \\mathrm{Y}; \\mathrm{w}\\right) = -\\sum_{i=1}^{N}\\sum_{j=1}^{k} I\\left[y_{i} = j\\right]\\mathrm{log}\\frac{\\mathcal{e}^{w_{j}^{\\top}x_{i}}}{\\sum_{j=1}^{k}\\mathcal{e}^{w_{j}^{\\top} x_{i}}}.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd42ea",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fedb148",
   "metadata": {},
   "source": [
    "### 2.1. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5d459",
   "metadata": {},
   "source": [
    "In this exercise, you have to implement 4 different functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8128d23",
   "metadata": {},
   "source": [
    "* `softmax`: compute the softmax of a vector. This function takes as input a tensor and outputs a discrete probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b539cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d144aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits, stable=False):\n",
    "    \"\"\"Returns the softmax probability distribution.\n",
    "    \n",
    "    :param logits: a 1xN tensor of logits.\n",
    "    :param stable: optional, flag indicating whether\n",
    "        or not to normalise the input data.\n",
    "    returns: soft_logits, an 1xN tensor of real values\n",
    "        in range (0,1) that sum up to 1.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: perform on tensor objects\n",
    "    if stable:\n",
    "        logits -= np.max(logits)\n",
    "    soft_logits = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    return soft_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ab041f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "       0.10902364, 0.29635698])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=7 predictions\n",
    "x = [1.0, 2.0, 3.0, 1.0, 2.0, 2.0, 3.0]\n",
    "x_scaled = softmax(x)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ff7ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/qk186lys1kd1yhnstdfyjjr80000gn/T/ipykernel_87737/1200169668.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  soft_logits = np.exp(logits) / np.sum(np.exp(logits))\n",
      "/var/folders/wf/qk186lys1kd1yhnstdfyjjr80000gn/T/ipykernel_87737/1200169668.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  soft_logits = np.exp(logits) / np.sum(np.exp(logits))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=3 large values (without normalising)\n",
    "x_large = [1000, 2000, 3000]\n",
    "softmax(x_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85209812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the softmax function with N=3 large values (with normalising)\n",
    "x_large = [1000, 2000, 3000]\n",
    "softmax(x_large, stable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f978eb",
   "metadata": {},
   "source": [
    "**Note**: this output isn't very ideal either, since the softmax function does not typically result in a zero value. However, for very large numbers, we are expecting a result extremely close to zero anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea6156",
   "metadata": {},
   "source": [
    "### 2.2. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ba4a0",
   "metadata": {},
   "source": [
    "* `cross_entropy`: calculate the cross entropy loss given a vector of predictions (after softmax) and a vector of ground truth (one-hot vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae0b187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "340e7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(scaled_logits, one_hot):\n",
    "    \"\"\"Returns the cross-entropy loss.\n",
    "    \n",
    "    :param scaled_logits: an NxC tensor of scaled softmax\n",
    "        distribution values, [n_samples x n_classes].\n",
    "    :param one_hot: an CxN tensor of one-hot encoded \n",
    "        ground truth labels, [n_classes x n_samples].\n",
    "    :returns: loss, a 1x1 tensor with cross-entropy loss. \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: perform on tensor objects\n",
    "    assert scaled_logits.shape == one_hot.T.shape\n",
    "    n_classes = one_hot.shape[0]\n",
    "    class_label = one_hot.argmax(axis=1)\n",
    "    log_likelihood = -np.log(scaled_logits[:n_classes, class_label])\n",
    "    loss = np.sum(log_likelihood) / n_classes\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be3caa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our ground-truth labels and using one-hot encoding\n",
    "y = np.array([2.0, 2.0, 3.0, 0.0, 2.0, 1.0, 3.0])\n",
    "y_one_hot = utils.to_categorical(y, num_classes=np.unique(y).shape[0])\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca2f43ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "        0.10902364, 0.29635698],\n",
       "       [0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "        0.10902364, 0.29635698],\n",
       "       [0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "        0.10902364, 0.29635698],\n",
       "       [0.04010756, 0.10902364, 0.29635698, 0.04010756, 0.10902364,\n",
       "        0.10902364, 0.29635698]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating pseudo-batched data by repeating 'predictions'\n",
    "X_scaled = np.stack([x_scaled] * 4)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12b86f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.864762120074198"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing the cross-entropy loss function with N=1 batch\n",
    "loss = cross_entropy(X_scaled, y_one_hot)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de1fa3",
   "metadata": {},
   "source": [
    "### 2.3. Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa6d04",
   "metadata": {},
   "source": [
    "* `model`: takes a batch of images (stack of images along the first dimensions) and feeds it through the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92083e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "    \"\"\"\n",
    "    logistic regression model\n",
    "    args:\n",
    "    - X [tensor]: input HxWx3\n",
    "    - W [tensor]: weights\n",
    "    - b [tensor]: bias\n",
    "    returns:\n",
    "    - output [tensor]\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7151ec",
   "metadata": {},
   "source": [
    "### 2.4. Prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aca75b",
   "metadata": {},
   "source": [
    "* `accuracy`: given a vector of predictions and a vector of ground truth, calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06181bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `logistic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, Y):\n",
    "    \"\"\"\n",
    "    calculate accuracy\n",
    "    args:\n",
    "    - y_hat [tensor]: NxC tensor of models predictions\n",
    "    - y [tensor]: N tensor of ground truth classes\n",
    "    returns:\n",
    "    - acc [tensor]: accuracy\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8b2e1",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be159df3",
   "metadata": {},
   "source": [
    "You can leverage the `tf.boolean_mask` function to calculate the cross entropy. Keep in mind\n",
    "that most elements of the ground truth vector are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726f8a5",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This assignment was prepared by Thomas Hossler and Michael Virgo et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "References\n",
    "* [1] Ji, S. Xie, Y. Logistic Regression: From Binary to Multi-Class. http://people.tamu.edu/~sji/classes/LR.pdf\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [Softmax Regression and How is it Related to Logistic Regression? | KDnuggets](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html)\n",
    "* [The Softmax function and its derivative | E. Bendersky](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\n",
    "* [Softmax and Cross Entropy Loss | P. Dahal](https://deepnotes.io/softmax-crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15996404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
