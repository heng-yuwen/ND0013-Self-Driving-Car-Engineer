{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f13c1a",
   "metadata": {},
   "source": [
    "# Exercise 1.3.2: Stochastic Gradient Descent\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec40be",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca14290",
   "metadata": {},
   "source": [
    "* Create training and validation loops in TensorFlow using the custom functions built in [Exercise 1.3.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-3-1-Logistic-Regression/2022-08-27-Logistic-Regression.ipynb);\n",
    "* Implement the logistic regression model using [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent);\n",
    "* Train the model on the [German Traffic Sign Recognition Benchmark](https://benchmark.ini.rub.de) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4e8ab",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdaafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d264e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c878b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225f754",
   "metadata": {},
   "source": [
    "### 1.1. Custom model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00cc562",
   "metadata": {},
   "source": [
    "We will wrap the cross-entropy loss and accuracy metric functions in a [`tf.keras.metrics.MeanMetricWrapper`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanMetricWrapper) which is a quick way to build a custom metric function in TensorFlow. Since the `MeanMetricWrapper` expects a per-sample loss array as output, we will slightly modify our `cross_entropy` and `accuracy` functions from [Exercise 1.3.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-3-1-Logistic-Regression/2022-08-27-Logistic-Regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e6c45",
   "metadata": {},
   "source": [
    "#### Softmax activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f89fa",
   "metadata": {},
   "source": [
    "This [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function is a generalisation of the sigmoid [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to multiple dimensions. The softmax function computes the discrete probability distribution over all classes for each observation in the training step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340308ce",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950b993",
   "metadata": {},
   "source": [
    "The [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) loss function serves as measure of dissimilarity between the maximum likelihood estimate (the class label prediction) and the corresponding ground truth class label. We will use cross-entropy loss as a cost function to minimise over all data points in our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f82810",
   "metadata": {},
   "source": [
    "#### Accuracy scoring metric function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d659d4f",
   "metadata": {},
   "source": [
    "The average number of true class predictions over the total number of predicted class labels. For a single prediction, this value is either $1$ (correct) or $0$ (incorrect)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22cb16",
   "metadata": {},
   "source": [
    "### 1.2. Modelling with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8873fa",
   "metadata": {},
   "source": [
    "In this section we will build and prepare the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) model for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984cd9d2",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e81b5",
   "metadata": {},
   "source": [
    "The logistic regression solver we have selected for this assignment is called [_stochastic gradient descent_](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD). Stochastic gradient descent is simply a gradient descent optimisation with a batch size of $1$. That is, on every weight update step only a single training example is used to compute the gradients. This differs from _batch_ or _mini-batch_ gradient descent, which use the full, or subset of the full datasets, on each update step.\n",
    "\n",
    "The `sgd` function loops over all weight values in vector $\\mathrm{w}$ and subtracts the gradient computation from each value $w_{i}$, performing a gradient _descent_ towards the function minima. Here we perform the _stochastic_ approach with `grad` computed on a per-sample basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1e12d",
   "metadata": {},
   "source": [
    "#### Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e813c7d",
   "metadata": {},
   "source": [
    "Logistic regression is a simple [linear model](https://en.wikipedia.org/wiki/Linear_model) for classification that attempts to fit a mapping between input data $X$ and output labels $y$. To do so, a linear function $y = m*x + b$ is approximated such that two variables $m$ and $b$ minimise the loss. For a linear model such as this, our $m$ variable represents a vector of _weights_ (denoted $\\mathrm{W}$) and our $b$ variable represents our _bias_ term. The loss (error) we are attempting to minimise is computed with an off-the-shelf [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) function (commonly referred to as _log loss_). This function produces a loss value which essentially measures the difference between the predicted and true outcome variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af0197",
   "metadata": {},
   "source": [
    "### 1.3. Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d4282",
   "metadata": {},
   "source": [
    "By utilising several TensorFlow Functional APIs we can perform custom training and validation loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce7dbf",
   "metadata": {},
   "source": [
    "**Training loop**:\n",
    "1. Iterate over training dataset sample-by-sample;\n",
    "2. Perform the forward pass of the model on each sample:\n",
    "    * Scale each image data array to values between 0 and 1;\n",
    "    * Compute the maximum likelihood estimate;\n",
    "    * Calculate the cross-entropy loss (i.e., measure prediction error);\n",
    "    * Obtain the gradient with respect to model weight vectors;\n",
    "    * Update the weight vectors by subtracting the gradient (stochastic gradient descent);\n",
    "    * Calculate the prediction accuracy;\n",
    "3. Return the mean average loss and accuracy metrics.\n",
    "\n",
    "\n",
    "**Validation loop**:\n",
    "1. Iterate over the validation dataset sample-by-sample;\n",
    "    * Scale each image data array to values between 0 and 1;\n",
    "    * Compute the maximum likelihood estimate (i.e., making a prediction);\n",
    "    * Calculate the prediction accuracy;\n",
    "2. Return the mean average accuracy metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ad676",
   "metadata": {},
   "source": [
    "In our `training_loop()` method we will implement the [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) tool for performing [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (_autodiff_). TensorFlow's `GradientTape` API uses a reverse-order traversal of operations in the forward and backward passes of a model in order to differentiate a desired function. This is known as _reverse mode differentiation_ and is a more-optimal technique over [symbolic differentiation](https://en.wikipedia.org/wiki/Symbolic_differentiation) and [numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation) for computing the partial derivatives of a function with respect to many inputs.\n",
    "\n",
    "Here our `GradientTape` routine will calculate the gradient of the loss with respect to the model variables after a forward pass of the model has been run. Once a maximum likelihood estimate has been worked out for an observation $x_{i}$ given its true class label $y = j$ with respect to the weight $w_{j}$, that is,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P\\left(y = j \\vert z_{i} = x_{i}; w_{j} \\right) &= \\phi_{softmax}\\left(z_{i}\\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the input $z_{i}$ is defined to be\n",
    "$$\n",
    "\\begin{align}\n",
    "    z_{i} = w_{i}x_{i} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "such that $w$ is the model weight vector and $x_{i}$ is the feature vector belonging to the single training observation. \n",
    "\n",
    "The gradients computed with `GradientTape` for the cross-entropy loss\n",
    "$$\n",
    "\\begin{align}\n",
    "    loss\\left(x_{i}, y; w_{i}\\right) = H\\left(y, \\hat{y}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for a one-hot encoded true class label vector $y$ and the corresponding predicted probability distribution $\\hat{y}$ are with respect to the model weight and bias parameters $w$ and $b$.\n",
    "\n",
    "In stochastic gradient descent, these iterative gradient computations are subtracted from the weight vector on a per-sample basis. An unbiased estimate of the _true_ gradient (i.e., gradient computed over the full dataset) using only a single observation can be achieved when sampling the observation uniformly at random over the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbb84f",
   "metadata": {},
   "source": [
    "### 1.4. German Traffic Sign Recognition Benchmark (GTSRB) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403c8b8",
   "metadata": {},
   "source": [
    "The German Traffic Sign Recognition Benchmark is a multi-class, single-image classification challenge created by J. Stallkamp et al. (2012) at the [Institut für Neuroinformatik](https://benchmark.ini.rub.de/gtsrb_news.html) [1]. In this dataset there exists over 50.000 unique images of more than 40 distinct classes of traffic signs. \n",
    "\n",
    "Each image has been reliably annotated with the following information:\n",
    "* **Filename**: filename of the corresponding image;\n",
    "* **Width**: width of the image;\n",
    "* **Height**: height of the image;\n",
    "* **ROI.x1**: x-coordinate of top-left corner of traffic sign bounding box;\n",
    "* **ROI.y1**: y-coordinate of the top-left corner of traffic sign bounding box;\n",
    "* **ROI.x2**: x-coordinate of bottom-right corner of traffic sign bounding box;\n",
    "* **ROI.y2**: y-coordinate of the bottom-right corner of traffic sign bounding box; \n",
    "* **ClassId**: assigned class label.\n",
    "\n",
    "In addition to the CSV-formatted annotations, the following information about the images is provided:\n",
    "* Images contain one traffic sign each;\n",
    "* Images contain a border of 10% around the actual traffic sign (at least 5 pixels) to allow for edge-based approaches;\n",
    "* Images are stored in PPM format ([Portable Pixmap, P6](http://en.wikipedia.org/wiki/Netpbm_format));\n",
    "* Image sizes vary between 15x15 to 250x250 pixels;\n",
    "* Images are not necessarily squared;\n",
    "* The actual traffic sign is not necessarily centred within the image. This is true for images that were close to the image border in the full camera image;\n",
    "* The bounding box of the traffic sign is part of the annotations.\n",
    "\n",
    "Lastly, several pre-calculated feature sets are provided. Namely, _Histogram of Oriented Gradients_ (HOG) features, _Haar-like_ features (5 distinct Haar-like features), and _hue histograms_ (256-bin HSV colour space).\n",
    "\n",
    "This dataset presents unique real-world challenges within object recognition by providing traffic sign images captured in a variety of lighting/illumination conditions and images that have distortions (e.g., blurring, pixelation) as well as differences in shape, size, etc.\n",
    "\n",
    "For more information on the GTSRB dataset, see [here](https://benchmark.ini.rub.de/gtsrb_dataset.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec31b1b",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f074f",
   "metadata": {},
   "source": [
    "### 2.1. Custom model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b8953",
   "metadata": {},
   "source": [
    "The following functions from [Exercise 1.3.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-3-1-Logistic-Regression/2022-08-27-Logistic-Regression.ipynb) have been slightly modified to work on a single input observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc028d",
   "metadata": {},
   "source": [
    "#### Softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5aa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From J. Moran's `2022-08-27-Logistic-Regression.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits: tf.Tensor, stable: bool=False) -> tf.Tensor:\n",
    "    \"\"\"Returns the softmax probability distribution.\n",
    "    \n",
    "    :param logits: a 1xN tf.Tensor of logits.\n",
    "    :param stable: optional, flag indicating whether\n",
    "        or not to normalise the input data.\n",
    "    returns: soft_logits, a 1xN tf.Tensor of real \n",
    "        values in range (0,1) that sum up to 1.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(logits, tf.Tensor)\n",
    "    if stable:\n",
    "        logits = tf.subtract(logits, tf.reduce_max(logits))\n",
    "    soft_logits = tf.math.exp(logits)\n",
    "    soft_logits /= tf.math.reduce_sum(soft_logits)\n",
    "    return soft_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c3935",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Returns the per-sample cross-entropy loss.\n",
    "    \n",
    "    :param y_true: a 1xC tf.Tensor, the ground truth class label\n",
    "        as a one-hot encoded vector of length C (num of total classes).\n",
    "    :param y_pred: a 1xC tf.Tensor, the predicted per-class probabilities.\n",
    "    :returns: a 1x1 tf.Tensor, the categorical cross-entropy loss value\n",
    "        for a single observation and its ground truth label.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pick the probability value from the distribution\n",
    "    # that corresponds to the true class label\n",
    "    preds = tf.boolean_mask(y_pred, mask=y_true)\n",
    "    # Taking the negative log-likelihood\n",
    "    neg_log_likelihood = -tf.math.log(preds)\n",
    "    # Here we return the categorical cross-entropy loss\n",
    "    # value for a single observation (no need to normalise)\n",
    "    return tf.reduce_sum(neg_log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8655a9b0",
   "metadata": {},
   "source": [
    "#### Accuracy scoring metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c16638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Evaluates a single prediction against the ground truth.\n",
    "\n",
    "    :param y_true: a 1x1 scalar tf.Tensor, the ground truth class label\n",
    "        (not one-hot encoded).\n",
    "    :param y_pred: a 1x1 scalar tf.Tensor, the predicted class label.\n",
    "    returns: acc, a 1x1 scalar tf.Tensor object, 1.0 if correct else 0.0.\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.cast(tf.math.equal(y_true, y_pred), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08bb1a2",
   "metadata": {},
   "source": [
    "### 2.2. Modelling with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b95106",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3959c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params: List[tf.Tensor], grads: List[tf.Tensor], lr: float, bs: int=1):\n",
    "    \"\"\"Performs the stochastic gradient descent update step.\n",
    "    \n",
    "    SGD fits a linear model to the input data and target labels.\n",
    "    Here we assume a logistic regression implementation with a\n",
    "    softmax activation function and categorical cross-entropy loss.\n",
    "    \n",
    "    args:\n",
    "    - params [list[tensor]]: model params\n",
    "    - grads [list[tensor]]: param gradient such that params[0].shape == grad[0].shape\n",
    "    - lr [float]: learning rate\n",
    "    - bs [int]: batch_size\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418e08d",
   "metadata": {},
   "source": [
    "#### Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From J. Moran's `2022-08-27-Logistic-Regression.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bba478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "    \"\"\"Performs one step of the logistic regression model.\n",
    "    \n",
    "    :param X: tf.Tensor object, a training observation\n",
    "        i.e., a single HxWx3 RGB image.\n",
    "    :param W: tf.Tensor object, the weight vector.\n",
    "    :param b: the bias term, tf.Tensor-like object.\n",
    "    returns: tf.Tensor, the softmax probability distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(X, tf.Tensor)\n",
    "    assert isinstance(W, tf.Variable)\n",
    "    assert isinstance(b, tf.Variable)\n",
    "    # Compute the product between flattened input and weight vectors\n",
    "    Z = tf.matmul(tf.reshape(X, shape=(-1, W.shape[0])), W)\n",
    "    # Add the bias term\n",
    "    Z += b\n",
    "    # Return the softmax probabilities P\n",
    "    return softmax(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54e689",
   "metadata": {},
   "source": [
    "### 2.3. Training and validation loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b68e6d",
   "metadata": {},
   "source": [
    "#### Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_dataset, model, loss, optimizer):\n",
    "    \"\"\"\n",
    "    training loop\n",
    "    args:\n",
    "    - train_dataset: \n",
    "    - model [func]: model function\n",
    "    - loss [func]: loss function\n",
    "    - optimizer [func]: optimizer func\n",
    "    returns:\n",
    "    - mean_loss [tensor]: mean training loss\n",
    "    - mean_acc [tensor]: mean training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    for X_train, y_train in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # IMPLEMENT THIS FUNCTION\n",
    "            pass\n",
    "    mean_acc = tf.math.reduce_mean(tf.concat(accuracies, axis=0))\n",
    "    mean_loss = tf.math.reduce_mean(losses)\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ccc13",
   "metadata": {},
   "source": [
    "#### Custom validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b484a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(val_dataset, model):\n",
    "    \"\"\"\n",
    "    training loop\n",
    "    args:\n",
    "    - train_dataset: \n",
    "    - model [func]: model function\n",
    "    - loss [func]: loss function\n",
    "    - optimizer [func]: optimizer func\n",
    "    returns:\n",
    "    - mean_acc [tensor]: mean validation accuracy\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7d557",
   "metadata": {},
   "source": [
    "### 2.4. Evaluation on the GTSRB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925dfc0e",
   "metadata": {},
   "source": [
    "#### Considerations for our input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd5cb6",
   "metadata": {},
   "source": [
    "The following `get_datasets()` method returns a tuple of [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances containing the training and validation datasets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f7f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `dataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(imdir: str) -> tuple:\n",
    "    \"\"\"Return the training and validation datasets.\n",
    "    \n",
    "    :param imdir: absolute path to the directory where the data is stored in.\n",
    "    :returns: (train_dataset, val_dataset), tuple of tf.data.Dataset instances.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "                        imdir, \n",
    "                        image_size=(32, 32),\n",
    "                        batch_size=256,\n",
    "                        validation_split=0.1,\n",
    "                        subset='training',\n",
    "                        seed=123\n",
    "    )\n",
    "    val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "                        imdir, \n",
    "                        image_size=(32, 32),\n",
    "                        batch_size=256,\n",
    "                        validation_split=0.1,\n",
    "                        subset='validation',\n",
    "                        seed=123\n",
    "    )\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10192a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3932af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdir = os.path.join(DIR_SRC, 'GTSRB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = get_datasets(imdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750dbfec",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6e3a1",
   "metadata": {},
   "source": [
    "##### Initialising model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the model input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a228cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 1024*3\n",
    "num_outputs = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760cf215",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdir = os.path.join(DIR_SRC, 'GTSRB')\n",
    "epochs = None\n",
    "batch_size = None\n",
    "lr = None\n",
    "args = {'imdir': , 'epochs': , 'batch_size': batch_size, 'lr': lr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928427a",
   "metadata": {},
   "source": [
    "We will use the TensorFlow built-in [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable?hl=en) tensor object to distinguish our trainable model parameters (weight, bias vectors) from otherwise static tensor objects. These `tf.Variable` objects maintain a shared, persistent state and therefore come with a few useful operations (e.g., [`assign_sub()`](https://www.tensorflow.org/api_docs/python/tf/Variable#assign_sub) method) that we will use during training to manipulate their values. Another nice feature of the `tf.Variable` is that they are automatically traced and watched during the `tf.GradientTape` steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ccd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialising the model variables (weights and bias vectors)\n",
    "W = tf.Variable(tf.random.normal(shape=(num_inputs, num_outputs), mean=0, stddev=0.01))\n",
    "b = tf.Variable(tf.zeros(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9c9fd",
   "metadata": {},
   "source": [
    "##### Performing the training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_logger(mod_name):\n",
    "    logger = logging.getLogger(mod_name)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the console logger\n",
    "logger = get_module_logger(__name__)\n",
    "logger.info(f'Training for {args['batch_size']} epochs using {args['imdir']} data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03511065",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    logger.info(f'Epoch {epoch}')\n",
    "    ### Perform stochastic gradient descent over training data\n",
    "    loss, acc = training_loop(X_train, model, negative_log_likelihood, sgd)\n",
    "    logger.info(f'Mean training loss: {loss}, mean training accuracy: {acc}')\n",
    "    ### Compute validation set accuracy\n",
    "    val_acc = validation_loop(val_datset)\n",
    "    logger.info(f'Mean validation accuracy {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cddfd2",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ac154",
   "metadata": {},
   "source": [
    "A training loop goes through element of the training dataset and uses it to update the model's weights.\n",
    "A validation loop goes through each element of the validation dataset and uses it to calculate\n",
    "the metrics (eg, accuracy). We call **epoch** an iteration of one training loop and one validation loop.\n",
    "\n",
    "The input to your model should be normalized. You can do this by dividing them by 255: `X /= 255`.\n",
    "\n",
    "You can run `python training.py` to train your first machine learning model!\n",
    "\n",
    "You will need to specify the `--imdir`, e.g. `--imdir GTSRB/Final_Training/Images/`, using the provided GTSRB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82dcf5",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3f9b4",
   "metadata": {},
   "source": [
    "You don't need `tf.GradientTape` for the validation loop as you will not be updating gradients. \n",
    "\n",
    "The `assign_sub` Variable method will be useful to perform the weights update in the sgd optimizer.\n",
    "\n",
    "Use the `tf.one_hot` function to get the one vector from the ground truth label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660235f",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cf4af",
   "metadata": {},
   "source": [
    "This assignment was prepared by Thomas Hossler and Michael Virgo et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "References\n",
    "* [1] Stallkamp, J. et al. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural Networks. 32:323-332. [doi:10.1016/j.neunet.2012.02.016](https://doi.org/10.1016/j.neunet.2012.02.016).\n",
    "\n",
    "Helpful resources:\n",
    "   * [An overview of gradient descent optimization algorithms | S. Ruder](https://ruder.io/optimizing-gradient-descent/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
