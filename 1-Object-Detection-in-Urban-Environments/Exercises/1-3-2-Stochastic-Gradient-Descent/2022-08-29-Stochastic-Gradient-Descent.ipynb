{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f13c1a",
   "metadata": {},
   "source": [
    "# Exercise 2 - Custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec40be",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca14290",
   "metadata": {},
   "source": [
    "In this exercise, you have to implement your first training and validation loops from scratch to train\n",
    "the logistic model you implemented. To do so, you will also have to create an optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4e8ab",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d264e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = False                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c878b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec31b1b",
   "metadata": {},
   "source": [
    "## 2. Programming Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5aa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From J. Moran's `2022-08-27-Logistic-Regression.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits, stable=False):\n",
    "    \"\"\"Returns the softmax probability distribution.\n",
    "    \n",
    "    :param logits: a 1xN tf.Tensor of logits.\n",
    "    :param stable: optional, flag indicating whether\n",
    "        or not to normalise the input data.\n",
    "    returns: soft_logits, a 1xN tf.Tensor of real \n",
    "        values in range (0,1) that sum up to 1.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(logits, tf.Tensor)\n",
    "    if stable:\n",
    "        logits = tf.subtract(logits, tf.reduce_max(logits))\n",
    "    soft_logits = tf.math.exp(logits)\n",
    "    soft_logits /= tf.math.reduce_sum(soft_logits)\n",
    "    return soft_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(scaled_logits, one_hot, use_numpy=True):\n",
    "    \"\"\"Returns the cross-entropy loss.\n",
    "    \n",
    "    :param scaled_logits: an NxC tf.Tensor of scaled softmax\n",
    "        distribution values, [n_samples x n_classes].\n",
    "    :param one_hot: an NxC tf.Tensor of one-hot encoded \n",
    "        ground truth labels, [n_samples x n_classes].\n",
    "    :param use_numpy: optional, uses  Numpy multidimensional\n",
    "        array indexing on type-casted tf.experimental.numpy\n",
    "        ndarrays, uses boolean masking if False.\n",
    "    :returns: loss, a 1x1 tf.Tensor with cross-entropy loss. \n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(scaled_logits, tf.Tensor)\n",
    "    assert isinstance(one_hot, tf.Tensor)\n",
    "    if use_numpy:\n",
    "        n_samples = one_hot.shape[0]\n",
    "        class_labels = tf.math.argmax(one_hot, axis=1)\n",
    "        preds = tnp.asarray(scaled_logits)[range(n_samples), class_labels]\n",
    "        log_likelihood = -tf.math.log(preds)\n",
    "    else:\n",
    "        n_samples = one_hot.shape[0]\n",
    "        # For each sample, pick the probability value from the distribution\n",
    "        # that corresponds to the true class label\n",
    "        preds = tf.boolean_mask(scaled_logits, one_hot)\n",
    "        # Taking the negative log-likelihood\n",
    "        log_likelihood = -tf.math.log(preds)\n",
    "    # Normalising by the sample size\n",
    "    loss = tf.math.reduce_sum(log_likelihood) / n_samples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c16638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Calculates the average correct predictions.\n",
    "\n",
    "    :param y_hat: tf.Tensor, NxC tensor-like object of \n",
    "        models predictions [n_samples x n_classes].\n",
    "    :param y: tf.Tensor, N-dimensional tensor of\n",
    "        ground truth class labels (not one-hot encoded).\n",
    "    returns: acc, a 1x1 scalar tf.Tensor-like object\n",
    "        with the accuracy score (correct / total predictions).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(y, tf.Tensor) and isinstance(y_hat, tf.Tensor)\n",
    "    # Get predicted labels with highest probabilities\n",
    "    y_preds = tf.cast(tf.math.argmax(y_hat, axis=1), dtype=y.dtype)\n",
    "    # Get number of correct predictions\n",
    "    n_correct = tf.math.count_nonzero(tf.cast(tf.math.equal(y_preds, y), dtype=tf.int32))\n",
    "    # Compute average correct predictions\n",
    "    acc = n_correct / y_hat.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30921d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_logger(mod_name):\n",
    "    logger = logging.getLogger(mod_name)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, grad, lr, bs):\n",
    "    \"\"\"\n",
    "    stochastic gradient descent implementation\n",
    "    args:\n",
    "    - params [list[tensor]]: model params\n",
    "    - grad [list[tensor]]: param gradient such that params[0].shape == grad[0].shape\n",
    "    - lr [float]: learning rate\n",
    "    - bs [int]: batch_size\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_dataset, model, loss, optimizer):\n",
    "    \"\"\"\n",
    "    training loop\n",
    "    args:\n",
    "    - train_dataset: \n",
    "    - model [func]: model function\n",
    "    - loss [func]: loss function\n",
    "    - optimizer [func]: optimizer func\n",
    "    returns:\n",
    "    - mean_loss [tensor]: mean training loss\n",
    "    - mean_acc [tensor]: mean training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    for X, Y in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # IMPLEMENT THIS FUNCTION\n",
    "            pass\n",
    "    mean_acc = tf.math.reduce_mean(tf.concat(accuracies, axis=0))\n",
    "    mean_loss = tf.math.reduce_mean(losses)\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b484a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(val_dataset, model):\n",
    "    \"\"\"\n",
    "    training loop\n",
    "    args:\n",
    "    - train_dataset: \n",
    "    - model [func]: model function\n",
    "    - loss [func]: loss function\n",
    "    - optimizer [func]: optimizer func\n",
    "    returns:\n",
    "    - mean_acc [tensor]: mean validation accuracy\n",
    "    \"\"\"\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f7f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `dataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(imdir):\n",
    "    train_dataset = image_dataset_from_directory(imdir, \n",
    "                                       image_size=(32, 32),\n",
    "                                       batch_size=256,\n",
    "                                       validation_split=0.1,\n",
    "                                       subset='training',\n",
    "                                       seed=123)\n",
    "    val_dataset = image_dataset_from_directory(imdir, \n",
    "                                        image_size=(32, 32),\n",
    "                                        batch_size=256,\n",
    "                                        validation_split=0.1,\n",
    "                                        subset='validation',\n",
    "                                        seed=123)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a78867",
   "metadata": {},
   "source": [
    "### Evaluating stochastic gradient descent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58484122",
   "metadata": {},
   "source": [
    "#### Considerations for our input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143d062",
   "metadata": {},
   "source": [
    "*Notes here on German Traffic Sign Recognition Benchmark dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760cf215",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdir = os.path.join(DIR_SRC, 'GTSRB')\n",
    "epochs = None\n",
    "batch_size = None\n",
    "lr = None\n",
    "args = {'imdir': , 'epochs': , 'batch_size': batch_size, 'lr': lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the console logger\n",
    "logger = get_module_logger(__name__)\n",
    "logger.info(f'Training for {args['batch_size']} epochs using {args['imdir']} data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08756a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = get_datasets(args['imdir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 1024*3\n",
    "num_outputs = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ccd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialising the model variables (weights and bias vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal(shape=(num_inputs, num_outputs), mean=0, stddev=0.01))\n",
    "b = tf.Variable(tf.zeros(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03511065",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    logger.info(f'Epoch {epoch}')\n",
    "    ### Perform stochastic gradient descent over training data\n",
    "    loss, acc = training_loop(X_train, model, negative_log_likelihood, sgd)\n",
    "    logger.info(f'Mean training loss: {loss}, mean training accuracy: {acc}')\n",
    "    ### Compute validation set accuracy\n",
    "    val_acc = validation_loop(val_datset)\n",
    "    logger.info(f'Mean validation accuracy {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cddfd2",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ac154",
   "metadata": {},
   "source": [
    "A training loop goes through element of the training dataset and uses it to update the model's weights.\n",
    "A validation loop goes through each element of the validation dataset and uses it to calculate\n",
    "the metrics (eg, accuracy). We call **epoch** an iteration of one training loop and one validation loop.\n",
    "\n",
    "The input to your model should be normalized. You can do this by dividing them by 255: `X /= 255`.\n",
    "\n",
    "You can run `python training.py` to train your first machine learning model!\n",
    "\n",
    "You will need to specify the `--imdir`, e.g. `--imdir GTSRB/Final_Training/Images/`, using the provided GTSRB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82dcf5",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3f9b4",
   "metadata": {},
   "source": [
    "You don't need `tf.GradientTape` for the validation loop as you will not be updating gradients. \n",
    "\n",
    "The `assign_sub` Variable method will be useful to perform the weights update in the sgd optimizer.\n",
    "\n",
    "Use the `tf.one_hot` function to get the one vector from the ground truth label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
