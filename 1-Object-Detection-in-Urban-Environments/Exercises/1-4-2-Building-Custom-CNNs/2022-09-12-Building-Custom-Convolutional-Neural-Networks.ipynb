{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ebf9df",
   "metadata": {},
   "source": [
    "# Exercise 1.4.2 - Building Custom CNNs\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34676065",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a39a9",
   "metadata": {},
   "source": [
    "* Use the TensorFlow Keras [Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to create a convolutional neural network ([CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network));\n",
    "* Train the custom CNN model on the German Traffic Sign Recognition Benchmark [GTSRB](https://benchmark.ini.rub.de/gtsrb_dataset.html) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddea45f",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc75485",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f75984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c54e9",
   "metadata": {},
   "source": [
    "### 1.1. Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87a453",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ae852",
   "metadata": {},
   "source": [
    "A [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (\"ConvNet\") is a special type of neural network particularly well-suited to handle image data. Like the feedforward neural networks we saw in [Exercise 1.3.3](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-3-3-Image-Classification-FNNs/2022-09-05-Image-Classification-Feed-Forward-Neural-Networks.ipynb), CNNs are composed of layers of neurons that have trainable weights and biases. In addition, CNNs often use activation functions and fully-connected layers similar to FNNs (e.g., softmax).\n",
    "\n",
    "With CNNs, we make the assumption that we will always be using image data as our inputs. Since image data is multi-dimensional, resizing these $\\left[H, W, D\\right]$ arrays to flattened $\\left[1x\\left(H*W*D\\right)\\right]$ vectors is necessary when using fully-connected layers, such as in feedforward neural networks. With ConvNets, however, we make use of locally-connected layers that operate over small portions of _volumetric data_. That is, ConvNets allow us to preserve the existing structure of image data and drastically reduce the amount of trainable parameters needed in each layer of our network. For example, a single fully-connected neuron will require a number of weight values equal to the number of attributes in our image input. For an image of size $\\left[64, 64, 3\\right]$, we obtain $(64*64*3) = 12288$ weight values _per neuron_. For a layer with only $n = 10$ fully-connected neurons, the number of trainable weights we have skyrockets to an astonishing $n * 12288 = 122880$ parameters per layer. We will see in this exercise that by limiting the connectivity of neurons between layers, we can design a network that reduces the number of parameters required while simultaneously _increasing_ the number of layers in the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da97ab",
   "metadata": {},
   "source": [
    "#### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e7064",
   "metadata": {},
   "source": [
    "Most, if not all, convolutional neural network architectures use some combination of the following three distinct layer types: _convolutional layer_, _pooling layer_, and _fully-connected layer_. We have implemented fully-connected layers in [Exercise 1.3.3](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-3-3-Image-Classification-FNNs/2022-09-05-Image-Classification-Feed-Forward-Neural-Networks.ipynb) for feedforward neural networks, and their application in ConvNets is no different. However, we introduce the _convolutional layer_ in this exercise. We will touch on these in more detail in just a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857ef7a",
   "metadata": {},
   "source": [
    "##### Input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbc687",
   "metadata": {},
   "source": [
    "It is important to first note that our _input layer_ corresponds directly to the dimensionality of our image data. One major assumption we make with convolutional neural networks is that our input image dimensions are constant. That is, that every image passing through our network shares the same _height_, _width_, and _channel_ dimensions. In ConvNets, the _depth_ of an image (its colour channels, e.g., R, G, B) is maintained throughout the network while the height and width might change. For convolutional and pooling layers, this matters because each kernel and filter will require an extra dimension $D$ equal to the number of colour channels in an image in addition to their width and height attributes. For an RGB image, a kernel/filter of size $\\left(h \\ x \\ w\\right)$ will therefore have dimensions $\\left(h \\ x \\ w \\ x \\ 3\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47030a9a",
   "metadata": {},
   "source": [
    "##### Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d76cb",
   "metadata": {},
   "source": [
    "Very similar to the pooling layers we studied in [Exercise 1.4.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-4-1-Pooling-Layers-CNNs/2022-09-07-Pooling-Layers-Convolutional-Neural-Networks.ipynb), these layers take an input, the volumetric image data, and perform sliding window operations ([convolutions](https://en.wikipedia.org/wiki/Convolution)) over the image. The convolution layer, like pooling layers, have _stride_, _kernel size_ and _padding_ attributes. We also introduce a _filter_ hyperparameter which allows us to specify the desired dimensionality of our convolution layer output. Convolutional layers rarely ever have just _one filter_. In fact, it is very common to see a single convolutional layer have anywhere from 32 filters up to 512. Each _filter_ represents a _feature_ in the input image (hence the output name \"feature map\"). These filters are templates which, when convolved with an input image, help locate and identify vertical, horizontal edges, arches, diagonal lines, etc.\n",
    "\n",
    "The convolutional layer in a network is often directly followed by an element-wise activation layer such as the Rectified Linear Unit ([ReLU](https://en.wikipedia.org/wiki/Rectifier_\\(neural_networks\\))) to introduce further non-linearities into the network. Note that the activation layer's output dimensions remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988341c",
   "metadata": {},
   "source": [
    "##### Pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7aed9b",
   "metadata": {},
   "source": [
    "Pooling layers, as mentioned in the previous Exercise 1.4.1, reduce the size (spatial dimensions) of the images through a downsampling operation. By \"summarising\" neighbouring pixel values with e.g., _max_, _min_, or _average_ calculations we are able to transform the output and effectively manipulate the [receptive field](https://en.wikipedia.org/wiki/Receptive_field#In_the_context_of_neural_networks) of the image (the region within an image of which a particular neuron is sensitive to). By shrinking the image, we are essentially _increasing_ the amount of its pixels (information) we can fit into a single \"patch\" for a neuron to consider.\n",
    "\n",
    "As the image becomes reduced in size as it passes through the network, neurons in deeper layers get to \"see\" larger and larger parts of the whole image. In turn, we can increase the receptive field of a network exponentially while increasing the number of parameters linearly. This trade-off is what affords us the ability to swap only a few fully-connected layers for _handfuls_ of convolutional stacks without increasing complexity or decreasing the receptive field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419949c",
   "metadata": {},
   "source": [
    "##### Fully-connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64c5fb",
   "metadata": {},
   "source": [
    "While we aren't able to eliminate fully-connected layers altogether, their use in convolutional neural networks is limited. Fully-connected layers appear at the end of a network for classification; the number of neurons of the last fully-connected layer in a CNN equals the number of distinct classes predicted. Practitioners may also choose to implement several preceding fully-connected layers with an arbitrarily large number of neurons, e.g., 512, 256, 64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c710ad0",
   "metadata": {},
   "source": [
    "#### In summary\n",
    "   * Inputs to a ConvNet are volumetric image data with fixed dimensions;\n",
    "   * We use pooling layers to _downsample_ images (reduce their spatial dimensions);\n",
    "   * By reducing the spatial dimensions, we _increase_ the neurons' receptive fields (how much of the image they can \"see\");\n",
    "   * With convolutional networks we are able to eventually cover the entire region of an input image at a cost significantly reduced to fully-connected layers;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bd32a",
   "metadata": {},
   "source": [
    "We have now learned the benefits of convolutional architectures to more-traditional neural networks (specifically, feedforward neural networks); by adding \"stacks\" of convolutional/pooling layers, we can exploit the complexity-efficiency trade-off. Convolutional stacks help us set reasonable limits on neural network complexity by decreasing the number of overall trainable parameters we have to work with layer-for-layer. This not only improves training efficiency but also helps boost performance on image classification tasks.\n",
    "\n",
    "While several other CNN architectures make use of even greater improvements, such as _dropout_/_inverted dropout_ and _batch normalisation_, we will limit our discussion on those topics for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a5e3e",
   "metadata": {},
   "source": [
    "### 1.2. LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ea42d",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7428ab",
   "metadata": {},
   "source": [
    "Using the Keras API, you have to create a small convolutional neural networks using less than 15 layers, containing at least one convolutional layer, one pooling layer and one dense (fully connected layer). You can find a list of the different layers available [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    net = tf.keras.models.Sequential()\n",
    "    # IMPLEMENT THIS FUNCTION\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe222bd",
   "metadata": {},
   "source": [
    "You should experiment with different designs (number of layers, types of pooling, filter sizes, number of fully connected layers, number of neurons)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5621e3",
   "metadata": {},
   "source": [
    "You will need to feed the image directory to `training.py` (`GTSRB/Final_Training/Images/`) with `-d`, and can view the final metrics visualization in the Desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_logger(mod_name):\n",
    "    logger = logging.getLogger(mod_name)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8eb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_module_logger(__name__)\n",
    "parser = argparse.ArgumentParser(description='Download and process tf files')\n",
    "parser.add_argument('-d', '--imdir', required=True, type=str,\n",
    "                    help='data directory')\n",
    "parser.add_argument('-e', '--epochs', default=10, type=int,\n",
    "                    help='Number of epochs')\n",
    "args = parser.parse_args()    \n",
    "\n",
    "logger.info(f'Training for {args.epochs} epochs using {args.imdir} data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image,label):\n",
    "    \"\"\" small function to normalize input images \"\"\"\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(imdir):\n",
    "    \"\"\" extract GTSRB dataset from directory \"\"\"\n",
    "    train_dataset = image_dataset_from_directory(imdir, \n",
    "                                       image_size=(32, 32),\n",
    "                                       batch_size=32,\n",
    "                                       validation_split=0.2,\n",
    "                                       subset='training',\n",
    "                                       seed=123,\n",
    "                                       label_mode='int')\n",
    "\n",
    "    val_dataset = image_dataset_from_directory(imdir, \n",
    "                                        image_size=(32, 32),\n",
    "                                        batch_size=32,\n",
    "                                        validation_split=0.2,\n",
    "                                        subset='validation',\n",
    "                                        seed=123,\n",
    "                                        label_mode='int')\n",
    "    train_dataset = train_dataset.map(process)\n",
    "    val_dataset = val_dataset.map(process)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae21df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928dd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94076d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the datasets\n",
    "train_dataset, val_dataset = get_datasets(args.imdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "history = model.fit(x=train_dataset, \n",
    "                    epochs=args.epochs, \n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(history):\n",
    "    \"\"\" plot loss and accuracy from keras history object \"\"\"\n",
    "    f, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax[0].plot(history.history['loss'], linewidth=3)\n",
    "    ax[0].plot(history.history['val_loss'], linewidth=3)\n",
    "    ax[0].set_title('Loss', fontsize=16)\n",
    "    ax[0].set_ylabel('Loss', fontsize=16)\n",
    "    ax[0].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[0].legend(['train loss', 'val loss'], loc='upper right')\n",
    "    ax[1].plot(history.history['accuracy'], linewidth=3)\n",
    "    ax[1].plot(history.history['val_accuracy'], linewidth=3)\n",
    "    ax[1].set_title('Accuracy', fontsize=16)\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=16)\n",
    "    ax[1].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[1].legend(['train acc', 'val acc'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1dce5",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb64e82",
   "metadata": {},
   "source": [
    "A good starting point for small networks is LeNet5. You will find many existing implementations online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9df23",
   "metadata": {},
   "source": [
    "Don't forget the basic structure of a convnet: convolutional layer, activation and pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5127941",
   "metadata": {},
   "source": [
    "You can use the [`summary`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary) method of the Keras model API to print the description of your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
