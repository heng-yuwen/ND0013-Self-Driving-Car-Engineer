{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c8fd7ff",
      "metadata": {
        "id": "8c8fd7ff"
      },
      "source": [
        "# Exercise 1.4.2 - Building Custom CNNs\n",
        "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
        "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d90eed14",
      "metadata": {
        "id": "d90eed14"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ef1e77",
      "metadata": {
        "id": "06ef1e77"
      },
      "source": [
        "* Use the TensorFlow Keras [Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to create a convolutional neural network ([CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network));\n",
        "* Implement the [LeNet-5](https://en.wikipedia.org/wiki/LeNet) CNN architecture;\n",
        "* Train the LeNet-5 on the modified [MNIST digits](http://yann.lecun.com/exdb/mnist/) dataset;\n",
        "* Train a CNN model (e.g., LeNet-5) on the German Traffic Sign Recognition Benchmark [GTSRB](https://benchmark.ini.rub.de/gtsrb_dataset.html) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e859be21",
      "metadata": {
        "id": "e859be21"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e81ad1b",
      "metadata": {
        "id": "0e81ad1b"
      },
      "outputs": [],
      "source": [
        "### Importing the required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b25a0963",
      "metadata": {
        "id": "b25a0963"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from typing import Generator, Iterator, List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VlCuik5WFo-G",
        "outputId": "5a082564-c913-404c-abef-52cde69c323a"
      },
      "id": "VlCuik5WFo-G",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Bdl34d5jFo6V",
        "outputId": "1916352e-9ff6-4142-e882-066b61d34cbf"
      },
      "id": "Bdl34d5jFo6V",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting the environment variables"
      ],
      "metadata": {
        "id": "14SKbFEhFowk"
      },
      "id": "14SKbFEhFowk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_COLAB = True                # True if running in Google Colab instance"
      ],
      "metadata": {
        "id": "QRK1yNpUFyNg"
      },
      "id": "QRK1yNpUFyNg",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Root directory\n",
        "DIR_BASE = '' if not ENV_COLAB else '/content/'"
      ],
      "metadata": {
        "id": "mo7YMWrIFyJW"
      },
      "id": "mo7YMWrIFyJW",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subdirectory to save output files\n",
        "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
        "# Subdirectory pointing to input data\n",
        "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
      ],
      "metadata": {
        "id": "600UKHaQFyFI"
      },
      "id": "600UKHaQFyFI",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Unzipping the GTSRB dataset\n",
        "!unzip -q /content/GTSRB.zip -d /content/data/"
      ],
      "metadata": {
        "id": "fyBCboPTFyA_"
      },
      "id": "fyBCboPTFyA_",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating subdirectories (if not exists)\n",
        "os.makedirs(DIR_OUT, exist_ok=True)"
      ],
      "metadata": {
        "id": "vAjY0rSzF4Of"
      },
      "id": "vAjY0rSzF4Of",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3145fd5c",
      "metadata": {
        "id": "3145fd5c"
      },
      "source": [
        "### 1.1. Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e247b07b",
      "metadata": {
        "id": "e247b07b"
      },
      "source": [
        "#### Background"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e145f5",
      "metadata": {
        "id": "d0e145f5"
      },
      "source": [
        "A [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (\"ConvNet\") is a special type of neural network particularly well-suited to handle image data. Like the feedforward neural networks we saw in [Exercise 1.3.3](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-3-3-Image-Classification-FNNs/2022-09-05-Image-Classification-Feed-Forward-Neural-Networks.ipynb), CNNs are composed of layers of neurons that have trainable weights and biases. In addition, CNNs often use activation functions and fully-connected layers similar to FNNs (e.g., softmax).\n",
        "\n",
        "With CNNs, we make the assumption that we will always be using image data as our inputs. Since image data is multi-dimensional, resizing these $\\left[H, W, D\\right]$ arrays to flattened $\\left[1x\\left(H*W*D\\right)\\right]$ vectors is necessary when using fully-connected layers, such as in feedforward neural networks. With ConvNets, however, we make use of locally-connected layers that operate over small portions of _volumetric data_. That is, ConvNets allow us to preserve the existing structure of image data and drastically reduce the amount of trainable parameters needed in each layer of our network. For example, a single fully-connected neuron will require a number of weight values equal to the number of attributes in our image input. For an image of size $\\left[64, 64, 3\\right]$, we obtain $(64*64*3) = 12288$ weight values _per neuron_. For a layer with only $n = 10$ fully-connected neurons, the number of trainable weights we have skyrockets to an astonishing $n * 12288 = 122880$ parameters per layer. We will see in this exercise that by limiting the connectivity of neurons between layers, we can design a network that reduces the number of parameters required while simultaneously _increasing_ the number of layers in the architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f117afd2",
      "metadata": {
        "id": "f117afd2"
      },
      "source": [
        "![Fig. 1. The convolutional neural network architecture (simplified).](figures/2022-09-12-Figure-1-Convolutional-Neural-Network-Simplified.png)\n",
        "\n",
        "$$\n",
        "\\textrm{Fig. 1. The convolutional neural network architecture (simplified)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ced2a2da",
      "metadata": {
        "id": "ced2a2da"
      },
      "source": [
        "#### Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e40b6610",
      "metadata": {
        "id": "e40b6610"
      },
      "source": [
        "Most, if not all, convolutional neural network architectures use some combination of the following three distinct layer types: _convolutional layer_, _pooling layer_, and _fully-connected layer_. We have implemented fully-connected layers in [Exercise 1.3.3](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-3-3-Image-Classification-FNNs/2022-09-05-Image-Classification-Feed-Forward-Neural-Networks.ipynb) for feedforward neural networks, and their application in ConvNets is no different. However, we introduce the _convolutional layer_ in this exercise. We will touch on these in more detail in just a minute."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40bbf870",
      "metadata": {
        "id": "40bbf870"
      },
      "source": [
        "##### Input layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "996a3410",
      "metadata": {
        "id": "996a3410"
      },
      "source": [
        "It is important to first note that our _input layer_ corresponds directly to the dimensionality of our image data. One major assumption we make with convolutional neural networks is that our input image dimensions are constant. That is, that every image passing through our network shares the same _height_, _width_, and _channel_ dimensions. In ConvNets, the _depth_ of an image (its colour channels, e.g., R, G, B) is maintained throughout the network while the height and width might change. For convolutional and pooling layers, this matters because each kernel and filter will require an extra dimension $D$ equal to the number of colour channels in an image in addition to their width and height attributes. For an RGB image, a kernel/filter of size $\\left(h \\ x \\ w\\right)$ will therefore have dimensions $\\left(h \\ x \\ w \\ x \\ 3\\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63727ab3",
      "metadata": {
        "id": "63727ab3"
      },
      "source": [
        "##### Convolutional layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c364662b",
      "metadata": {
        "id": "c364662b"
      },
      "source": [
        "Very similar to the pooling layers we studied in [Exercise 1.4.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-4-1-Pooling-Layers-CNNs/2022-09-07-Pooling-Layers-Convolutional-Neural-Networks.ipynb), these layers take an input, the volumetric image data, and perform sliding window operations ([convolutions](https://en.wikipedia.org/wiki/Convolution)) over the image. The convolution layer, like pooling layers, have _stride_, _kernel size_ and _padding_ attributes. We also introduce a _filter_ hyperparameter which allows us to specify the desired dimensionality of our convolution layer output. Convolutional layers rarely ever have just _one filter_. In fact, it is very common to see a single convolutional layer have anywhere from 32 filters up to 512. Each _filter_ represents a _feature_ in the input image (hence the output name \"feature map\"). These filters are templates which, when convolved with an input image, help locate and identify vertical, horizontal edges, arches, diagonal lines, etc.\n",
        "\n",
        "The convolutional layer in a network is often directly followed by an element-wise activation layer such as the Rectified Linear Unit ([ReLU](https://en.wikipedia.org/wiki/Rectifier_\\(neural_networks\\))) to introduce further non-linearities into the network. Note that the activation layer's output dimensions remain constant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50502aca",
      "metadata": {
        "id": "50502aca"
      },
      "source": [
        "##### Pooling layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcabccc2",
      "metadata": {
        "id": "fcabccc2"
      },
      "source": [
        "Pooling layers, as mentioned in the previous Exercise 1.4.1, reduce the size (spatial dimensions) of the images through a downsampling (averaging) or sub-sampling (max/min) operation. By replacing adjacent pixel values with neighbourhood calculations we are able to transform the output and effectively manipulate (increase _multiplicatively_) the [receptive field](https://en.wikipedia.org/wiki/Receptive_field#In_the_context_of_neural_networks) of the image (i.e., the region within an image of which a particular neuron is sensitive to). On the other hand, convolutional layers increase the receptive field _linearly_ proportional to the kernel size. Cascading convolutional layers with _dialated filters_ increase the receptive field exponentially [1]. By shrinking the image, we are essentially _increasing_ the amount of its pixels (information) we can fit into a single \"patch\" for a neuron to consider.\n",
        "\n",
        "As the image becomes reduced in size as it passes through the network, neurons in deeper layers get to \"see\" larger and larger parts of the whole image. In turn, we can increase the receptive field of a network exponentially while increasing the number of parameters linearly. This trade-off is what affords us the ability to swap only a few fully-connected layers for _handfuls_ of convolutional stacks without increasing complexity or decreasing the receptive field."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c79bbc9d",
      "metadata": {
        "id": "c79bbc9d"
      },
      "source": [
        "##### Fully-connected layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a51e94",
      "metadata": {
        "id": "d0a51e94"
      },
      "source": [
        "While we aren't able to eliminate fully-connected layers altogether, their use in convolutional neural networks is limited. Fully-connected layers appear at the end of a network for classification; the number of neurons of the last fully-connected layer in a CNN equal the number of distinct classes predicted (e.g., there are $3$ distinct classes being predicted in the example shown in Fig. 1). Practitioners may also choose to implement several preceding fully-connected layers with an arbitrarily large number of neurons, e.g., 512, 256, 64."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6d040a",
      "metadata": {
        "id": "ec6d040a"
      },
      "source": [
        "#### In summary\n",
        "   * Inputs to a ConvNet are volumetric image data with fixed dimensions;\n",
        "   * We use pooling layers to _downsample_ or _sub-sample_ images (reduce their spatial dimensions);\n",
        "   * By reducing the spatial dimensions, we _increase_ the neurons' receptive fields (how much of the image they can \"see\");\n",
        "   * With convolutional networks we are able to eventually cover the entire region of an input image at a cost significantly reduced to fully-connected layers;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad873e8",
      "metadata": {
        "id": "cad873e8"
      },
      "source": [
        "We have now learned the benefits of convolutional architectures to more-traditional neural networks (specifically, feedforward neural networks); by adding \"stacks\" of convolutional/pooling layers, we can exploit the complexity-efficiency trade-off. Convolutional stacks help us set reasonable limits on neural network complexity by decreasing the number of overall trainable parameters we have to work with layer-for-layer. This not only improves training efficiency but also helps boost performance on image classification tasks.\n",
        "\n",
        "While popular CNN architectures often make use of even more efficiency-focused layers, such as _dropout_/_inverted dropout_, _skip connections_, and _batch normalisation_, we will limit our discussion on those topics for now."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c4503d3",
      "metadata": {
        "id": "2c4503d3"
      },
      "source": [
        "### 1.2. LeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fc7fafa",
      "metadata": {
        "id": "4fc7fafa"
      },
      "source": [
        "#### Background"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4365ef26",
      "metadata": {
        "id": "4365ef26"
      },
      "source": [
        "![Fig. 2. The LeNet-5 convolutional neural network architecture for digits recognition.](figures/2022-09-12-Figure-2-LeNet-5-CNN-Architecture.png)\n",
        "\n",
        "$$\n",
        "\\textrm{Fig. 2. The LeNet-5 convolutional neural network architecture for digits recognition (proposed in LeCun 1998:7).}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cc2883",
      "metadata": {
        "id": "30cc2883"
      },
      "source": [
        "The [LeNet-5](https://en.wikipedia.org/wiki/LeNet) is a convolutional neural network architecture proposed by Yann LeCun et al. of Bell Labs in 1998 [2]. LeNet-5 is recognised in the deep learning community as being one of the earliest ConvNet architectures with work dating back to [1989](https://doi.org/10.1162%2Fneco.1989.1.4.541). LeCun's model (illustrated in Fig. 2) has all the basic units of a convolutional neural network: convolutional layers, pooling layers and fully-connected layers. Like most ConvNet architectures we studied, the LeNet-5 has alternating convolutional and sub-sampling (average pooling) layers. Also as expected we see that the number of filters/kernel windows increases as we go deeper in the network, starting with $6$ and increasing to $16$ in the final convolutional layer. In Fig. 2, we can interpret the number preceeding the \"@\" symbol as the number of filters/kernels of each layer, followed by the filter/kernel size (omitting the _depth_ dimension from the notation). Lastly, we have for each layer in the convolutional stack a _stride_ hyperparameter denoted by $s$.\n",
        "\n",
        "Not shown in Fig. 2 is the use of the [tanh](https://paperswithcode.com/method/tanh-activation) non-linear activation function throughout the network. While this proved to be a better performer than the sigmoid function for a multi-layer neural network, _tanh_ did not appropriately handle the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), architectures today utilise other activation functions such as [ReLU](https://en.wikipedia.org/wiki/ReLU) or [batch normalisation](https://en.wikipedia.org/wiki/Batch_normalization) to solve this. The last layer of the network utilises a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function to fit a Gaussian distribution to the predicted class probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bedd6f",
      "metadata": {
        "id": "f0bedd6f"
      },
      "source": [
        "#### Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04997dcd",
      "metadata": {
        "id": "04997dcd"
      },
      "source": [
        "##### Input layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10c814c",
      "metadata": {
        "id": "f10c814c"
      },
      "source": [
        "The input to the LeNet-5 network is a set of $\\left(32 \\ x \\ 32 \\ x \\ 1\\right)$ grayscale images, each consisting of a single handwritten digit (read more on the modified MNIST SD-1/SD-3 dataset [here](http://yann.lecun.com/exdb/mnist/)). The pixel values of the input images are normalised (anti-aliasing) so that the background (white) has a pixel value of $-0.1$ and the foreground (black) has a value $1.175$. This choice for normalisation is made such that the mean value of the input is roughly $0$ and the variance roughly $1$ to accelerate learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72433e84",
      "metadata": {
        "id": "72433e84"
      },
      "source": [
        "##### Convolutional layer (C1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e402f1f5",
      "metadata": {
        "id": "e402f1f5"
      },
      "source": [
        "* **Input size**: $32 x 32 x 1$\n",
        "* **Feature maps**: $6$\n",
        "* **Kernel size**: $5x5x1$\n",
        "* **Stride**: $1$\n",
        "* **Padding**: $0$\n",
        "* **Output size**: $(28x28x1)$; $(32 - 5 + 2*0) \\ / \\ 1 + 1 = 28$\n",
        "* **Neurons**: $6 * (28x28x1) = 4704$\n",
        "* **Trainable parameters**: $6 * (5x5x1 + 1) = 156$\n",
        "* **Total connections**: $6 * (5x5x1 + 1) * (28x28) = 122304$\n",
        "* **Activation function**: Tanh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79655646",
      "metadata": {
        "id": "79655646"
      },
      "source": [
        "This convolutional layer has $6$ feature maps. Each unit of each feature map connects to a $5 x 5$ region within the input image. The feature map size of $28 x 28$ has been selected so that the strided convolutions do not result in any dropped pixels (for more information on that, see [Exercise 1.4.1](https://github.com/jonathanloganmoran/ND0013-Self-Driving-Car-Engineer/blob/main/1-Object-Detection-in-Urban-Environments/Exercises/1-4-1-Pooling-Layers-CNNs/2022-09-07-Pooling-Layers-Convolutional-Neural-Networks.ipynb)). each kernel has an additional bias term (hence the \"+1\" in the above calculation of trainable parameters. While the number of connections seems tremendous, due to _weight sharing_ we only need to learn $156$ parameters for this layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e5849b2",
      "metadata": {
        "id": "8e5849b2"
      },
      "source": [
        "##### Average pooling layer (S2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5c3bfd",
      "metadata": {
        "id": "ba5c3bfd"
      },
      "source": [
        "* **Input size**: $28 x 28 x 1$\n",
        "* **Feature maps**: $6$\n",
        "* **Filter size**: $2x2x1$\n",
        "* **Stride**: $2$\n",
        "* **Padding**: $0$\n",
        "* **Output size**: $(14x14x1)$; $(28 - 2 + 2*0) \\ / \\ 2 + 1 = 14$\n",
        "* **Neurons**: $6 * (14x14x1) = 1176$\n",
        "* **Trainable parameters**: $6 * (1 + 1) = 12$\n",
        "* **Total connections**: $6 * (2x2x1 + 1) * (14x14x1) = 5880$\n",
        "* **Activation function**: Tanh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b079f610",
      "metadata": {
        "id": "b079f610"
      },
      "source": [
        "This sub-sampling layer uses the local average pooling calculations to reduce the input size in half from $28x28$ to $14x14$. Each window of values is summed, multiplied by a weight coefficient $w$ and then added together with a trainable bias offset term $b$ (hence the number of trainable parameters shown above). These parameters are learned with [back-propagation](https://en.wikipedia.org/wiki/Backpropagation). The intermediate output values are then passed into a sigmoid activation function which is used to enforce non-linearity. The coefficient and bias terms regulate the intensity of the sigmoid function on the output. If the coefficient values are small, then the units operate in a quasi-linear mode such that the sub-sampling layer merely blurs the input. If the coefficient is large, sub-sampling performs similarly to a \"noisy OR\", \"noisy AND\" function depending on the value of the bias (LeCun 1998).\n",
        "\n",
        "The affect of downsampling of the input is a reduction in sensitivity of the output to shifts and distortions. By progressively reducing the spatial resolution with sub-sampling, we compensate the effect by progressively increasing the number of feature maps (the richness of the representation)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e40afdc",
      "metadata": {
        "id": "8e40afdc"
      },
      "source": [
        "##### Convolutional layer (C3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a1a3e9",
      "metadata": {
        "id": "68a1a3e9"
      },
      "source": [
        "* **Input size**: $14 x 14 x 1$\n",
        "* **Feature maps**: $16$\n",
        "* **Kernel size**: $5 x 5 x 1$\n",
        "* **Stride**: $1$\n",
        "* **Padding**: $0$\n",
        "* **Output size**: $(10x10x1)$; $(14 - 5 + 2*0) \\ / \\ 1 + 1 = 10$\n",
        "* **Neurons**: $16 * (10x10x1) = 1600$\n",
        "* **Trainable parameters**: $6 * \\left[(3x5x5 + 1) + (4x5x5 + 1)\\right] + 3*\\left[(4x5x5 + 1)\\right] + 1*\\left[(6x5x5 + 1)\\right] = 1516$\n",
        "* **Total connections**: $1516*(10x10x1) = 151600$\n",
        "* **Activation function**: Tanh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3bc0166",
      "metadata": {
        "id": "d3bc0166"
      },
      "source": [
        "In this second convolutional layer, we increase the number of feature maps to $16$. In conjunction with the progressive decrease in spatial dimensions of the input images, we now _increase_ the richness of the representation of the input by increasing the number of feature maps. With the increase in feature maps brings a non-complete connection scheme in order to keep the number of connections of $S2$ and $C3$ within resonable bounds."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b52b855a",
      "metadata": {
        "id": "b52b855a"
      },
      "source": [
        "![Fig. 3. The non-complete connection scheme for layers S2 and C3 (reproduced from LeCun et al., 1998)](figures/2022-09-12-Figure-3-Table-1-LeCun-1998.png)\n",
        "\n",
        "$$\n",
        "\\textrm{Fig. 3. The non-complete connection scheme for layers} \\ S2  \\ \\textrm{and} \\  C3  \\ \\textrm{(reproduced from LeCun et al., 1998)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91eb2e87",
      "metadata": {
        "id": "91eb2e87"
      },
      "source": [
        "From the _total connections_ calculation above we see that the first _six_ feature maps of $C3$ take inputs from every contiguous subset of _three_ feature maps of $S2$. The next _six_ feature maps of $C3$ take inputs from every contiguous subset of _four_ feature maps of $S2$. The following _three_ feature maps of $C3$ take inputs from _discontinuous_ subsets of _four_ feature maps of $S2$. The last feature map ($1$) takes _all six_ inputs of the feature maps of $S2$.\n",
        "\n",
        "With that, we calculate the total number of trainable parameters of $C3$ equal to $1516$ and the number of total connections equal to $151600$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a150043",
      "metadata": {
        "id": "3a150043"
      },
      "source": [
        "##### Average Pooling layer (S4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9563c472",
      "metadata": {
        "id": "9563c472"
      },
      "source": [
        "* **Input size**: $10 x 10 x 1$\n",
        "* **Feature maps**: $16$\n",
        "* **Filter size**: $2x2x1$\n",
        "* **Stride**: $2$\n",
        "* **Padding**: $0$\n",
        "* **Output size**: $(5x5x1)$; $(10 - 2 + 2*0) \\ / \\ 2 + 1 = 5$\n",
        "* **Neurons**: $16 * (5x5x1) = 400$\n",
        "* **Trainable parameters**: $16 * (1 + 1) = 32$\n",
        "* **Total connections**: $16 * (2x2x1 + 1) * (5x5x1) = 2000$\n",
        "* **Activation function**: Tanh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7c1fc9c",
      "metadata": {
        "id": "c7c1fc9c"
      },
      "source": [
        "The down-sampling layer $S4$ reduces its input by a factor of 2 from $10x10x1$ down to $5x5x1$. The $16$ feature maps of size $10x10x1$ from $C3$ are pooled into $2x2x1$ units to obtain $16$ feature maps of size $5x5x1$.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f28f69a",
      "metadata": {
        "id": "1f28f69a"
      },
      "source": [
        "##### Convolutional layer (C5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061e316d",
      "metadata": {
        "id": "061e316d"
      },
      "source": [
        "* **Input size**: $5 x 5 x 1$\n",
        "* **Feature maps**: $120$\n",
        "* **Kernel size**: $5 x 5 x 1$\n",
        "* **Stride**: $1$\n",
        "* **Padding**: $0$\n",
        "* **Output size**: $(1x1x1)$; $(5 - 5 + 2*0) \\ / \\ 1 + 1 = 1$\n",
        "* **Neurons**: $120 * (1x1x1) = 120$\n",
        "* **Trainable parameters**: $120 * (16x5x5x1 + 1) = 48120$\n",
        "* **Total connections**: $120 * (16x5x5x1) = 48000$\n",
        "* **Activation function**: Tanh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50148929",
      "metadata": {
        "id": "50148929"
      },
      "source": [
        "Here the final convolutional layer $C5$ is considered to be fully-connected with respect to the input dimensionality of the digits images. For any other configuration (i.e., an increase in the number of channels, as with RGB images), we can expect this to be explicitly a convolutional layer. For the digits dataset, given a kernel size of $5x5x1$ and a corresponding output of layer $S4$ of size $5x5x1$, we obtain a fully-connected layer with $(1x1x1)$ feature map output dimensions. Therefore, we have $48120$ trainable parameters and $48000$ total connections in this last convolutional layer.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c574fd",
      "metadata": {
        "id": "e7c574fd"
      },
      "source": [
        "##### Fully-connected layer (F6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0818a69e",
      "metadata": {
        "id": "0818a69e"
      },
      "source": [
        "* **Input size**: $1 x 1 x 1$\n",
        "* **Feature maps**: N/A\n",
        "* **Kernel size**: N/A\n",
        "* **Stride**: N/A\n",
        "* **Padding**: N/A\n",
        "* **Output size**: $(1x1)$\n",
        "* **Neurons**: $84 * (1x1x1) = 84$\n",
        "* **Trainable parameters**: $84*(120x1x1 + 1) = 10164$\n",
        "* **Total connections**: $120 * (16x5x5x1) = 48000$\n",
        "* **Activation function**: Tanh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93aa4ef",
      "metadata": {
        "id": "e93aa4ef"
      },
      "source": [
        "The second-to-last layer in the LeNet-5 network is fully-connected to the convolutional layer $C5$; we can consider $F6$ to be a true fully-connected layer with $84$ units. Here the number of units for this layer was selected according to a $7x12$ bitmap representation corresponding to one of the $10$ character classes. This is considering that the representation used is not particularly useful for _isolated_ digits but rather _strings_ of characters selected from the printable ASCII set which tends to produce _confusable_ i.e., different characters with similar appearances (e.g., an upper/lower-case \"O\" and a zero, or an uppercase \"I\" and the digit 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80d6b0b7",
      "metadata": {
        "id": "80d6b0b7"
      },
      "source": [
        "##### Output layer (OUTPUT or F7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b365343b",
      "metadata": {
        "id": "b365343b"
      },
      "source": [
        "* **Classes**: 10\n",
        "* **Output size**: $(10x1)$\n",
        "* **Neurons**: $84 * (1x1x1) = 84$\n",
        "* **Trainable parameters**: $84*(10x1x1 + 0) = 840$\n",
        "* **Total connections**: $10 * (84x1x1x1) = 840$\n",
        "* **Activation function**: RBF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c46e5d",
      "metadata": {
        "id": "43c46e5d"
      },
      "source": [
        "The final layer in the LeNet-5 is of course the classification output layer — a fully-connected layer composed of Euclidean Radial Basis Function units (RBF). There are 10 RBF units, one for each digit class, each with a set of 84 inputs. The output of each RBF unit is computed with a Euclidean distance summation between its input vector and its parameter vector. The Euclidean distance (L2 loss) serves as a penalty for the input and RBF digit class; the larger the RBF output, the farther away the input is from the parameter vector. The choice of L2 loss here greatly penalises outliers (distance \"away\" from parameter vectors). The RBF output can also be interpreted as the un-normalised negative log-likelihood of the Gaussian distribution of $F6$.\n",
        "\n",
        "In short, the parameter vectors of the RBF units act as target vectors for the $F6$ layer. The components of the parameter vectors are valued $+1$ or $-1$, which are points along the maximum curvature of the scaled _hyperbolic tangent_, i.e., [tanh](https://en.wikipedia.org/wiki/Hyperbolic_functions#Hyperbolic_tangent), or in other words — the re-scaled and shifted sigmoid activation function. Therefore, the RBF activation function is chosen for this output layer to enforce the maximally non-linear range of the tanh function applied to units in $F6$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "384c8474",
      "metadata": {
        "id": "384c8474"
      },
      "source": [
        "#### Reflecting on the LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc0f3ad",
      "metadata": {
        "id": "cdc0f3ad"
      },
      "source": [
        "##### Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f458176",
      "metadata": {
        "id": "6f458176"
      },
      "source": [
        "The tanh and sigmoid activation functions used in LeNet-5 have their downsides. As pointed out in the original paper by LeCun et al., 1998, the scaled tanh function presents issues with saturation at the maximum and minimum boundary points. This leads to [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) as the derivatives of the function approach zero. Since we have a handful of hidden layers, the multiplication of these near-zero gradient values results in an exponential decrease in the gradient as it is propagated from the final to the initial layer.\n",
        "\n",
        "ReLu or Leaky ReLu are more often used nowadays to address this. ReLU imposes a lower-bound for gradient values at $0$. This handles the vanishing gradient problem by eliminating saturation at very small gradient values — their values simply become zero, ultimately encourages sparsity in the learned representations [3]. Yet in some cases, ReLU may end up always returning the lower-bound ($0$) for any input. The [dying ReLU problem](https://www.educative.io/answers/what-is-the-dying-relu-problem) is the result of zero-valued gradient propagations and a high learning rate or a large negative bias could be at fault. To mitigate this, the [Leaky ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU) is used which does not produce zero-valued gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb3b264",
      "metadata": {
        "id": "eeb3b264"
      },
      "source": [
        "## 2. Programming Task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a92cf7b",
      "metadata": {
        "id": "8a92cf7b"
      },
      "source": [
        "Using the Keras API, you have to create a small convolutional neural networks using less than 15 layers, containing at least one convolutional layer, one pooling layer and one dense (fully connected layer). You can find a list of the different layers available [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "83d6a81d",
      "metadata": {
        "id": "83d6a81d"
      },
      "outputs": [],
      "source": [
        "model_params = defaultdict(dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "069cea3b",
      "metadata": {
        "id": "069cea3b"
      },
      "source": [
        "### 2.1. LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a69ba1b",
      "metadata": {
        "id": "7a69ba1b"
      },
      "source": [
        "#### Defining the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4a64f8ef",
      "metadata": {
        "id": "4a64f8ef"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `training.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a7e59f0b",
      "metadata": {
        "id": "a7e59f0b"
      },
      "outputs": [],
      "source": [
        "def create_lenet_network(input_shape: Tuple[int], \n",
        "                         output_shape: int) -> tf.keras.Sequential:\n",
        "    \"\"\"Creates the LeNet-5 network (LeCun et al., 1998).\n",
        "    \n",
        "    :param input_shape: tuple, the n-dimensional shape of the input images.\n",
        "    :param: output_shape: tuple, the 1D shape corresponding to number of\n",
        "        distinct classes to predict.\n",
        "    :returns: the LeNet-5 Model object.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT THIS FUNCTION\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "        # The C1 (convolutional) layer\n",
        "        tf.keras.layers.Conv2D(\n",
        "                input_shape=input_shape,    # Shape of the input images\n",
        "                filters=6, \n",
        "                kernel_size=(5, 5), \n",
        "                strides=(1, 1), \n",
        "                padding='valid',            # No padding\n",
        "                activation='tanh'\n",
        "        ),\n",
        "        # The S2 (sub-sampling) layer\n",
        "        tf.keras.layers.AveragePooling2D(\n",
        "                pool_size=(2, 2),\n",
        "                strides=(2, 2),\n",
        "                padding='valid'\n",
        "        ),\n",
        "        tf.keras.layers.Activation(\n",
        "                activation=tf.keras.activations.sigmoid\n",
        "        ),\n",
        "        # The C3 (convolutional) layer\n",
        "        tf.keras.layers.Conv2D(\n",
        "                filters=16,\n",
        "                kernel_size=(5, 5),\n",
        "                strides=(1, 1),\n",
        "                padding='valid',\n",
        "                activation='tanh'\n",
        "        ),\n",
        "        # The S4 (sub-sampling) layer\n",
        "        tf.keras.layers.AveragePooling2D(\n",
        "                pool_size=(2, 2),\n",
        "                strides=(2, 2),\n",
        "                padding='valid'\n",
        "        ),\n",
        "        tf.keras.layers.Activation(\n",
        "                activation=tf.keras.activations.sigmoid\n",
        "        ),\n",
        "        # The C5 (convolutional) layer\n",
        "        tf.keras.layers.Conv2D(\n",
        "                filters=16,\n",
        "                kernel_size=(5, 5),\n",
        "                strides=(1, 1),\n",
        "                padding='valid',\n",
        "                activation='tanh'\n",
        "        \n",
        "        ),\n",
        "        # The Flatten layer\n",
        "        tf.keras.layers.Flatten(),\n",
        "        # The F6 (fully-connected) layer\n",
        "        tf.keras.layers.Dense(\n",
        "                units=84,\n",
        "                activation='tanh'\n",
        "        ),\n",
        "        # The F7 (Output) layer\n",
        "        tf.keras.layers.Dense(\n",
        "                units=output_shape,\n",
        "                activation='softmax'\n",
        "        )\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaca5b56",
      "metadata": {
        "id": "eaca5b56"
      },
      "source": [
        "#### Defining the model build parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "efd8e50f",
      "metadata": {
        "id": "efd8e50f"
      },
      "outputs": [],
      "source": [
        "### Defining our model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e258a733",
      "metadata": {
        "id": "e258a733"
      },
      "outputs": [],
      "source": [
        "model_name = 'LeNet-5'\n",
        "optimizer = 'adam'\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "metrics = ['accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e7fd7ad4",
      "metadata": {
        "id": "e7fd7ad4"
      },
      "outputs": [],
      "source": [
        "model_params[model_name].update({'optimizer': optimizer})\n",
        "model_params[model_name].update({'loss_fn': loss})\n",
        "model_params[model_name].update({'metrics': metrics})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87214c8e",
      "metadata": {
        "id": "87214c8e"
      },
      "source": [
        "### 2.2. Custom CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb05f10",
      "metadata": {
        "id": "fdb05f10"
      },
      "source": [
        "You should experiment with different designs (number of layers, types of pooling, filter sizes, number of fully connected layers, number of neurons)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "81a3f069",
      "metadata": {
        "id": "81a3f069"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `training.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c1b343a5",
      "metadata": {
        "id": "c1b343a5"
      },
      "outputs": [],
      "source": [
        "def create_custom_network():\n",
        "    #net = tf.keras.Sequential()\n",
        "    # IMPLEMENT THIS FUNCTION\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031da250",
      "metadata": {
        "id": "031da250"
      },
      "source": [
        "### 2.3. Training and validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b079a6d0",
      "metadata": {
        "id": "b079a6d0"
      },
      "source": [
        "The following helper functions are used for the training and validation loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "303a7673",
      "metadata": {
        "id": "303a7673"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `utils.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "60ed051a",
      "metadata": {
        "id": "60ed051a"
      },
      "outputs": [],
      "source": [
        "def get_module_logger(mod_name: str) -> logging.Logger:\n",
        "    \"\"\"Initialises a console logger instance.\n",
        "    \n",
        "    :param mod_name: the model name to assign to the logger.\n",
        "    :returns: a logger instance.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### Setting up the console logger and formatter\n",
        "    logger = logging.getLogger(mod_name)\n",
        "    handler = logging.StreamHandler()\n",
        "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    ### Prevent messages going to root handler\n",
        "    logger.propagate = False\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ac325f83",
      "metadata": {
        "id": "ac325f83"
      },
      "outputs": [],
      "source": [
        "def get_datasets(model_params: dict) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
        "    \"\"\"Return the training and validation datasets.\n",
        "    \n",
        "    :param model_params: the dict containing the dataset- and \n",
        "        model-specific arguments.\n",
        "    :returns: (train_dataset, validation_dataset), tuple of tf.data.Dataset \n",
        "        instances.\n",
        "    \"\"\"\n",
        "    \n",
        "    model_params = dict(model_params)\n",
        "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "                        directory=model_params.get('imdir', None),\n",
        "                        labels='inferred',\n",
        "                        label_mode='int',\n",
        "                        color_mode=model_params.get('color_mode', None),\n",
        "                        batch_size=model_params.get('batch_size', None),\n",
        "                        image_size=model_params.get('image_size', None),\n",
        "                        shuffle=True,\n",
        "                        seed=123,\n",
        "                        validation_split=0.1,\n",
        "                        subset='training',\n",
        "    )\n",
        "    validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "                        directory=model_params.get('imdir', None),\n",
        "                        labels='inferred',\n",
        "                        label_mode='int',\n",
        "                        color_mode=model_params.get('color_mode', None),\n",
        "                        batch_size=model_params.get('batch_size', None),\n",
        "                        image_size=model_params.get('image_size', None),\n",
        "                        shuffle=True,\n",
        "                        seed=123,\n",
        "                        validation_split=0.1,\n",
        "                        subset='validation',\n",
        "    )\n",
        "    return train_dataset, validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "32c84a6b",
      "metadata": {
        "id": "32c84a6b"
      },
      "outputs": [],
      "source": [
        "def display_metrics(history: tf.keras.callbacks.History):\n",
        "    \"\"\"Plots the per-epoch loss and accuracy metrics.\n",
        "    \n",
        "    :param history: the Keras `callbacks.History` object.\n",
        "    \"\"\"\n",
        "    \n",
        "    f, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax[0].plot(history.history['loss'], linewidth=3)\n",
        "    ax[0].plot(history.history['val_loss'], linewidth=3)\n",
        "    ax[0].set_title('Loss', fontsize=16)\n",
        "    ax[0].set_ylabel('Loss', fontsize=16)\n",
        "    ax[0].set_xlabel('Epoch', fontsize=16)\n",
        "    ax[0].legend(['train loss', 'val loss'], loc='upper right')\n",
        "    ax[1].plot(history.history['accuracy'], linewidth=3)\n",
        "    ax[1].plot(history.history['val_accuracy'], linewidth=3)\n",
        "    ax[1].set_title('Accuracy', fontsize=16)\n",
        "    ax[1].set_ylabel('Accuracy', fontsize=16)\n",
        "    ax[1].set_xlabel('Epoch', fontsize=16)\n",
        "    ax[1].legend(['train acc', 'val acc'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c2ecf6",
      "metadata": {
        "id": "25c2ecf6"
      },
      "source": [
        "### 2.3. Evaluation on the modified MNIST digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ad93743d",
      "metadata": {
        "id": "ad93743d"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'MNIST'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetching the modified MNIST data"
      ],
      "metadata": {
        "id": "5XcI-_cwdY0N"
      },
      "id": "5XcI-_cwdY0N"
    },
    {
      "cell_type": "code",
      "source": [
        "### Specifying the validation/test split percentages\n",
        "VAL_PERCENT = 25\n",
        "TEST_PERCENT = 75"
      ],
      "metadata": {
        "id": "T4dMdK6zIeeQ"
      },
      "id": "T4dMdK6zIeeQ",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Specifying the desired batch size\n",
        "BATCH_SIZE = 128\n",
        "model_params[DATASET_NAME].update({'batch_size': BATCH_SIZE})"
      ],
      "metadata": {
        "id": "4i4wlqwgtYHr"
      },
      "id": "4i4wlqwgtYHr",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Fetching the MNIST digits dataset (hosted by TensorFlow)\n",
        "# See: https://www.tensorflow.org/datasets/catalog/mnist"
      ],
      "metadata": {
        "id": "eomSJ38DZDuq"
      },
      "id": "eomSJ38DZDuq",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digits_data, digits_info = tfds.load(DATASET_NAME,\n",
        "                                     batch_size=BATCH_SIZE,\n",
        "                                     split=['train', \n",
        "                                            'test[{}%:]'.format(VAL_PERCENT),\n",
        "                                            'test[:{}%]'.format(TEST_PERCENT)\n",
        "                                     ],\n",
        "                                     shuffle_files=True, \n",
        "                                     with_info=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "a93fd23698b34ba98740725e2fb803df",
            "d095fd1cda654834a6dfff126d92e554",
            "66313e2ad05c425599ec481b132048b2",
            "4108dc53d0194af2a8fa4f670d17662c",
            "9d41cf71884b4205aab8afef59ccd4f7",
            "d2279450772b47d9bc7e3cc5ea2ebbb5",
            "f3e3ea05dfad4578bb8f45c33fccca20",
            "fac162b490074a0d8095207479d7f119",
            "d9ddc266a7124f3e9957c3e4f0e8848c",
            "42b1bf702eea4f2aab12d18800bed515",
            "088c7b2aa6484285af2d5324a7264172"
          ]
        },
        "id": "NHuM93bLZX1G",
        "outputId": "2563a877-92e1-4ad6-c44f-d4213b3a7950"
      },
      "id": "NHuM93bLZX1G",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to ~/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a93fd23698b34ba98740725e2fb803df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset mnist downloaded and prepared to ~/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205af274",
      "metadata": {
        "id": "205af274"
      },
      "source": [
        "#### Considerations for our input data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Viewing the dataset metadata attributes\n",
        "digits_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL8FVN_Ga7YN",
        "outputId": "bd3caddb-a1ab-4408-e982-f4720bd394cb"
      },
      "id": "iL8FVN_Ga7YN",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='mnist',\n",
              "    full_name='mnist/3.0.1',\n",
              "    description=\"\"\"\n",
              "    The MNIST database of handwritten digits.\n",
              "    \"\"\",\n",
              "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
              "    data_path='~/tensorflow_datasets/mnist/3.0.1',\n",
              "    file_format=tfrecord,\n",
              "    download_size=11.06 MiB,\n",
              "    dataset_size=21.00 MiB,\n",
              "    features=FeaturesDict({\n",
              "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
              "    }),\n",
              "    supervised_keys=('image', 'label'),\n",
              "    disable_shuffling=False,\n",
              "    splits={\n",
              "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
              "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
              "    },\n",
              "    citation=\"\"\"@article{lecun2010mnist,\n",
              "      title={MNIST handwritten digit database},\n",
              "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
              "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
              "      volume={2},\n",
              "      year={2010}\n",
              "    }\"\"\",\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d3b5c0cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3b5c0cb",
        "outputId": "76859840-6ec2-4311-d6c5-39f59323749f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nr. classes in MNIST: 10\n"
          ]
        }
      ],
      "source": [
        "# MNIST has 10 distinct digit classes\n",
        "N_CLASSES = digits_info.features['label'].num_classes\n",
        "print('Nr. classes in MNIST:', N_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e2128b77",
      "metadata": {
        "id": "e2128b77"
      },
      "outputs": [],
      "source": [
        "### Fetching our input image specs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d3ecf0c8",
      "metadata": {
        "id": "d3ecf0c8"
      },
      "outputs": [],
      "source": [
        "IMG_COLOR_MODE = 'grayscale'\n",
        "IMG_SHAPE = digits_info.features['image'].shape\n",
        "IMG_SIZE = IMG_SHAPE[:2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('MNIST:', digits_info.features['image'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AK2tWzOaMSV",
        "outputId": "4887f171-4470-4e0a-a683-cbc10b455a74"
      },
      "id": "3AK2tWzOaMSV",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST: Image(shape=(28, 28, 1), dtype=tf.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: the LeNet-5 network expects $32x32$ px grayscale images as input. We will fix this by padding our images to the expected size."
      ],
      "metadata": {
        "id": "gOYJHL-3aKTE"
      },
      "id": "gOYJHL-3aKTE"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3bcc3e2a",
      "metadata": {
        "id": "3bcc3e2a"
      },
      "outputs": [],
      "source": [
        "model_params[DATASET_NAME].update({'image_size': IMG_SIZE})\n",
        "model_params[DATASET_NAME].update({'color_mode': IMG_COLOR_MODE})\n",
        "model_params[DATASET_NAME].update({'input_shape': IMG_SHAPE})\n",
        "model_params[DATASET_NAME].update({'n_classes': N_CLASSES})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34eca184",
      "metadata": {
        "id": "34eca184"
      },
      "source": [
        "#### Processing the image data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Number of features (pixel values) in a single image\n",
        "len(iter(digits_data[0]).get_next()['image'][0].numpy().flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd62E5aRd0Jp",
        "outputId": "461adf87-aef9-4962-f746-d1d2f9dfe549"
      },
      "id": "Cd62E5aRd0Jp",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Defining the preprocessing functions"
      ],
      "metadata": {
        "id": "AW7Nlzx9YULn"
      },
      "id": "AW7Nlzx9YULn"
    },
    {
      "cell_type": "code",
      "source": [
        "### From J. Moran's `2022-09-07-Pooling-Layers-Convolutional-Neural-Networks.ipynb`"
      ],
      "metadata": {
        "id": "aJS9K8tOvHZ3"
      },
      "id": "aJS9K8tOvHZ3",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paddings(input_tensor: tf.Tensor, output_shape: Tuple[int, int, int], \n",
        "                 pool_size: int, pool_stride: int) -> List[List[int]]:\n",
        "    \"\"\"Returns the range of values to pad on each edge using TensorFlow.\n",
        "    \n",
        "    Computes the padding amounts along each dimension such that the\n",
        "    spatial dimensionality of the input is preserved (akin to the\n",
        "    TensorFlow Keras `padding=\"same\"` argument). Note that the input\n",
        "    is a 4-dimensional array but we computing only the padding amounts \n",
        "    for the 2D image surface (the width and height axes).\n",
        "\n",
        "    For more information on the TensorFlow `pad()` function, see:\n",
        "    https://www.tensorflow.org/api_docs/python/tf/pad\n",
        "    \n",
        "    :param array: tf.Tensor, the input data of dimensions [NxWxHxC], \n",
        "        i.e., [n_samples x width x height x n_channels].\n",
        "    :param pool_size: int, the symmetric kernel window dimension.\n",
        "    :param pool_stride: int, the amount to displace the kernel window\n",
        "        in the horizontal and vertical direction (symmetric).\n",
        "    :returns: paddings, the TensorFlow `paddings` padding sequences.\n",
        "    \"\"\"\n",
        "    \n",
        "    wpad = None\n",
        "    hpad = None\n",
        "    # Get the per-sample input dimensions\n",
        "    _, w, h, _ = tf.shape(input_tensor).numpy()\n",
        "    # Set the desired output shape\n",
        "    _, w_out, h_out, _ = output_shape\n",
        "    # Compute the padding dimensions\n",
        "    wpad = (pool_stride * w_out - w + pool_size - pool_stride) // 2\n",
        "    hpad = (pool_stride * h_out - h + pool_size - pool_stride) // 2\n",
        "    # Return the range of values to pad along each dimension\n",
        "    # [n_samples, width, height, n_channels]\n",
        "    return [[0, 0], [0, wpad], [0, hpad], [0, 0]]"
      ],
      "metadata": {
        "id": "IrcN4QPcvGgb"
      },
      "id": "IrcN4QPcvGgb",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### From J. Moran's `2022-09-05-Image-Classification-Feed-Forward-Neural-Networks.ipynb`"
      ],
      "metadata": {
        "id": "PkngCaIlYJ_O"
      },
      "id": "PkngCaIlYJ_O",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(dataset: tf.data.Dataset) -> Generator[tuple, None, None]:\n",
        "    \"\"\"Processes the input dataset and returns a generator object.\n",
        "\n",
        "    :param dataset: the tf.data.Dataset instance of batched image, label data.\n",
        "    :returns: the generator instance modifying the image attribute values.\n",
        "    \"\"\"\n",
        "\n",
        "    def process_generator() -> Iterator[Tuple[tf.Tensor, tf.Tensor]]:\n",
        "        \"\"\"Zero-pads and scales the input images.\n",
        "        \n",
        "        All images in each batch are padded according to the pre-computed\n",
        "        pad widths. All pixel values are scaled in the range [0, 1].\n",
        "\n",
        "        Note that the input dataset is expected to contain images with a \n",
        "        consistent (equal) shape, e.g., (28, 28, 1). This function also \n",
        "        assumes a zero-padding configuration. If your dataset contains images\n",
        "        with inconsistent dimensions, and you want to use zero-padding,\n",
        "        an easier technique is a `tf.bitwise_or` with a zero-valued mask of\n",
        "        dimensions equal to your desired output dataset.\n",
        "\n",
        "        :returns: an iterator yielding the modfied images and labels.\n",
        "        \"\"\"\n",
        "\n",
        "        for batch in dataset:\n",
        "            # Padding the input image to 32x32 resolution\n",
        "            images = tf.pad(batch['image'],\n",
        "                            paddings=PADDINGS,\n",
        "                            mode='CONSTANT',\n",
        "                            constant_values=tf.constant(\n",
        "                                value=PADDING_VALUE,\n",
        "                                dtype=digits_info.features['image'].dtype\n",
        "                            )\n",
        "            )\n",
        "            # Scaling the image data in range [0, 1]\n",
        "            images = tf.cast(images / 255, \n",
        "                             dtype=digits_info.features['image'].dtype\n",
        "            )\n",
        "            yield images, batch['label']\n",
        "    return process_generator"
      ],
      "metadata": {
        "id": "pHUHjhxeYJbn"
      },
      "id": "pHUHjhxeYJbn",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Preprocessing the dataset"
      ],
      "metadata": {
        "id": "-m6Gxk2TYg04"
      },
      "id": "-m6Gxk2TYg04"
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting the desired shape after padding\n",
        "IMG_SHAPE_PADDED = (None, 32, 32, 1)"
      ],
      "metadata": {
        "id": "n8c94BKTUQby"
      },
      "id": "n8c94BKTUQby",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Computing the padding amounts for each axis\n",
        "# Assuming our dataset image sizes are consistent\n",
        "# Here we use the first LeNet-5 layer (C1) parameters"
      ],
      "metadata": {
        "id": "9HVnc8fozt4x"
      },
      "id": "9HVnc8fozt4x",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PADDINGS = get_paddings(input_tensor=iter(digits_data[0]).get_next()['image'],\n",
        "                        output_shape=IMG_SHAPE_PADDED,\n",
        "                        pool_size=5,\n",
        "                        pool_stride=1\n",
        ")\n",
        "print('Paddings:', PADDINGS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03WeNLxOXzAB",
        "outputId": "90f35416-e1ba-4490-8839-cd0f57218ad1"
      },
      "id": "03WeNLxOXzAB",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paddings: [[0, 0], [0, 4], [0, 4], [0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting the scalar value to pad the image arrays with\n",
        "PADDING_VALUE = 0"
      ],
      "metadata": {
        "id": "4J46xGpHOryg"
      },
      "id": "4J46xGpHOryg",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Updating the model parameters"
      ],
      "metadata": {
        "id": "CRGj1HMQX_aT"
      },
      "id": "CRGj1HMQX_aT",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params[DATASET_NAME].update({'image_size_padded': IMG_SHAPE_PADDED[:2]})\n",
        "model_params[DATASET_NAME].update({'input_shape_padded': IMG_SHAPE_PADDED})\n",
        "model_params[DATASET_NAME].update({'paddings': PADDINGS})\n",
        "model_params[DATASET_NAME].update({'padding_value': PADDING_VALUE})"
      ],
      "metadata": {
        "id": "uAkq7E7LVDIh"
      },
      "id": "uAkq7E7LVDIh",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "645f2170",
      "metadata": {
        "id": "645f2170"
      },
      "outputs": [],
      "source": [
        "### Unbatching the datasets and preprocessing the images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = digits_data"
      ],
      "metadata": {
        "id": "Na9OuD0McxtE"
      },
      "id": "Na9OuD0McxtE",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_processed = tf.data.Dataset.from_generator(\n",
        "        generator=process(train_data), \n",
        "        output_signature=(\n",
        "            tf.TensorSpec(\n",
        "                shape=model_params[DATASET_NAME].get('input_shape_padded', None),\n",
        "                dtype=tf.dtypes.float32\n",
        "            ), \n",
        "            tf.TensorSpec(\n",
        "                shape=(None, *digits_info.features['label'].shape), \n",
        "                dtype=digits_info.features['label'].dtype\n",
        "            )\n",
        "        )\n",
        ")"
      ],
      "metadata": {
        "id": "65RQCLVHNrkW"
      },
      "id": "65RQCLVHNrkW",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "a3892eb9",
      "metadata": {
        "id": "a3892eb9"
      },
      "outputs": [],
      "source": [
        "validation_dataset_processed = tf.data.Dataset.from_generator(\n",
        "        generator=process(val_data), \n",
        "        output_signature=(\n",
        "            tf.TensorSpec(\n",
        "                shape=model_params[DATASET_NAME].get('input_shape_padded', None),\n",
        "                dtype=tf.dtypes.float32\n",
        "            ), \n",
        "            tf.TensorSpec(\n",
        "                shape=(None, *digits_info.features['label'].shape), \n",
        "                dtype=digits_info.features['label'].dtype\n",
        "            )\n",
        "        )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_processed = tf.data.Dataset.from_generator(\n",
        "        generator=process(test_data), \n",
        "        output_signature=(\n",
        "            tf.TensorSpec(\n",
        "                shape=model_params[DATASET_NAME].get('input_shape_padded', None),\n",
        "                dtype=tf.dtypes.float32\n",
        "            ), \n",
        "            tf.TensorSpec(\n",
        "                shape=(None, *digits_info.features['label'].shape), \n",
        "                dtype=digits_info.features['label'].dtype\n",
        "            )\n",
        "        )\n",
        ")"
      ],
      "metadata": {
        "id": "p5tKPe_9dAIx"
      },
      "id": "p5tKPe_9dAIx",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Previewing the first scaled and padded image in test set"
      ],
      "metadata": {
        "id": "oRDQLg1xP3hS"
      },
      "id": "oRDQLg1xP3hS",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter(test_dataset_processed).get_next()[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZT3X-CrO2tj",
        "outputId": "556262c4-18b2-4243-d2a9-0396677a3c06"
      },
      "id": "GZT3X-CrO2tj",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 32, 1), dtype=float32, numpy=\n",
              "array([[[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        ...,\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        ...,\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        ...,\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        ...,\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        ...,\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        ...,\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41fe458d",
      "metadata": {
        "id": "41fe458d"
      },
      "source": [
        "#### Evaluating the LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94982cb",
      "metadata": {
        "id": "a94982cb"
      },
      "source": [
        "##### Initialising the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "a4253c23",
      "metadata": {
        "id": "a4253c23"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'LetNet-5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b316e2a2",
      "metadata": {
        "id": "b316e2a2"
      },
      "outputs": [],
      "source": [
        "### Creating the model instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "9a70da23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "9a70da23",
        "outputId": "e2c22bdc-4875-4d73-fefd-10bfd83994e2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-215-9bed1aa10334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model = create_lenet_network(\n\u001b[1;32m      2\u001b[0m             \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATASET_NAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATASET_NAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-102-fa81afe7535b>\u001b[0m in \u001b[0;36mcreate_lenet_network\u001b[0;34m(input_shape, output_shape)\u001b[0m\n\u001b[1;32m     65\u001b[0m         tf.keras.layers.Dense(\n\u001b[1;32m     66\u001b[0m                 \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     69\u001b[0m     ])\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   2011\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"conv2d_20\" (type Conv2D).\n\nNegative dimension size caused by subtracting 5 from 4 for '{{node conv2d_20/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_20/Conv2D/ReadVariableOp)' with input shapes: [?,4,4,16], [5,5,16,16].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 4, 4, 16), dtype=float32)"
          ]
        }
      ],
      "source": [
        "model = create_lenet_network(\n",
        "            input_shape=model_params[DATASET_NAME]['input_shape'],\n",
        "            output_shape=model_params[DATASET_NAME]['n_classes']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04db909f",
      "metadata": {
        "id": "04db909f"
      },
      "source": [
        "##### Performing training and validation loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e6d3916",
      "metadata": {
        "id": "4e6d3916"
      },
      "outputs": [],
      "source": [
        "### Defining the model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b810597f",
      "metadata": {
        "id": "b810597f"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7333d331",
      "metadata": {
        "id": "7333d331"
      },
      "outputs": [],
      "source": [
        "model_params[MODEL_NAME].update({'epochs': EPOCHS})\n",
        "model_params[MODEL_NAME].update({'batch_size': BATCH_SIZE})\n",
        "model_params[MODEL_NAME].update({'shuffle': True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb571ec8",
      "metadata": {
        "id": "cb571ec8"
      },
      "outputs": [],
      "source": [
        "### Configuring the model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e6598a",
      "metadata": {
        "id": "71e6598a"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "        optimizer=model_params[MODEL_NAME]['optimizer'],\n",
        "        loss=model_params[MODEL_NAME]['loss_fn'],\n",
        "        metrics=model_params[MODEL_NAME]['metrics']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1649cde7",
      "metadata": {
        "id": "1649cde7"
      },
      "outputs": [],
      "source": [
        "### Batching the training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8a8e46",
      "metadata": {
        "id": "9e8a8e46"
      },
      "outputs": [],
      "source": [
        "train_dataset_scaled = train_dataset_scaled.batch(model_params[MODEL_NAME]['batch_size'])\n",
        "validation_dataset_scaled = validation_dataset_scaled.batch(model_params[MODEL_NAME]['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6198ce5c",
      "metadata": {
        "id": "6198ce5c"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `training.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5495159d",
      "metadata": {
        "id": "5495159d"
      },
      "outputs": [],
      "source": [
        "logger.info(f\"\\nTraining for {model_params[MODEL_NAME]['epochs']} epochs \" + \n",
        "            f\"using '{model_params[DATASET_NAME]['imdir']}' data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fafce66b",
      "metadata": {
        "id": "fafce66b"
      },
      "outputs": [],
      "source": [
        "### Usage of the `fit()` API\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1934aa",
      "metadata": {
        "id": "5d1934aa"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "                x=train_dataset_scaled,\n",
        "                epochs=model_params[MODEL_NAME].get('epochs', 10),\n",
        "                batch_size=model_params[MODEL_NAME].get('batch_size', 128),\n",
        "                validation_data=validation_dataset_scaled,\n",
        "                shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ffb5a7a",
      "metadata": {
        "id": "7ffb5a7a"
      },
      "outputs": [],
      "source": [
        "### Usage of the `callbacks()` method\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06d45d16",
      "metadata": {
        "id": "06d45d16"
      },
      "outputs": [],
      "source": [
        "type(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf48c62",
      "metadata": {
        "id": "9bf48c62"
      },
      "source": [
        "##### Visualsing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2788c63",
      "metadata": {
        "id": "b2788c63"
      },
      "outputs": [],
      "source": [
        "display_metrics(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25615960",
      "metadata": {
        "id": "25615960"
      },
      "source": [
        "#### Evaluating the custom CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1607d0d",
      "metadata": {
        "id": "e1607d0d"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'Custom_CNN'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8354134",
      "metadata": {
        "id": "a8354134"
      },
      "source": [
        "##### Initialising the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50d1963",
      "metadata": {
        "id": "a50d1963"
      },
      "source": [
        "##### Performing training and validation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb4c42b",
      "metadata": {
        "id": "0cb4c42b"
      },
      "source": [
        "##### Visualsing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036e5ec4",
      "metadata": {
        "id": "036e5ec4"
      },
      "source": [
        "### 2.3 Evaluation on the GTSRB dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdad1f6d",
      "metadata": {
        "id": "bdad1f6d"
      },
      "source": [
        "#### Considerations for our input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5f711ba6",
      "metadata": {
        "id": "5f711ba6"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'GTSRB'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7288e8a5",
      "metadata": {
        "id": "7288e8a5"
      },
      "outputs": [],
      "source": [
        "N_CLASSES = 43                                    # GTSRB has 43 distinct digit classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfc0c89",
      "metadata": {
        "id": "fdfc0c89"
      },
      "outputs": [],
      "source": [
        "### Defining our input image specs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "5896cacd",
      "metadata": {
        "id": "5896cacd"
      },
      "outputs": [],
      "source": [
        "IMG_DIR = os.path.join(DIR_SRC, 'GTSRB/Final_Training/Images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "bd232e25",
      "metadata": {
        "id": "bd232e25"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "04c04d92",
      "metadata": {
        "id": "04c04d92"
      },
      "outputs": [],
      "source": [
        "IMG_COLOR_MODE = 'rgb'\n",
        "IMG_SIZE = (32, 32)                               # Each RGB image has 32x32 px resolution\n",
        "IMG_SHAPE = (32, 32, 3)                           # Each RGB image has 3 channels\n",
        "N_FEATURES = (32 * 32) * 3                        # Each pixel value is considered an attribute (feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "2de2807c",
      "metadata": {
        "id": "2de2807c"
      },
      "outputs": [],
      "source": [
        "model_params[DATASET_NAME].update({'n_classes': N_CLASSES})\n",
        "model_params[DATASET_NAME].update({'imdir': IMG_DIR})\n",
        "model_params[DATASET_NAME].update({'batch_size': BATCH_SIZE})\n",
        "model_params[DATASET_NAME].update({'color_mode': IMG_COLOR_MODE})\n",
        "model_params[DATASET_NAME].update({'image_size': IMG_SIZE})\n",
        "model_params[DATASET_NAME].update({'input_shape': IMG_SHAPE})\n",
        "model_params[DATASET_NAME].update({'n_features': N_FEATURES})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ec3188",
      "metadata": {
        "id": "28ec3188"
      },
      "source": [
        "#### Fetching the GTSRB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "37655102",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37655102",
        "outputId": "b827bd17-4c7b-4235-fbed-c97fa548bfb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4300 files belonging to 43 classes.\n",
            "Using 3870 files for training.\n",
            "Found 4300 files belonging to 43 classes.\n",
            "Using 430 files for validation.\n"
          ]
        }
      ],
      "source": [
        "### Fetching the training and validation datasets\n",
        "train_dataset, validation_dataset = get_datasets(model_params[DATASET_NAME])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff3e692",
      "metadata": {
        "id": "cff3e692"
      },
      "source": [
        "#### Processing the image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "cd22a1a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd22a1a9",
        "outputId": "56dd4182-e226-4d34-e27a-48859d2d6251"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3072"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "### Number of features (pixel values) in a single image\n",
        "train_iter = iter(train_dataset)\n",
        "len(train_iter.get_next()[0][0].numpy().flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a5d5385d",
      "metadata": {
        "id": "a5d5385d"
      },
      "outputs": [],
      "source": [
        "### From J. Moran's `2022-09-05-Image-Classification-Feed-Forward-Neural-Networks.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "90115a32",
      "metadata": {
        "id": "90115a32"
      },
      "outputs": [],
      "source": [
        "def process(dataset: tf.data.Dataset) -> Generator[tuple, None, None]:\n",
        "    \"\"\"Processes the input dataset and returns a generator object.\n",
        "\n",
        "    :param dataset: the tf.data.Dataset instance of batched image, label data.\n",
        "    :returns: the generator instance modifying the image attribute values.\n",
        "    \"\"\"\n",
        "\n",
        "    def process_generator()  -> Iterator[Tuple[tf.Tensor, tf.Tensor]]:\n",
        "        \"\"\"Scales the input image data.\n",
        "        \n",
        "        Unbatches the tf.data.Dataset and scales all image attributes,\n",
        "        i.e., pixel values, in the range [0, 1].\n",
        "\n",
        "        :returns: an iterator yielding the modfied image data and\n",
        "            corresponding label.\n",
        "        \"\"\"\n",
        "\n",
        "        for batch in dataset:\n",
        "            for image, label in zip(*batch):\n",
        "                image = tf.cast(image/255., tf.float32)\n",
        "                yield image, label\n",
        "    return process_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f9cad4",
      "metadata": {
        "id": "a3f9cad4"
      },
      "outputs": [],
      "source": [
        "### Unbatching the datasets and scaling the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "eadc5547",
      "metadata": {
        "id": "eadc5547"
      },
      "outputs": [],
      "source": [
        "train_dataset_scaled = tf.data.Dataset.from_generator(\n",
        "        generator=process(train_dataset), \n",
        "        output_signature=(\n",
        "            tf.TensorSpec(\n",
        "                shape=model_params[DATASET_NAME].get('input_shape', None),\n",
        "                dtype=tf.dtypes.float32), \n",
        "            tf.TensorSpec(\n",
        "                shape=(),\n",
        "                dtype=tf.dtypes.int32)\n",
        "        )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "3ebba510",
      "metadata": {
        "id": "3ebba510"
      },
      "outputs": [],
      "source": [
        "validation_dataset_scaled = tf.data.Dataset.from_generator(\n",
        "        generator=process(validation_dataset), \n",
        "        output_signature=(\n",
        "            tf.TensorSpec(\n",
        "                shape=model_params[DATASET_NAME].get('input_shape', None),\n",
        "                dtype=tf.dtypes.float32), \n",
        "            tf.TensorSpec(\n",
        "                shape=(),\n",
        "                dtype=tf.dtypes.int32)\n",
        "        )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "e4699eba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4699eba",
        "outputId": "9aaa3d84-1c70-4ecf-93c2-fddc7389aae3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.FlatMapDataset"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "type(validation_dataset_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf3b6ae4",
      "metadata": {
        "id": "cf3b6ae4"
      },
      "source": [
        "#### Evaluating the LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d29c235",
      "metadata": {
        "id": "7d29c235"
      },
      "source": [
        "##### Initialising the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "dd28fc67",
      "metadata": {
        "id": "dd28fc67"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'LeNet-5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "2271732e",
      "metadata": {
        "id": "2271732e"
      },
      "outputs": [],
      "source": [
        "### Creating the model instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "96e47fa9",
      "metadata": {
        "id": "96e47fa9"
      },
      "outputs": [],
      "source": [
        "model = create_lenet_network(\n",
        "            input_shape=model_params[DATASET_NAME]['input_shape'],\n",
        "            output_shape=model_params[DATASET_NAME]['n_classes']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mb4FJMBOoMU",
        "outputId": "fe365b7b-ce11-4e8a-9e29-f9898892a01d"
      },
      "id": "8Mb4FJMBOoMU",
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " average_pooling2d_10 (Avera  (None, 14, 14, 6)        0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 14, 14, 6)         0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " average_pooling2d_11 (Avera  (None, 5, 5, 16)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 5, 5, 16)          0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 1, 1, 16)          6416      \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 84)                1428      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 43)                3655      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,371\n",
            "Trainable params: 14,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556f2c4f",
      "metadata": {
        "id": "556f2c4f"
      },
      "source": [
        "##### Performing the training and validation loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c48a510",
      "metadata": {
        "id": "0c48a510"
      },
      "outputs": [],
      "source": [
        "### Defining the model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "6bbb1ad4",
      "metadata": {
        "id": "6bbb1ad4"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "199f2956",
      "metadata": {
        "id": "199f2956"
      },
      "outputs": [],
      "source": [
        "model_params[MODEL_NAME].update({'epochs': EPOCHS})\n",
        "model_params[MODEL_NAME].update({'batch_size': BATCH_SIZE})\n",
        "model_params[MODEL_NAME].update({'shuffle': True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d364fb39",
      "metadata": {
        "id": "d364fb39"
      },
      "outputs": [],
      "source": [
        "### Configuring the model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "ecd4aafd",
      "metadata": {
        "id": "ecd4aafd"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "        optimizer=model_params[MODEL_NAME]['optimizer'],\n",
        "        loss=model_params[MODEL_NAME]['loss_fn'],\n",
        "        metrics=model_params[MODEL_NAME]['metrics']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7195aa2",
      "metadata": {
        "id": "a7195aa2"
      },
      "outputs": [],
      "source": [
        "### Batching the training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "f07eee54",
      "metadata": {
        "id": "f07eee54"
      },
      "outputs": [],
      "source": [
        "train_dataset_scaled = train_dataset_scaled.batch(\n",
        "                            batch_size=model_params[MODEL_NAME]['batch_size']\n",
        ")\n",
        "validation_dataset_scaled = validation_dataset_scaled.batch(\n",
        "                            batch_size=model_params[MODEL_NAME]['batch_size']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter(train_dataset_scaled).get_next()"
      ],
      "metadata": {
        "id": "NAlhWSgJQyAo"
      },
      "id": "NAlhWSgJQyAo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c5f9129",
      "metadata": {
        "id": "8c5f9129"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `training.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "d45158f5",
      "metadata": {
        "id": "d45158f5"
      },
      "outputs": [],
      "source": [
        "logger = get_module_logger(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "5fb5bbc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fb5bbc4",
        "outputId": "3e1f661f-09d5-4c0b-91b4-b5f9755f22dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-14 23:26:17,445 INFO     \n",
            "Training for 100 epochs using '/content/data/GTSRB/Final_Training/Images' data\n",
            "2022-09-14 23:26:17,445 INFO     \n",
            "Training for 100 epochs using '/content/data/GTSRB/Final_Training/Images' data\n",
            "2022-09-14 23:26:17,445 INFO     \n",
            "Training for 100 epochs using '/content/data/GTSRB/Final_Training/Images' data\n"
          ]
        }
      ],
      "source": [
        "logger.info(f\"\\nTraining for {model_params[MODEL_NAME]['epochs']} epochs \" + \n",
        "            f\"using '{model_params[DATASET_NAME]['imdir']}' data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb2ec5c4",
      "metadata": {
        "id": "cb2ec5c4"
      },
      "outputs": [],
      "source": [
        "### Usage of the `fit()` API\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "c35a2d5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c35a2d5b",
        "outputId": "c70dc258-1648-4cab-a093-5a6402815d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "31/31 [==============================] - 5s 156ms/step - loss: 3.7762 - accuracy: 0.0274 - val_loss: 3.7666 - val_accuracy: 0.0302\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 5s 149ms/step - loss: 3.7681 - accuracy: 0.0212 - val_loss: 3.7687 - val_accuracy: 0.0070\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 5s 150ms/step - loss: 3.7675 - accuracy: 0.0233 - val_loss: 3.7771 - val_accuracy: 0.0116\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 6s 167ms/step - loss: 3.7630 - accuracy: 0.0199 - val_loss: 3.7753 - val_accuracy: 0.0395\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 5s 156ms/step - loss: 3.7553 - accuracy: 0.0282 - val_loss: 3.7455 - val_accuracy: 0.0163\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 5s 152ms/step - loss: 3.7089 - accuracy: 0.0380 - val_loss: 3.7030 - val_accuracy: 0.0465\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 5s 152ms/step - loss: 3.6651 - accuracy: 0.0429 - val_loss: 3.7008 - val_accuracy: 0.0442\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 5s 158ms/step - loss: 3.6508 - accuracy: 0.0460 - val_loss: 3.6974 - val_accuracy: 0.0302\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 5s 158ms/step - loss: 3.6500 - accuracy: 0.0481 - val_loss: 3.6992 - val_accuracy: 0.0442\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 5s 156ms/step - loss: 3.6450 - accuracy: 0.0429 - val_loss: 3.6744 - val_accuracy: 0.0605\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 5s 158ms/step - loss: 3.6248 - accuracy: 0.0581 - val_loss: 3.6551 - val_accuracy: 0.0605\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 6s 162ms/step - loss: 3.6019 - accuracy: 0.0612 - val_loss: 3.6265 - val_accuracy: 0.0605\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 6s 164ms/step - loss: 3.5788 - accuracy: 0.0685 - val_loss: 3.6010 - val_accuracy: 0.0674\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 5s 154ms/step - loss: 3.5572 - accuracy: 0.0700 - val_loss: 3.5913 - val_accuracy: 0.0605\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 5s 152ms/step - loss: 3.5349 - accuracy: 0.0780 - val_loss: 3.5340 - val_accuracy: 0.0837\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 5s 155ms/step - loss: 3.4985 - accuracy: 0.0907 - val_loss: 3.4892 - val_accuracy: 0.1023\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 5s 162ms/step - loss: 3.4399 - accuracy: 0.1047 - val_loss: 3.4098 - val_accuracy: 0.1000\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 3.3540 - accuracy: 0.0969 - val_loss: 3.3150 - val_accuracy: 0.1163\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 3.2257 - accuracy: 0.1214 - val_loss: 3.1802 - val_accuracy: 0.1535\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 5s 157ms/step - loss: 3.1337 - accuracy: 0.1357 - val_loss: 3.1120 - val_accuracy: 0.1558\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 5s 159ms/step - loss: 3.0553 - accuracy: 0.1390 - val_loss: 3.0645 - val_accuracy: 0.1581\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 5s 161ms/step - loss: 3.0015 - accuracy: 0.1478 - val_loss: 3.0528 - val_accuracy: 0.1279\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 5s 163ms/step - loss: 2.9601 - accuracy: 0.1545 - val_loss: 2.9396 - val_accuracy: 0.1535\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 5s 158ms/step - loss: 2.9138 - accuracy: 0.1623 - val_loss: 2.9648 - val_accuracy: 0.1209\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 5s 159ms/step - loss: 2.8854 - accuracy: 0.1638 - val_loss: 2.8608 - val_accuracy: 0.1791\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 6s 161ms/step - loss: 2.8286 - accuracy: 0.1690 - val_loss: 2.8543 - val_accuracy: 0.1558\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 2.7871 - accuracy: 0.1829 - val_loss: 2.8235 - val_accuracy: 0.1837\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 6s 164ms/step - loss: 2.7642 - accuracy: 0.1793 - val_loss: 2.7823 - val_accuracy: 0.1698\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 2.7230 - accuracy: 0.1915 - val_loss: 2.7146 - val_accuracy: 0.1930\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 6s 165ms/step - loss: 2.6423 - accuracy: 0.2127 - val_loss: 2.6667 - val_accuracy: 0.1837\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 6s 167ms/step - loss: 2.5977 - accuracy: 0.2109 - val_loss: 2.6007 - val_accuracy: 0.1930\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 6s 162ms/step - loss: 2.5395 - accuracy: 0.2305 - val_loss: 2.5087 - val_accuracy: 0.2116\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 6s 166ms/step - loss: 2.4703 - accuracy: 0.2483 - val_loss: 2.4527 - val_accuracy: 0.2302\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 6s 162ms/step - loss: 2.4111 - accuracy: 0.2618 - val_loss: 2.3939 - val_accuracy: 0.2442\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 2.3580 - accuracy: 0.2628 - val_loss: 2.3316 - val_accuracy: 0.2605\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 6s 165ms/step - loss: 2.3145 - accuracy: 0.2946 - val_loss: 2.3202 - val_accuracy: 0.2791\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 6s 165ms/step - loss: 2.2740 - accuracy: 0.2930 - val_loss: 2.2477 - val_accuracy: 0.2907\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 2.2155 - accuracy: 0.3134 - val_loss: 2.2082 - val_accuracy: 0.3047\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 6s 159ms/step - loss: 2.1492 - accuracy: 0.3295 - val_loss: 2.1425 - val_accuracy: 0.3233\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 6s 168ms/step - loss: 2.1014 - accuracy: 0.3370 - val_loss: 2.2740 - val_accuracy: 0.2930\n",
            "Epoch 41/100\n",
            "31/31 [==============================] - 6s 166ms/step - loss: 2.0925 - accuracy: 0.3416 - val_loss: 2.0649 - val_accuracy: 0.3535\n",
            "Epoch 42/100\n",
            "31/31 [==============================] - 6s 158ms/step - loss: 2.0215 - accuracy: 0.3667 - val_loss: 1.9910 - val_accuracy: 0.3605\n",
            "Epoch 43/100\n",
            "31/31 [==============================] - 6s 160ms/step - loss: 1.9713 - accuracy: 0.3884 - val_loss: 1.9856 - val_accuracy: 0.3465\n",
            "Epoch 44/100\n",
            "31/31 [==============================] - 6s 160ms/step - loss: 1.9378 - accuracy: 0.3938 - val_loss: 1.9120 - val_accuracy: 0.3837\n",
            "Epoch 45/100\n",
            "31/31 [==============================] - 5s 158ms/step - loss: 1.8988 - accuracy: 0.4052 - val_loss: 1.8891 - val_accuracy: 0.3767\n",
            "Epoch 46/100\n",
            "31/31 [==============================] - 6s 167ms/step - loss: 1.8733 - accuracy: 0.4142 - val_loss: 1.8903 - val_accuracy: 0.3744\n",
            "Epoch 47/100\n",
            "31/31 [==============================] - 6s 166ms/step - loss: 1.8340 - accuracy: 0.4307 - val_loss: 1.8577 - val_accuracy: 0.3837\n",
            "Epoch 48/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 1.7902 - accuracy: 0.4406 - val_loss: 1.7864 - val_accuracy: 0.4209\n",
            "Epoch 49/100\n",
            "31/31 [==============================] - 6s 166ms/step - loss: 1.7803 - accuracy: 0.4496 - val_loss: 1.7569 - val_accuracy: 0.4093\n",
            "Epoch 50/100\n",
            "31/31 [==============================] - 6s 160ms/step - loss: 1.7319 - accuracy: 0.4540 - val_loss: 1.7764 - val_accuracy: 0.4140\n",
            "Epoch 51/100\n",
            "31/31 [==============================] - 6s 164ms/step - loss: 1.7235 - accuracy: 0.4651 - val_loss: 1.7818 - val_accuracy: 0.4116\n",
            "Epoch 52/100\n",
            "31/31 [==============================] - 6s 166ms/step - loss: 1.7023 - accuracy: 0.4687 - val_loss: 1.7249 - val_accuracy: 0.4372\n",
            "Epoch 53/100\n",
            "31/31 [==============================] - 6s 161ms/step - loss: 1.6603 - accuracy: 0.4866 - val_loss: 1.6458 - val_accuracy: 0.4674\n",
            "Epoch 54/100\n",
            "31/31 [==============================] - 5s 163ms/step - loss: 1.6010 - accuracy: 0.5062 - val_loss: 1.6459 - val_accuracy: 0.4767\n",
            "Epoch 55/100\n",
            "31/31 [==============================] - 6s 162ms/step - loss: 1.5716 - accuracy: 0.5114 - val_loss: 1.5721 - val_accuracy: 0.5023\n",
            "Epoch 56/100\n",
            "31/31 [==============================] - 6s 165ms/step - loss: 1.5405 - accuracy: 0.5261 - val_loss: 1.5451 - val_accuracy: 0.4860\n",
            "Epoch 57/100\n",
            "31/31 [==============================] - 6s 162ms/step - loss: 1.5114 - accuracy: 0.5517 - val_loss: 1.5620 - val_accuracy: 0.4953\n",
            "Epoch 58/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 1.5040 - accuracy: 0.5411 - val_loss: 1.4976 - val_accuracy: 0.5140\n",
            "Epoch 59/100\n",
            "31/31 [==============================] - 6s 173ms/step - loss: 1.4706 - accuracy: 0.5543 - val_loss: 1.5144 - val_accuracy: 0.5279\n",
            "Epoch 60/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 1.4351 - accuracy: 0.5659 - val_loss: 1.4322 - val_accuracy: 0.5488\n",
            "Epoch 61/100\n",
            "31/31 [==============================] - 6s 169ms/step - loss: 1.3838 - accuracy: 0.5912 - val_loss: 1.4776 - val_accuracy: 0.5209\n",
            "Epoch 62/100\n",
            "31/31 [==============================] - 6s 169ms/step - loss: 1.3793 - accuracy: 0.5860 - val_loss: 1.3776 - val_accuracy: 0.5884\n",
            "Epoch 63/100\n",
            "31/31 [==============================] - 6s 176ms/step - loss: 1.3440 - accuracy: 0.6016 - val_loss: 1.3799 - val_accuracy: 0.5372\n",
            "Epoch 64/100\n",
            "31/31 [==============================] - 6s 178ms/step - loss: 1.3322 - accuracy: 0.5990 - val_loss: 1.3196 - val_accuracy: 0.5953\n",
            "Epoch 65/100\n",
            "31/31 [==============================] - 7s 191ms/step - loss: 1.2907 - accuracy: 0.6171 - val_loss: 1.3092 - val_accuracy: 0.5837\n",
            "Epoch 66/100\n",
            "31/31 [==============================] - 6s 159ms/step - loss: 1.2743 - accuracy: 0.6189 - val_loss: 1.2945 - val_accuracy: 0.6140\n",
            "Epoch 67/100\n",
            "31/31 [==============================] - 6s 161ms/step - loss: 1.2573 - accuracy: 0.6240 - val_loss: 1.2821 - val_accuracy: 0.5791\n",
            "Epoch 68/100\n",
            "31/31 [==============================] - 6s 162ms/step - loss: 1.2303 - accuracy: 0.6264 - val_loss: 1.2542 - val_accuracy: 0.6279\n",
            "Epoch 69/100\n",
            "31/31 [==============================] - 6s 164ms/step - loss: 1.1953 - accuracy: 0.6416 - val_loss: 1.2121 - val_accuracy: 0.6326\n",
            "Epoch 70/100\n",
            "31/31 [==============================] - 6s 168ms/step - loss: 1.1885 - accuracy: 0.6478 - val_loss: 1.2323 - val_accuracy: 0.6186\n",
            "Epoch 71/100\n",
            "31/31 [==============================] - 6s 166ms/step - loss: 1.1560 - accuracy: 0.6568 - val_loss: 1.2092 - val_accuracy: 0.6302\n",
            "Epoch 72/100\n",
            "31/31 [==============================] - 6s 164ms/step - loss: 1.1318 - accuracy: 0.6713 - val_loss: 1.1622 - val_accuracy: 0.6395\n",
            "Epoch 73/100\n",
            "31/31 [==============================] - 6s 165ms/step - loss: 1.1124 - accuracy: 0.6711 - val_loss: 1.1621 - val_accuracy: 0.6488\n",
            "Epoch 74/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 1.1012 - accuracy: 0.6739 - val_loss: 1.1298 - val_accuracy: 0.6558\n",
            "Epoch 75/100\n",
            "31/31 [==============================] - 6s 164ms/step - loss: 1.0769 - accuracy: 0.6786 - val_loss: 1.1133 - val_accuracy: 0.6558\n",
            "Epoch 76/100\n",
            "31/31 [==============================] - 6s 161ms/step - loss: 1.0536 - accuracy: 0.6943 - val_loss: 1.1076 - val_accuracy: 0.6674\n",
            "Epoch 77/100\n",
            "31/31 [==============================] - 5s 159ms/step - loss: 1.0470 - accuracy: 0.6899 - val_loss: 1.1003 - val_accuracy: 0.6605\n",
            "Epoch 78/100\n",
            "31/31 [==============================] - 5s 163ms/step - loss: 1.0250 - accuracy: 0.7013 - val_loss: 1.0800 - val_accuracy: 0.6651\n",
            "Epoch 79/100\n",
            "31/31 [==============================] - 5s 162ms/step - loss: 1.0156 - accuracy: 0.6995 - val_loss: 1.1007 - val_accuracy: 0.6791\n",
            "Epoch 80/100\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 1.0111 - accuracy: 0.7026 - val_loss: 1.0558 - val_accuracy: 0.6884\n",
            "Epoch 81/100\n",
            "31/31 [==============================] - 6s 164ms/step - loss: 0.9881 - accuracy: 0.6995 - val_loss: 1.0470 - val_accuracy: 0.6767\n",
            "Epoch 82/100\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.9698 - accuracy: 0.7147 - val_loss: 1.0428 - val_accuracy: 0.6651\n",
            "Epoch 83/100\n",
            "31/31 [==============================] - 5s 160ms/step - loss: 0.9557 - accuracy: 0.7183 - val_loss: 1.0167 - val_accuracy: 0.7140\n",
            "Epoch 84/100\n",
            "31/31 [==============================] - 5s 153ms/step - loss: 0.9582 - accuracy: 0.7152 - val_loss: 1.0139 - val_accuracy: 0.7047\n",
            "Epoch 85/100\n",
            "31/31 [==============================] - 5s 154ms/step - loss: 0.9397 - accuracy: 0.7181 - val_loss: 1.0189 - val_accuracy: 0.6907\n",
            "Epoch 86/100\n",
            "31/31 [==============================] - 5s 156ms/step - loss: 0.9383 - accuracy: 0.7212 - val_loss: 0.9887 - val_accuracy: 0.6977\n",
            "Epoch 87/100\n",
            "31/31 [==============================] - 5s 156ms/step - loss: 0.8964 - accuracy: 0.7351 - val_loss: 0.9462 - val_accuracy: 0.7070\n",
            "Epoch 88/100\n",
            "31/31 [==============================] - 5s 154ms/step - loss: 0.8914 - accuracy: 0.7271 - val_loss: 0.9940 - val_accuracy: 0.6977\n",
            "Epoch 89/100\n",
            "31/31 [==============================] - 5s 156ms/step - loss: 0.8979 - accuracy: 0.7323 - val_loss: 0.9673 - val_accuracy: 0.7070\n",
            "Epoch 90/100\n",
            "31/31 [==============================] - 6s 161ms/step - loss: 0.8895 - accuracy: 0.7276 - val_loss: 0.9374 - val_accuracy: 0.7186\n",
            "Epoch 91/100\n",
            "31/31 [==============================] - 6s 156ms/step - loss: 0.8868 - accuracy: 0.7362 - val_loss: 0.9266 - val_accuracy: 0.7140\n",
            "Epoch 92/100\n",
            "31/31 [==============================] - 5s 155ms/step - loss: 0.8473 - accuracy: 0.7496 - val_loss: 0.9235 - val_accuracy: 0.7163\n",
            "Epoch 93/100\n",
            "31/31 [==============================] - 5s 156ms/step - loss: 0.8286 - accuracy: 0.7615 - val_loss: 0.8985 - val_accuracy: 0.7326\n",
            "Epoch 94/100\n",
            "31/31 [==============================] - 5s 152ms/step - loss: 0.8267 - accuracy: 0.7530 - val_loss: 0.8945 - val_accuracy: 0.7326\n",
            "Epoch 95/100\n",
            "31/31 [==============================] - 6s 165ms/step - loss: 0.7983 - accuracy: 0.7661 - val_loss: 0.8980 - val_accuracy: 0.7279\n",
            "Epoch 96/100\n",
            "31/31 [==============================] - 6s 164ms/step - loss: 0.8242 - accuracy: 0.7581 - val_loss: 0.8927 - val_accuracy: 0.7256\n",
            "Epoch 97/100\n",
            "31/31 [==============================] - 6s 170ms/step - loss: 0.8028 - accuracy: 0.7685 - val_loss: 0.9021 - val_accuracy: 0.7279\n",
            "Epoch 98/100\n",
            "31/31 [==============================] - 6s 163ms/step - loss: 0.7914 - accuracy: 0.7716 - val_loss: 0.8667 - val_accuracy: 0.7488\n",
            "Epoch 99/100\n",
            "31/31 [==============================] - 5s 163ms/step - loss: 0.7746 - accuracy: 0.7726 - val_loss: 0.8906 - val_accuracy: 0.7256\n",
            "Epoch 100/100\n",
            "31/31 [==============================] - 6s 165ms/step - loss: 0.7589 - accuracy: 0.7796 - val_loss: 0.9010 - val_accuracy: 0.7488\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "                x=train_dataset_scaled,\n",
        "                epochs=model_params[MODEL_NAME].get('epochs', 10),\n",
        "                batch_size=model_params[MODEL_NAME].get('batch_size', 128),\n",
        "                validation_data=validation_dataset_scaled,\n",
        "                shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821d23bf",
      "metadata": {
        "id": "821d23bf"
      },
      "outputs": [],
      "source": [
        "### Usage of the `callbacks()` method\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History\n",
        "# See: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "f0af6ac8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0af6ac8",
        "outputId": "4cb6e070-9e4e-4126-e68d-9d2272212b1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.callbacks.History"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "type(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5827b5f0",
      "metadata": {
        "id": "5827b5f0"
      },
      "source": [
        "##### Visualising the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe4a1ce",
      "metadata": {
        "id": "2fe4a1ce"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `utils.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "bf0563b2",
      "metadata": {
        "id": "bf0563b2"
      },
      "outputs": [],
      "source": [
        "def display_metrics(history: tf.keras.callbacks.History):\n",
        "    \"\"\"Plots the per-epoch loss and accuracy metrics.\n",
        "    \n",
        "    :param history: the Keras `callbacks.History` object.\n",
        "    \"\"\"\n",
        "    \n",
        "    f, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax[0].plot(history.history['loss'], linewidth=3)\n",
        "    ax[0].plot(history.history['val_loss'], linewidth=3)\n",
        "    ax[0].set_title('Loss', fontsize=16)\n",
        "    ax[0].set_ylabel('Loss', fontsize=16)\n",
        "    ax[0].set_xlabel('Epoch', fontsize=16)\n",
        "    ax[0].legend(['train loss', 'val loss'], loc='upper right')\n",
        "    ax[1].plot(history.history['accuracy'], linewidth=3)\n",
        "    ax[1].plot(history.history['val_accuracy'], linewidth=3)\n",
        "    ax[1].set_title('Accuracy', fontsize=16)\n",
        "    ax[1].set_ylabel('Accuracy', fontsize=16)\n",
        "    ax[1].set_xlabel('Epoch', fontsize=16)\n",
        "    ax[1].legend(['train acc', 'val acc'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b974d2d8",
      "metadata": {
        "id": "b974d2d8"
      },
      "outputs": [],
      "source": [
        "### From Udacity's `training.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "72c43183",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "72c43183",
        "outputId": "2454bd5e-3219-435d-e7b3-0d636200a987"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA30AAAFUCAYAAACQtlVuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3RU1fbA8e+eSYdAQkIAE3rvvUmTIoIoIojIAwQLWLHrwwq2Zxef/lAUxIeKgIooIIIgvUqXKiA9tISEQCB15vz+uEMmjZ7MhGR/1pqVe889986e4S5m9pwmxhiUUkoppZRSShVONm8HoJRSSimllFIq/2jSp5RSSimllFKFmCZ9SimllFJKKVWIadKnlFJKKaWUUoWYJn1KKaWUUkopVYhp0qeUUkoppZRShZgmfUp5kYgMEREjItW8HYtSSimVF0RknOuzbbS3Y1FKWTTpU0oppZRSeUJEAoE7Xbv/EhEfb8ajlLJo0qeUUkoppfJKL6AEMBuIALp5N5ysRMSuiagqijTpU6oAExFfEXlDRPaJSKrr7xsi4pupjo+IvC4i/4hIsojEisgyEWmbqc6/RGSDiCSKyCkR2SwiD3jnVSmllCrEBgPxwBAgybWfhYjcLiLLM30m/SkiPTMd9xGRf4vINtfnWoyIzBGRWq7j54ZGVMp23VEiYrKVGRF5U0RGiMheIBWoLyIBIjJaRLa44jgqIjPPPUe2a1QWkW9cdVJEZI+I/Nd17GlXWels54ir3pQreheVymP6S4dSBdtErG4y/wGWAdcDLwJVgH+56vwbeNJVvhHrF9ZmQCkAV/L3LfAx8CzWjz21gBBPvQillFKFn4hcB3QBxhljYkTkZ6C3iIQaY+JddYZjfR79jJUQJgJNgEqZLjUFq8XwI2A+EAC0B8oBO64gtCHAHuAZ4AxwGPAHgoE3gCNYn5kPAytFpLYx5qgr3srAn8BZ4BVgF1AB6Oq69leua9wDvJvpObsClYF7ryBepfKcJn1KFVAiUg/oD7xqjBnlKv5dRNKB10XkbWPMX0Br4HdjzH8znT4z03Yr4KQx5olMZb/nY+hKKaWKpoGAHfjatT8R63OsHzBWREpg/Yg53RjTO9N5c89tiEgnoA/wuDHm40x1fr6KuAToaoxJylZ+f6bntbviOOaK+dwkNK8CgUBDY8zhTOdOBDDGxInIVGCYiLxnjDnX0vgAsMMYs+gq4lYqz2j3TqUKrvauv99mKz+338H1dw1ws6v7SlsR8ctWfw0QKiLfisgtIqItfEoppfLDYGCXMWala38+VqvauS6e1wPFgS8ucI2ugAHG5WFcc3JJ+BCRO0VktYicBNKxWgGLAzWzxTMrW8KX3adAVaCz67rlgFu58OtUyqM06VOq4Crl+nskW/nRbMf/A4wEegJLgRMi8pWIhAMYYxYDfYHywHQgRkTmi0iD/AxeKaVU0SEizYA6wE8iEuL6gTEY+AloJSI1gDBX9UMXuFQYEJdbknYVsn+OIiK3AlOB7VjDJVoCzYEYrO6kmeO5ULwYY/4E1gEPuorux0oiJ15t4ErlFU36lCq44lx/y2YrL5v5uDEmzRjzjjGmPtZ4hyexusaMOXeCMeZHY0wHIBS43VVvjojo/wFKKaXywrnWvH9jTeRy7vGoq/xuINa1HXmB68QCpVxLP5xPsutv9p4tYdkruphcyu4CdhtjhhhjZrsSt024f1DNHM+F4j3nU+A2EYnESvp+MMbEXeQcpTxGv/ApVXAtcf29K1v5ANffRdlPMMYcNcaMx+pSUy+X44nGmFnA51iJ3/k+IJVSSqlL4hpW0B9YDXTM5bERGASswJq4ZdgFLvc71hi8+y9QZ7/rb8bnnGsZhq65V89VEFZrXGaDsMYkZo/nFleXzQuZDJwGvsOa6GXsZcSiVL7TiVyUKhi6icjRbGUJWB8io1wfZiuwJm15GZhsjNkMICK/YP06uR7rV9XGWOsife46/hpQBliINbYiCngM2GiMicnn16WUUqrw64H1I+LTuU1cIiKfA59hzSz9PPCJiEwDJmElSo2AZGPMJ8aYha5jH4pIeWAB4Is1zv1X1/XXAP8A77l6rKRgzbzpfxkxzwF6ichoYJYrtuHAyWz1RgI3AytE5D/AbqyWv27GmIHnKhljkkTkf1i9bTYbY1ZcRixK5TtN+pQqGD7JpWwr1jTWe7CmfH4JK2l7B2s2sXOWYI3ZewTrl8sDWNNGv+k6vhoryRuN1W3lONYvly/n9YtQSilVJA3GSt5+OM/xycCHwGBjzBDXj5zPYiV9aVjj6l7PVP8urG6ig4EnsH4EXQOMBzDGpIvIbVjDGP6HNdzhI6zPu5GXGPM4rLHu92LNtLkGa/KV6ZkrGWP2iUgrrGUZ3sKa6CUa+CWXa/6AlfR9fokxKOUx4p5ZVimllFJKKXUlRORN4HHgOmPMKW/Ho1Rm2tKnlFJKKaXUFRKRxljLPDwOfKEJnyqItKVPKaWUUkqpKyQi+7DGzs8FBhljTns3IqVy0qRPKaWUUkoppQoxXbJBKaWUUkoppQoxTfqUUkoppZRSqhArFBO5hIeHm0qVKnk7DKWUUh6wbt26WGNMaW/Hca3Qz0illCoaLvT5WCiSvkqVKrF27Vpvh6GUUsoDRGS/t2O4luhnpFJKFQ0X+nzU7p1KKaVUASEi3UTkbxHZLSIjcjleQUQWisgGEflLRG72RpxKKaWuLZr0KaWUUgWAiNiBMUB3oA7QX0TqZKv2EvC9MaYxcBfwqWejVEopdS3SpE8ppZQqGFoAu40xe4wxqcAU4LZsdQxQwrVdEjjswfiUUkpdowrFmD6llCpo0tLSOHToEMnJyd4O5ZoVEBBAVFQUvr6+3g7FUyKBg5n2DwEts9UZBfwuIsOBYkCXK3kivT+vTBG8J5VShYQmfUoplQ8OHTpEcHAwlSpVQkS8Hc41xxjDiRMnOHToEJUrV/Z2OAVJf+B/xpgPRKQ18I2I1DPGODNXEpFhwDCAChUq5LiI3p+XT+9JpdS1TLt3KqVUPkhOTiYsLEy/UF8hESEsLKyotURFA+Uz7Ue5yjK7D/gewBizEggAwrNfyBjzhTGmmTGmWenSOWfv1vvz8hXRe1IpVUho0qeUUvlEv1BfnSL4/q0BqotIZRHxw5qoZUa2OgeAzgAiUhsr6Yu5kicrgu/vVdP3TCl1rdKkTymlCqGTJ0/y6adXNrHjzTffzMmTJy+5/qhRo3j//fev6LmUmzEmHXgUmAtsx5qlc6uIvCYiPV3VngaGisgmYDIwxBhjvBPxlfPk/amUUkqTPgC2RCdw/PR5ums40iB+HyREQ1I8pKd6NDallLoSF/pSnZ6efsFzZ8+eTUhISH6EpS7CGDPbGFPDGFPVGPOmq+wVY8wM1/Y2Y0wbY0xDY0wjY8zv3o34yuj9qZRSblsPJzB1zYF8fQ6dyMWRxufffMPpUycpG+CgeghUCnZS3hwm7NR2Qk79jd2ZLdELrwnd34GqHb0Ts1JKXcSIESP4559/aNSoETfeeCM9evTg5ZdfJjQ0lB07drBz50569erFwYMHSU5O5vHHH2fYsGEAVKpUibVr15KYmEj37t1p27YtK1asIDIykl9++YXAwMDzPu/GjRt58MEHOXv2LFWrVmXChAmEhoby8ccfM3bsWHx8fKhTpw5Tpkxh8eLFPP7444DVbW7JkiUEBwd75P1R3uXJ+3PmzJm88cYbpKamEhYWxqRJkyhTpgyJiYkMHz6ctWvXIiKMHDmSPn36MGfOHF544QUcDgfh4eH88ccf3niLlFJFQFKqg4/+2Mn4pXuxCTSuEEqNMvnzOVjkk75TZxL5JPlF8AOcQJzrcSGxf8M3vVgd0ZfoZiO4uXFlAnzt+R+sUkpdorfffpstW7awceNGABYtWsT69evZsmVLxsyDEyZMoFSpUiQlJdG8eXP69OlDWFhYluvs2rWLyZMnM27cOO68806mTZvGwIEDz/u8d999N5988gkdOnTglVde4dVXX+Wjjz7i7bffZu/evfj7+2d0zXv//fcZM2YMbdq0ITExkYCAgHx6N1RB48n7s23btqxatQoRYfz48bz77rt88MEHvP7665QsWZLNmzcDEB8fT0xMDEOHDmXJkiVUrlyZuLiLfSFQSqkrs2J3LM9P38z+E2cBcACjZmzlu6Gt8uX5inzSF5/qk7HK7fkcM1Y3kkBSKUYSdrGGT7Q8/gO7Zq3gscXP8dZD/Qkr7p/P0SqlrkWVRvyab9fe93aPS67bokWLLFPNf/zxx0yfPh2AgwcPsmvXrhxfqitXrkyjRo0AaNq0Kfv27Tvv9RMSEjh58iQdOnQAYPDgwfTt2xeABg0aMGDAAHr16kWvXr0AaNOmDU899RQDBgygd+/eREVFXfJrUXmnsN+fhw4dol+/fhw5coTU1NSM55g/fz5TpkzJqBcaGsrMmTNp3759Rp1SpUpdcvxKKZWbNIeT9fvjWbY7lv0nzhJzOoXjp5P5J+ZMlnotK5fijV718i2OIp/0VQwPhiodMWLnDAHEpflxPNWHWEI56F+dg/41iCOY46dTOJqQTNqpo7wun9PFvgGA6rZo3k8cwWvj0hn18BCK+xf5t1QpVUAVK1YsY3vRokXMnz+flStXEhQUxA033JDrVPT+/u4fs+x2O0lJSVf03L/++itLlixh5syZvPnmm2zevJkRI0bQo0cPZs+eTZs2bZg7dy61atW6ouura19+3Z/Dhw/nqaeeomfPnixatIhRo0blS/xKKZXZmn1xfLl0L8t3x3I65fxjlYMDfHjh5tr0a1Yemy3/ZgjWDAXg7p8RoLjrkXMZWzen0xAd35OtK8ZTfcN/8HMmU0KSeOXkS7w33s4LD9yNv4929VRKeVdwcDCnT58+7/GEhARCQ0MJCgpix44drFq16qqfs2TJkoSGhrJ06VLatWvHN998Q4cOHXA6nRw8eJCOHTvStm1bpkyZQmJiIidOnKB+/frUr1+fNWvWsGPHDk36ighP3p8JCQlERkYCMHHixIzyG2+8kTFjxvDRRx8BVvfOVq1a8fDDD7N3796M7p3a2qeUOp/vVh/gl43R3NrwOga0rJCxrMsvG6N56vtNOJznn1xZBLrXK8uoW+sSUSL/hzdo0neZbDahfFgxuPVxaN6Z5C9vISAtnhKSxDPHn+eTr4Qn7x2E3a6Jn1LKcjld3PJKWFgYbdq0oV69enTv3p0ePbLG0K1bN8aOHUvt2rWpWbMmrVrlzRiCiRMnZkzkUqVKFb766iscDgcDBw4kISEBYwyPPfYYISEhvPzyyyxcuBCbzUbdunXp3r17nsSgLk9hvz9HjRpF3759CQ0NpVOnTuzduxeAl156iUceeYR69epht9sZOXIkvXv35osvvqB37944nU4iIiKYN2/eVb1WpdS1LSnVwbr98TStGEqgn/v7/dQ1B3hhujUmePXeOBb9fZz37mjIb1uO8uLPm8m8mE5kSCAdapamcfkQypQIIKKEP+VKBFIyyNdjr0OuweV9cmjWrJlZu3atd5782FaSxt9MYFrWNYOMbzHEPxiCwiColPXX5gNpSZB2xtqu0wsa/QtsmiAqVdhs376d2rVrezuMa15u76OIrDPGNPNSSNec3D4j9f68cvreKVV0RJ9M4s6xK4k+mURkSCBjBzalflRJVu85wcAvV5PmyJpHhQb5En82LWO/RpnifNy/MTXLBGe0AuanC30+akvf1SpTl4D7fuXM+B4US3cnfpJ2xkruEo+e/9zd82HNOOj+LlTIn5l6lFJKKaWUUpcn7kwqg75cTfRJa6xw9Mkk+oxdwZNdajBu6Z6MhK90sD8xp1MAsiR8DaJKMvGeFoQW8/N88LnQxdnzgJStR+D9v7GnWCPOmMucwfPIJphwE/xwDxzdnD8BKqWUUkoppS5JYko693z1J3uyzbCZmu7knTk7iDtjreEdXtyPnx9pw4QhzQjN1FWzRaVSTLq/ZYFJ+EBb+vKMrWwdKj29iGd+2MTPGw5SjGSKk8QN5e10q+JL8whDkK+AbxD4BsLB1bD8v5Dumo1s60/Wo2onuH44VOlojfBUSimllFJKeURKuoMHvlnLpkMJgPV1/KUedfhh7UF2HHVPQOVnt/H5oKZEhgQSGRLIb4+3Z9zSPRTzs/PQDdWyjP8DIOZv8AmA0IpZy1PPwOznoMVQuK5Rvr0uTfrykM0mvHtHA1IdTmb9dYTTBDH5IEw+CL52oUaZYHztNnztgr9PZ2pWasKd8eOoeWK++yL/LLAeldtDz//LeWMopZRSSiml8pwxhhenb2H57hMZZW/0qseAlhXp36I8L/y0mZ83HsYm8Fbv+jSt6J7dt2zJAF6+pU7uF/7rB/hpqJX0DfkVopq6j63/BjZ+az2a3A09P8mX16ZJXx7zsdsY3a8RAb52flx3KKM8zWHYevhUlrrLgC+5lwbSgWE+v3Kz/U9sOK2De5fAZ9fDja9Bs3u11U8ppZRSSql89L8V+7J8f3+maw0GtLQaYIL8fBjdrxFD21fB38dGtYjgS7to6ln4/UXAQHoSLHwDBk23jjnSYEWmJK9sgzx6JTnpmL584Gu38X7fhix9riPPd69Fw6iSF6z/l6nKo2mP0SHlA75OvxHHuX+W1ET49Sn4+jaI3e2ByJVSSimllCp6lu+O5Y1ft2fs39E0ikc6VstSR0Soe13JS0/4AP78AhKPuff/WQCHN1rbm3+EU64kMygcGg+80vAvSlv68lH5UkE80KEqD3SoypGEJI6fSiHd6STNYUhKc3A0IZnDJ5P4JyaR37ce46CzDK+k38NPjna87zuWarbD1oX2LobPWkPrR6H9M+BXzLsvTClVKBUvXpzExMRLLlfKk/Q+VErll/0nzvDwpPUZi6k3Kh/CG73qXf0yC8kJsPyjnOXLP4I+E2DZaHdZq4eseT/yiSZ9HlKuZCDlSp7/HzL6ZBLjluxhypoDbEyrRo/U//Ckz48Mtf+KXQw4UmHZh/DX99DwLijfEqKaWWsAKqWUUkopVcRNWLaXyX8eoGvdMjzeuQZ+Plk7NRpjciRy24+c4oFv1pGQZC23EBHsz+eDmhLgm2kiljOx1nfxc4LCwecSZuZcOQaS4q3tYqXhTIy1ve0XuO7/IPZva98vGJrff1mv9XJp984CIjIkkFE967Ls3524p00lnHZ/3k7/Fz1T32Sds7q74qlDsPR9+K4vvFsZvu1j3YhKKZXJiBEjGDNmTMb+qFGjeP/990lMTKRz5840adKE+vXr88svv1zyNY0xPPvss9SrV4/69eszdepUAI4cOUL79u1p1KgR9erVY+nSpTgcDoYMGZJRd/To0Re5uipK8vL+7NWrF02bNqVu3bp88cUXGeVz5syhSZMmNGzYkM6dOwOQmJjIPffcQ/369WnQoAHTpk3L+xenlPKKaesO8dqsbew6nsiYhf9wx9gV7Is9gzGGFf/EMujL1dR8eQ53T/iTP7Yfw+k0/Lwhmts/Xc6BuLOAe0bOMiUCrIs6nTB1ILxXFT6s7X68WxlmPwuxu84f0JlYK+k7p+ubUK2LtW2cMO9l97Fm90BgSB6/I1mJMebitQq4Zs2ambVr13o7jDx1MO4sH87byc8bo8E46WNfygifyYTLqZyVq3aCAdPApjm8UgXF9u3bqV27tteef8OGDTzxxBMsXrwYgDp16jB37lzKlSvH2bNnKVGiBLGxsbRq1Ypdu3YhIhft3jlt2jTGjh3LnDlziI2NpXnz5qxevZrvvvuO5ORkXnzxRRwOB2fPnmXnzp2MGDGCefPmAXDy5ElCQi7/Ay2391FE1hljml3B21Ik5fYZWZjuz7i4OEqVKkVSUhLNmzdn8eLFOJ1OmjRpwpIlS6hcuXJGnX//+9+kpKTw0UdWd6v4+HhCQ0MvK3Zvv3dKqZy2Hk6g96crSEl3Zikv5menakRx/nItv5BZRLA/x12LqgME+dn5qF8jutYt6660+nP47bkLP3mVjlCjm9UDr2x9a3KWI5ussXzbfrbqlK4NDy2HAyvhfz2ynm/3gyc2Q3DZnNe+TBf6fNTunQVU+VJB1gxB7arwzpwd/LizA785WtDBtommtl00s++iobgmd/lnASwfDe2e9m7QSqncjbrwZE5Xd+2cH2QAjRs35vjx4xw+fJiYmBhCQ0MpX748aWlpvPDCCyxZsgSbzUZ0dDTHjh2jbNmLf9gsW7aM/v37Y7fbKVOmDB06dGDNmjU0b96ce++9l7S0NHr16kWjRo2oUqUKe/bsYfjw4fTo0YOuXbvm9StXeeUavz8//vhjpk+3ZsI7ePAgu3btIiYmhvbt21O5cmUASpWyhkLMnz+fKVOmZJx7uQmfUqrgSTibxkPfrs9I+MqVDCA2MYU0h+FMqiPXhA/IkvBVKV2Mzwc2pXqZTBO0nPgH5o107weFWQlaegokxbnL9yy0HgA2XzAOqyUvs04vgs0OFdtAVHM4tMZ9rNG/8iThuxhtGirg6lxXgon3tmDS/S2pHFmG2c5WvJ4+iNtSXmNMes+MembBm7B/pRcjVUoVNH379uXHH39k6tSp9OvXD4BJkyYRExPDunXr2LhxI2XKlCE5Ofmqnqd9+/YsWbKEyMhIhgwZwtdff01oaCibNm3ihhtuYOzYsdx/f/6OVVDXnry4PxctWsT8+fNZuXIlmzZtonHjxld9Pyulrh1HEpJ4fOqGjO6Zxf19+Pb+lvz0UBsqhQVl1POz2+jfogI/PNiaYe2rUDLQN+NY93pl+eWRNlkTPqcDfn7YWmIBIKIuPLUDnt4Bz+2Bu3+Bmj1AsqVSzrScCV/5VlDrFmtbBNo+6T4mNrj+sat+Hy6FtvRdI9pUC2fGI235fdsxPpq/kx1HT/Nhel9a2HbQ3LYTMQ4SvxuMz8PLCShZ2tvhKqUKgH79+jF06FBiY2MzutElJCQQERGBr68vCxcuZP/+/Zd8vXbt2vH5558zePBg4uLiWLJkCe+99x779+8nKiqKoUOHkpKSwvr167n55pvx8/OjT58+1KxZk4ED828aanVtyov7MyEhgdDQUIKCgtixYwerVq0CoFWrVjz88MPs3bs3S/fOG2+8kTFjxlxV906lVP5KTnOwZGcM246cokrp4rSvHk5IkDVpSlKqg7X741i2K5ZFf8fw97HTWc59v29DqpYuDsCsx9oxYdleAPo1L58xTq95pVI82SGK/bPexVecVOlyPxLgm+U6rB4LB63/T7D5QK9P3RO3iECVG6xH/H7YOQcOrYXotRC3BxCIqA2RTSCqBdS9Pet62zW6Q+2esH0GtH8Owqrm3Zt3AR5N+kQkAFgC+Lue+0djzMhsdYYA7wHRrqL/M8aM92ScBZXNJnSrV5audcowZ+tRRs/byWPHhzPb/3lCJZHiKcfY8MkdlH3oF8qF5e9gUKXUZThPF7f8VrduXU6fPk1kZCTlypUDYMCAAdx6663Ur1+fZs2aUatWrUu+3u23387KlStp2LAhIsK7775L2bJlmThxIu+99x6+vr4UL16cr7/+mujoaO655x6cTusXz7feeitfXqPKA9fw/dmtWzfGjh1L7dq1qVmzJq1atQKgdOnSfPHFF/Tu3Run00lERATz5s3jpZde4pFHHqFevXrY7XZGjhxJ79698/21KqUubsXuWKauPcgf24+TmJJOSRJJoDg2sZZQ8LXb2HDgJKkOZ67nP9ChCt3qubtJFvf34bHO1XNWTE8hcNogap3rkrltDFTvCo0HQOJxiF4PW39y12/3DFzXKPegQytCywesB1hLNIgd/Iuf/4XabHDn1+BMB7vv+evlMY9O5CLWHKnFjDGJIuILLAMeN8asylRnCNDMGPPopV63ME7kcikcTsOMTdGsmvMd76S8mVG+WJoTdu8U6pUP92J0ShVtOtlD3tCJXK5eQZzI5Vqm751SeW/KnwcY8dPmjP03fb5kgM8fLHPUZXjacOIpket5fnYbLSqXomej6+jbNOri6+o50uGHwbBj1qUFVqY+DF1wacszFAAFZiIXY2WY56be8nU9rv3pQ73EbhNubxxFj/rPsPa7eJrt+RSADmYNs8bdzdE7x9Ol3nVejlIppZRSSqnc7T5+mlEzt2bslyCRAT5/ANDWvpXptpHcm/ose4z1nbZ6RHHaVAunXfVwWlcNI8jvEtMZpxNmPJo14SvbAI7+lXv9kIrQZ9w1k/BdjMfH9ImIHVgHVAPGGGNW51Ktj4i0B3YCTxpjDnoyxmuNn4+NZoP+w6Ef0onaZq1RdIttOVOnPsQS33G0rxnh5QiVUkpdjIh0A/4L2IHxxpi3sx0fDXR07QYBEcYY7cuvlLpmpaQ7eGzyRpLTrC6bNcoUZ1zrNJjjrlNJjjG3+GusbfUxVZt3J+LcGnrZOR2w+F3YOh3q97VmtT+3nJnTAb8+DZsmu+u3fhS6vmHN0rlmnDUuL6QCRDa1ll8o1wh8z/Nc1yCPJ33GGAfQSERCgOkiUs8YsyVTlZnAZGNMiog8AEwEOmW/jogMA4YBVKhQwQORF3AiRPV9l5PTUgjZMhGAfvZFjJz6MbWffJnSwf5eDlAppdT5uH4QHQPcCBwC1ojIDGPMtnN1jDFPZqo/HGjs8UCVUuoiUtOdpDmcFPN3pxnGGPadOMvy3bE4jaF1lTCqRRTnvTl/s+2ItQa1n4+Nj/s3puK2T3Jc0zftFK2X3w8VpkKJLjmfNCURpt0PO3+z9he+AUc3we1fWEso/Hgf7Jrrrt9ksJXwiUB4Nej+Tp6+BwWR12bvNMacFJGFQDdgS6byE5mqjQfePc/5XwBfgDVeIR9DvXaIENL7I5KcSQRu+x6AoY7JjPi+G+PuaYPNdpF+zkqpPGWMufj4AnVenhxzXgC0AHYbY/YAiMgU4DZg23nq9wdGnufYJdH78/IVsXtSqcu2bn8cw75ex4kzqZQrGUC1iOJEBAewZl9cxrIK51xXMoDDCe4lVl7oXotaZUvA73+6K13/GPz1PSQetSY+mXYfPLAYQiu56yREw+R+cNQ9JhCA7TMh4WZrHN+xTMfq94VbRmedUbMI8Og6fSJS2tXCh4gEYv2iuSNbnXKZdnsC2z0XYSFgsxF42wwx4JwAACAASURBVIek+lsL0UZJLJX2fMf4ZXu8HJhSRUtAQAAnTpzQL4lXyBjDiRMnCAgoPF1rLiISyDyU4ZCrLAcRqQhUBhZc6ZPp/Xn5iuA9qdRl2XwogSET1nDiTCoARxKSWborlmnrD+VI+IAsCV/HmqUZfH0lqxvmoUwTTzW/H4b+AcGuOSqST8LUQZDmWj9v93wY3zlrwlehdaYn2ZA14Wv7lNX6Z7Nf7cu95ni6pa8cMNHVjcUGfG+MmSUirwFrjTEzgMdEpCeQDsQBQzwc47XPPxi/Ts/Db88C8KjPL3Sc04mWlcNoWF6HfyjlCVFRURw6dIiYmBhvh3LNCggIICoqytthFER3YS155DhfhYsNgdD788roPakUnDybygvTN3MiMZVbGpTjtsaRHD6ZxKAJqzmdkn7e84r52bm+Wjg+NmHZrtiMuqWD/Xn3DmspII5th1TXnI/Fy1pj7ESsJQ6+6m4tfn70L5gx3FrY/K+p7iew+VgteE3uhjXjYfZzVtdOAJsv3PoRNC66a8Z6dMmG/FJUl2y4oPRUzJjmSPw+AP4v/TZmht3Pb4+3026eSqlrWmFcskFEWgOjjDE3ufafBzDG5FjgUEQ2AI8YY1ZcyrX1M1IplVeMMTzwzTp+33YsoyzQ146fj42EpDQASgb6Mun+lgT42tl9/DSHTyZTq2wwzSqVws/H6mSY5nCy4cBJdh0/TadaEZQrGWhdbO0EmOUavlz7Vuj3rfvJ14y3JmPJTWAo9P2ftWD6ObvnW9cSO/T8BCq3y5s3oQArMEs2KA/y8UM6vWz1fQbus//G18e68tuWo/RoUO4iJyullPKwNUB1EakMRGO15v0reyURqQWEAis9G55SSsGP6w5lSfgAktIcJKVZLWrB/j58c18L6kWWBKBaRKZFyo9sslrfHCn43voxLSo3oEXlUlmf4OAa93ZUi6zHmt0Hh9bBpu+ylte7A7q9DcVLZy2v1gUe22S1FBax8Xu58eiYPuVhdXtb648AgZLK4z4/8fEfu3A6r/3WXaWUKkyMMenAo8BcrLHs3xtjtorIa64hD+fcBUwxhaGbjlLqmnIw7iyvznTPLdWuejg1ywRn7Af62vnqnuY0iMplKNHfv8GE7nBwlTXO7pteELMzlyfJtJJb+ZZZj4nALR+6y0uWh3/9AHd8mTPhO8dm04TPRVv6CjObDbqMgm97A9DXvoiPjvVmztaj3FxfW/uUUqogMcbMBmZnK3sl2/4oT8aklFIATqfhmR82kegah1c5vBifD2pKoK+d9QfiWbMvni61I6gWEZz1RGNg1acw90Ug029VZ09Yid+9c6xxewBnTkDcP9a2zRfKNcwZiG8g3PMbHNsKpWuCjy5Jdqm0pa+wq9opo3ncTxwM9vldW/uUUkoppdQlOZqQzEu/bGH13jgA7DbhwzsbEuTng4jQtGIpHuxQNWfCBzDvFZj7AhkJX8kK4FvM2j4VDV/fBqdd3UUPZeraWa7h+RdGt9mhXANN+C6TJn2FnQhcPzxjd6B9PvuPxvD7tqNeDEoppZRSShVUKekOlu2K5ZFJ62nzzgK+W30g49gjN1SlcYXQi19k42RY8bF7P6oFDF0Ad00Cu59VFrcHvrkdzsbBoUzr82Xv2qmumnbvLApq9YDQyhC/lxA5wx32Jfz3jwi61imrM3kqpZRSShVBxhhmbz7KjqOn8LXb8LXbSE13smZfHGv2xZGS7sxxTptqYQzvXP3iFz/yF8x6wr1fo7s1u6ZvABTvCHd8Bd/fbS2pcHwrTOqb9fzyza/uxakcNOkrCmx2aPVwxrp999tnM+lIFxbvjKFjrQgvB6eUUkoppTztq+X7eG3WtotXBFpWLsWQ6ytxY50y+NhtcOowOFIhtFLOyknxMHUgpLsWXw+vCX3GZe2uWfsW6PUpTH/A2o/OtqxM9pk71VXTpK+oaDwAFr4JySepaDvOjba1zNhUXpM+pZRSSqkiZsOBeP4ze/sF61QOL0a76uH0b1GB2uVKuA9smQbTH7SSvvp3Qre3oFi4dSw5AaYNhZP7rX2/YKs7p38u4/0a3gUpp2H2M1nLS0RCycireHUqN5r0FRV+xaDZvbDsQwCG+szmnm3Xk5LuwN/H7uXglFJKKaWUJ5w8m8qj320g3TWpX62ywXSpXYY0pxOn01CjTDBtqoVzXUhgzpN3/g4/DQOnNYsnm7+3FkHv+AIc3w6bpkDaGXf92z+D8At0B20x1EoUF7zuLiuvrXz5QZO+oqTlA5gVnyDONJrZdlI1ZQdLdzaiS50y3o5MKaWUUkrlM2OspReiTyYBEBzgw7i7m1G+VNDFT963HL4f5E74zkmKy9laB9D2Sah968Wv2+5pSDkFy/9r7dfpdfFz1GXTpK8oCS6L1O8Lm74D4Fb7SmZv7qBJn1JKKaVUIWeM4b9/7GL+9uMZZe/3bZh7wudIs1rtjm09dzZsmOQepxdSATq+BAvegIQDWc+NqAOtH4FGAy4tMBHo8ipU6QhOB1TrfPkvTl2UJn1FTf0+GUnfTfY1fLTtqHbxVEoppZQqxFLTnbz88xamrj2YUXZ/28rcVLdszsqH1sGM4dasmrkpXhbu/gVKVbFmiF/0FmydDpFNoMUDUKmtlchdDhGo2vHyzlGXRZO+oqZSe4x/CSTlFFESS4WUf1i2qzGda2trn1JKKaVUYRObmMJD365jzb74jLJWVUrx7+61slZMSbRa7laPJWMx9ewCQ2HQdCvhA/AvDje9aT1UgaZJX1Hj44fU6GYNvAW62tfw6+Y2mvQppZRSShUixhjmbj3KazO3cTghOaO8d5NI3updH1+7zV05fj981w9iMs3o6RsELR+EYqWtfZsP1LgJQit66BWovKRJX1FU+5aMpO8m21r6bjumXTyVUkoppQqJLdEJvD5rG6v3xmWUicDz3WsxtF0VJHP3y4NrYEp/OBPjLqvaGW75MPd1+NQ1SZO+oqhaF4xPAJKeTC3bQUqlHGL57lg61dLWPqWUUkqpa9lni/7h3bk7MJl6aIYE+fJB34Y5e3ZtnW6tuXdugha7H/T4ABoPuvxxeapAs128iip0/IohVTtl7N5kW8Osv454MSCllFJKKXW1lu2K5Z057oTPxybc06YSi565IWvCZwwseR9+GOJO+AJLwd0zoMndmvAVQpr0FVW1bsnYvMm+lt82HyX+TKoXA1JKKaWUUlcq7kwqT32/EYBAkmlSviRzn2zPyFvrEhLk566YngI/P5x1QfSw6jD0D6jY2sNRK0/R7p1FVc3uGLEjxkFT2y6KJ8cyceU+nuhSw9uRKaWUUkqpy2CM4cUf19LmzHwG+82lkW0PKcU64h8yJWvFs3EwdSDsX+4uq9QO+n1jzcypCi1t6SuqgkohFa/P2O1qX8fEFfs4m5ruxaCUUkoppdRlcTrZMHkUr+3px2i/z2hk2wOA/76F8Nuz7npJ8TDx1qwJX+OBMPAnTfiKAE36irLat2Zs3mpfSfzZVH5Ye8iLASmllFJKqYuJO5PKpNX7eeaHTbzy3ns02fkRpeVUzorrv4Z1E601+Cb1hWNb3Me6jIKe/wc+fjnPU4WOdu8symrdAr/9GzC0sm3nJtsaxi0NYkDLCvjY9fcApZRSSqmC5EhCEuOW7GXynwdISnMAMNJnXcY3+hgJI6TDQ/jGboct06zC2c9ayV/0WveFeo2FRv09HL3yJk36irKSkdB0CKz7CoBXfSfSJb4ev24+wm2NIr0bm1JKKaVUEbcnJpHtR06z89hpth85xcK/j5PmMFnqNLHtytg2t36Mb5NbIPUsxPxttew5UrImfDe/rwlfEaRJX1HXZSTs+BXOHKesxPO0zw+MXVyWng2vy7pwp1JKKaWU8pjXZm5jwvK9Gfv+pBJIKmkUB6BW2WD6NixF/SUHwJUHRtRuY234BcGdX8MXHSElwX3Rzq9Ai6GeegmqANE+fEVdYCh0eytjd7D9d/yOrmfBjuNeDEoppZRSquhauy8uS8JXjhMs93+Mdf4PMbTsLiYMacZvj7fjvsoJ2IxrEr7StbJOyBJWFfqMA5urjafNE9D2KQ++ClWQaNKnoF4fqNYFAJsY3vL9kg9+24rDaS5yolJKKaWUykvpDicv/eyecKVGmeJ8UGk14XIKX3HwYsh8OtUqY/XIOvSn+8So5jkvVuMmeHgVDF0AN76qi64XYZr0Kes/gB4fYHwCAahj28/guP8ybd1BLwemlFJFh4h0E5G/RWS3iIw4T507RWSbiGwVke88HaNSKv99vXI/O46eBiDA18aEu5tw/ZkF7gr7l8OZE9b2wUxJX/kWuV8wvDpENs2naNW1QpM+ZQmthHR8IWO3n88iEn97lWTXzFBKKaXyj4jYgTFAd6AO0F9E6mSrUx14HmhjjKkLPOHxQJVSecoYw4ETZzmVnAbA8VPJfDhvZ8bx4Z2qE5WwHk4fznSSE3bOAWOyJn1R50n6lEInclGZXT+ctKNb8d08BYB7nT+y+PtqdBjwvJcDU0qpQq8FsNsYswdARKYAtwHbMtUZCowxxsQDGGN08LVS17hXftnKN6v2IwK1ypbAJpCYYo3Rq1K6GEPbVYFZH+Y8ccevULE1nI219gNKQngND0aurjXa0qfcRPDt9X9Eh7fNKGq36x2SZzwDu+db0/8qpZTKD5FA5j71h1xlmdUAaojIchFZJSLdPBadUirPzdx0mG9W7QesRrvtR07xz+EYShMPwGs96+HnTIZtM3Ke/M8C2LPIvR/VHGz6tV6dn94dKiu7LxH3TWGHrToANgwB68fBt33gnUrw432QnurdGJVSqmjyAaoDNwD9gXEiEpJbRREZJiJrRWRtTEyMB0NUSl2KwyeTeHH65ixlpTnJKv9HWeH/GB9GLaFt9XD4ezakWuP7CKtmzdAJkJ4ES0e7T9auneoiNOlTOfgGBhPd42u2O8tnPeBIgS0/wsZvvROYUkoVXtFA5v90o1xlmR0CZhhj0owxe4GdWElgDsaYL4wxzYwxzUqXLp0vASulLs28bcfoOnoxwydvYEt0Ak6n4anvN3Iq2erGWb5UIKue78zk6w8TImfwFQe9Y8fCmi/hr+/dF2rQD2rd4t5POODeLp/LzJ1KZeLRpE9EAkTkTxHZ5Jp57NVc6viLyFTX7GWrRaSSJ2NUlk5NavNh5S8YkvocX6Z3Z58p5z64YZL3AlNKqcJpDVBdRCqLiB9wF5C9T9fPWK18iEg4VnfPPZ4MUil1eVLSHTz/01/sPJbIzE2H6fPJAja+2YF3Dt1NXdmLTWD0nY0oWzKAas69WU/+9WnYPc+93+BOqNUjl2cRiGyWr69DXfs83dKXAnQyxjQEGgHdRKRVtjr3AfHGmGrAaOAdD8eoABHh/X7N2Rt6Pa+nD6JXyijSzs37E70WYv72boBKKVWIGGPSgUeBucB24HtjzFYReU1EerqqzQVOiMg2YCHwrDHmhHciVkpdipmbjhCb6B4W0932J00cm6hoO86jPj/zSMdqNKtUyjp49K9sZxtrpk6ACq0htBJc1xhKZBvuG1EHAkrk22tQhYNHkz5jSXTt+roe2VcAvw2Y6Nr+EegsoitJekPJIF8+H9SUQF87JwlmnqOJ++BGbe1TSqm8ZIyZbYypYYypaox501X2ijFmhmvbGGOeMsbUMcbUN8ZM8W7ESqkLMcbw1XJ3612NMsWpbXPP19TCdw+PdXb10E5PheM73Cdnn4mzwZ3WX5GcrX3atVNdAo+P6RMRu4hsBI4D84wxq7NVyZjBzPXLZwIQ5tko1Tm1ypbgnTsaAPCDo0NGudk0BRzp3gpLKaWUUqpAW7c/nq2HTwHg72Nj6rDWDKqWnHE8zHkC38Qj1k7s3+C01uojpCIMnmm17AEEhUHd290Xzp706SQu6hJ4POkzxjiMMY2wBqm3EJF6V3IdnZnMc3o2vI5h7auwxNmA48aaKE4Sj5G+a76XI1NKKaWUKpi+Wr4vY/v2xpGEFvMjKGF31krRa62/RzPN5Fm2PgSXhaEL4ZbRMGQ2BIa6j1dsAwGZJu6tkH2klFI5eW32TmPMSawxCdnXGcqYwUxEfICSQI4xCzozmWeN6FaLfi0r85PDvYbfxpljSEl3QPw++P1lWDvBewEqpZRSShUQh08mMWfr0Yz9IW0qQVoSnNyftWL0OutvlqTP6mFFUClodi9E1Mp6jt0Xur0NweWg1SMQVjXvX4AqdHw8+WQiUhpIM8acFJFA4EZyTtQyAxgMrATuABYYY7KP+1MeZrMJb/aqx6ep/WH7LADqJ65gwScP0y1xOuJIsSoWLwu1bvZipEoppZRS3vXtqv04nNbX19ZVwqhVtoSV2J2bmOWcQ7klffUv/gSN+lsPpS6Rp1v6ygELReQvrOmp5xljZmWbnexLIExEdgNPASM8HKM6DxHh4Tt7cLhYHQD8JZ3uCVPcCR/An597KTqllFJKKe9LSnUw+U/3GnpD2lSyNnKb+fzwBmuOhMwzd15K0qfUZfJoS58x5i+gcS7lr2TaTgb6ejIudelEhHId7oPZT+deYc8iOPGPdjVQSimlVJGT5nAyfPJ64s9ak7JEhQbSpXYZ62BuSV/aGWstvuQEaz8gBEpGeShaVZR4bUyfunZJ/TswQeEAnDDB/DttKPMdmXJ5HdunlFJKqSLG6TQ888Mm5m8/nlH2WKfq2G2ulcdiMyV9Nl/39tqv3Ntl61vLMiiVxzTpU5cvMAQZugBH7wm8Xvlbpjo68o2ja8Zh54ZvrcHKSimllFJFgDGGl3/Zwi8bD2eUPXxDVe5sXt5dKXNLX42b3Nu757m3z03iolQe06RPXZnQitgb9OHtAe24vmoYS5z12e+MAMCWfJKNc75C599RSimlVGHhdBp2HjtNcpojx7EPft/JpNXucXyDWlXk2Ztquis40qzhL+c0+pd7O/PkLuU06VP5Q5M+dVUCfO2MH9yMfs0r8p2jc0a5WfMlb/+2w4uRKaWUUkrlnXfm7KDr6CXc8N4idh47nVH+84Zo/m+he/292xtH8mq7YsjhDe6T4/a6F18vEQmV2gG5dOPUSVxUPtGkT121ID8f3u7TgBv6PUGqa26gxrbdLFu6gN82H/FydEoppZRSV+f46WQmLN8LwNFTydz1xSq2HT7FhgPxPDfNPfPmDTVL825bwfZ5OxjXEf4cZx3IPJ4vvAYElIDS2dff87OOKZUPNOlTeaZ1g1pI3V4Z+9P8RlL1x66c/XYArP8anM4LnK2UUkopVTBNXn2QNId72ErcmVT6j1vF0K/XkZpufb+pHlGcT/o3xnfha5Dqaglc+oG1JENMpt5P55K9qKZZnySitrXwulL5QJM+lad8Ww7N2A6QNGrIAYJ2z4IZw+Gb2+CUtvwppZRS6tqRmu5k0ur9GfvnZuNMSEojNtFaqzgkyJfxg5sRfGwN7J7vPvn0EWs/Zqe7rLSrNS8yW9KnXTtVPtKkT+Wt8i2h3TOkBYTlPLZ3CXx2PeyY7fm4lFJKKaWuwJytRzl+2kruSgf78+ODrQkJcrfI+diEzwY0pWKpIPjjtZwXWD8x95a+yGZZ6+nMnSofadKn8pYIdH4Z3xF7+O6GJdye8iqfpd+K07gGKyfFwZT+1n+KOrunUkoppQq4iSv2ZWwPbFmRxhVCmTKsFZEhgfjahbd616d11TDY/QccWGlVtPm4L7BzbtblGsJds3pG1AGfQHe5tvSpfORz8SpKXZn+HRqw5GA672ytzmJnQ0b7fko5ibMOLv0AUs9Ct7d0EVKllFJKFUibDyWwbn88AL52oX9La929WmVLsPCZG0hzOCnm72P9kL0gUytf0yFwfAfsXwbGAQ7XMg9B4VDM1RvK7gN1b4dN30HJ8nBdEw++MlXUaEufyjciwof9GtK6ShirnHXonvIWCx0N3RVWfwazntQJXpRSSilVIP0vUytfj/rliAgOyNj387FZCR/A9hlwZJO17RMA7Z6BJnfnvGDpmln3bxkNA3+CYYvANyBnfaXyiCZ9Kl8F+fkwYUhz2lUP5yTBDEt7mtmOFu4K676CGY9q4qeUUkqpAuX4qWRm/nU4Y39Im8q5V0yMgTnPu/dbDIMS5aBOTwgombVu9iUZfAOgWmcoFp5HUSuVO036VL4L9LMz7u5mdK4VQRo+DE8bznRHG3eFjZOs5E8ppZRSqgBwOA1Pfr8xYzmGhuVDaFQ+JJeK6fDjPXAq2toPCIE2T1jbvoHQoF/W+tnX5lPKQzTpUx4R4Gvns4FNubNZFA7sPJ32ED+kt884nv77SDh91IsRKqWUUkpZPlu0m+W7TwDW1AP/vqlm7hUXvAb7lrp2BPqMd4/Zg5xdPEvr4uvKOzTpUx7j52Pj3TsaMuPRNrSuVpqX0u9lj7MsAD5pp4n94UkvR6iUUkqpou7PvXF8OM+9rt6jHatxfbVcul9u+wWW/9e9f8PzUP3GrHXK1ocKra1t32JQrlE+RKzUxWnSpzyuQVQIk+5vxbh72/Kuz4MZ5eEHZvPXgilejEwppZRSRVncmVQem7wBp2tVqRaVSvF45+o5K8bvh58fdu9XvwnaP5v7Re/4Cjq+BIN+gqBSeR+0UpdAkz7lNe1rlOa5h4byq61jRlnY4hdZsmWvF6NSSimlVFFz8mwq/52/i84fLOLoqWQAQoN8+W//RvjYc/m6PPcFSE20tkMrQe/PwXaer9UlykGHZ6FCq/wJXqlLoEmf8qoqpYvT8N5PiKcEAJESS+xPz5KUku7lyJRSyrNEpJuI/C0iu0VkRC7Hh4hIjIhsdD3u90acShUmxhjGLNxNm7cXMHr+TuLPpmUc++DOhpQrGZjzpH8WwI5Z7v3e4yAw1APRKnXlNOlTXhcVVR5uejNjv7dzHpumvurFiJRSyrNExA6MAboDdYD+IlInl6pTjTGNXI/xHg1SqULo543RvDf3b86kWounB5DC/cGr+N8tJehUq0zOExxp8Fum32Qa/gvKt8hZT6kCRpM+VSCEthrEvrLdMvZb7fmYs2sneTEipZTyqBbAbmPMHmNMKjAFuM3LMSlVqCWnOXhvzt8Z+9UjivNHlSm8lPYxNyzqC8e25jzpzy8g1nWOXzB0GemhaJW6Opr0qYJBhMh7/scGW72MIv9fh1tdKJRSqvCLBA5m2j/kKsuuj4j8JSI/ikh5z4SmVOH05bK9HE6wxu+FF/dj+l0RRB6eax1MT4KfH7Ja9s5JPA6L3nbvd3gOgst6MGKlrpwmfarA8PUP5HC38Wx3Wt9j7MaBc8pAOPGPlyNTSqnzE5HvRKSdB55qJlDJGNMAmAdMvEBMw0RkrYisjYmJ8UBoSl1bYhNT+GyR+/vFE11qUHzT/7JWOrIJlo22tlNOw/QHIOWUtR9WHVo+iFLXCk36VIHSvVkt3gh5nWhjLWxqSzsDMx8HY7wcmVJKnVcrYJGIbBWRx0Qk5AquEQ1kbrmLcpVlMMacMMakuHbHA03PdzFjzBfGmGbGmGalS5e+gnCUKtw+mr+TRNekcVVLF+OuBiGwIZdhJYvfgb/nwITuWXsfdXsbfPw8FK1SV0+TPlWg2GzCfTdfzwOpT5JuXLfnvqWw/rw/aCullFcZY6oANwN/A+8D0SLylYhczvzsa4DqIlJZRPyAu4AZmSuISLlMuz2B7VcXuVJF0+7jp5n8p7s39Qs318Zn81RIPW0VhNeAKNfkLM50mNwPjm12X6D9s1C9iwcjVurqadKnCpyONSMIrNiUcY4eGWWOOS/BqcNejEoppc7PGDPXGNMbqAC8DXQElovIBhF5UESKX+T8dOBRYC5WMve9MWariLwmIj1d1R5ztSZuAh4DhuTX61GqsDLG8Pqs7Thcq69fXzWMTjXDrQlazmkxDHp9Cj4BWU+2+cBtY6DTSx6MWKm8oUmfKnBEhLd6N+Brv7vY47QGSNvTTpP4k3bzVEoVbMaYo8aY14HrgaVAQ+BT4LCIvCcixS5w7mxjTA1jTFVjzJuusleMMTNc288bY+oaYxoaYzoaY3Z44CUpVajM3XqUxTutca4iViuf7FkIJ3ZZFfxLQMP+EF4dOr3sPjGgJAyaDo0HeiFqpa6eJn2qQKoWUZxx97XjdZt7kHTxfb8Ts3qKF6NSSqkLE5FOIvI9sBeoD4zGSgA/AR4EvvZieEoVaWdS0nl15raM/X+1qEC9yJKwOlMrX+OB4O9qmG/1sJX4NewP9y+Ayu09HLFSecfH2wEodT71Ikvy6D1DmDphGf1kPgDpc0eS1Kg3gQH+Xo5OKaUsIhIG3AMMA6oC67ESvMnGmGRXtVUishn40jtRKqU+mr+TI5mWaHjuplqw7f/Zu+/oqKrtgePfPTMpkEYJJfQuvQkIqBRRmiCgKEVEbIiCij79+fD57D6779l7QwEFFAFpAgqIIkU6hN5b6CQhfeb8/rhDJoEECCZzJ8n+rJWVe+4592aPa8nMnnvO2VNh68/eEQKt7/Zd4HBAh0f9H6hSBUCf9KmAdnn10tQc+DonjPWtW4yJY+r4922OSimlstkPPAf8DrQ1xrQ2xnyRJeE7YxNw2O/RKaXYdCiez3/fldl+okd9ov56ByYOBbxLR+p1g7K1bYlPqYKmSZ8KeG0a1GRfXd8c+sa7vmTGGt3URSkVMJ4AKhtj7jDGLM9tkDFmtTGmph/jUqrYc3sMf+44xmOT1mZu3nJlzQj67X0J5j/rG1imNvR83aYolSp4Or1TFQqN+z5K2htfEGxSaezYxdtTxtG06kiqlilpd2hKqWLOGPOm3TEopbI7nJDCe79sY+b6QxxJSM0873II70X/gKzOUpOvxtVwy1goWcaGSJXyD33SpwoFCS+Xbcesoe4pPPjtKjLcHhujUkopEJH/isjXufR9LSKv+TsmpYqzg6eS6f/BEr5asjtbwgfwz2uqUCo2y6ZwzW+FIT9owqeKPL8mfSJSVUR+FZGN3lpDvv53lwAAIABJREFUD+UwppOInBKR1d6fp/wZowpcwVc/iBEnAFc5N5C+dyVf/rHL3qCUUsoqlP5zLn1zgL5+jEWpYi0uPoVBH//JnuNJmeeiw0MY0rYak0a04+7oDZCRbHWUb2jV3XMF2xStUv7j7+mdGcA/jDErRSQC+EtE5hpjNp417jdjTC8/x6YCXekaSKN+sH4yACNcP/F/c+vSs0kMlUqVsDk4pVQxVhnYk0vfPm+/UqqAHU6wEr5dx6yEL8gpvH5zM3o1rYTTIdagsd/5Lmg6wCrWp1Qx4NcnfcaYg8aYld7jBCAWfTNUeXHlg5mHPRxLqZG+nWenb7AxIKWU4gRQJ5e+OkCiH2NRqtg5fjqNTxbtoN97f7Dj6GnAWrv3zqCW9Gle2ZfwxR+EnQu9Vwk0udmegJWygW1r+kSkBtACWJpDdzsRWSMis0SkkV8DU4EtphnU7gKAUwzvBb3FHxt2Mm9jnM2BKaWKsXnAkyJSIetJb/sJYK4tUSlVxO07kcTob1fR9j/zeXFmLPtPWtM2nQ7h7UEt6N64YvYL1k0C490LoObVEKXPHVTxYUvSJyLhwPfAaGNM/FndK4HqxphmwDvAj7ncY7iIrBCRFUeOHCnYgFVg6fEKBFt1+2o64ngl6GOenrqepLQMmwNTShVT/wbCga0iMl5EXhWRccAWIAx40tbolCqClu44xg3v/s6Pqw+QlmVTt4hQF+8MakHPJjHnXrR2ou+46QA/RKlU4PB70iciQVgJ3zhjzA9n9xtj4o0xid7jmUCQiETnMO5jY0wrY0yrcuXKFXjcKoBE14Ub3s5s9nQuo2viFP47d4uNQSmliitjzC6gNdaXlJ2B0d7fU4A2xpid9kWnVNHz7bI93PrpUo6fTss816xKFK/c1ISlT3TJOeGL2wBx66xjVyg0uMFP0SoVGPy6kYuICPAZEJtbXSMRqQjEGWOMiLTBSkyP+TFMVRg0vgl2L4HlnwDwhGs8A36vze+XlefKOud8R6CUUgXKm/gNtTsOpYoyt8fwwoyNfPH7rsxz0eHBvDu4JW1rlT3/xWuzbOByWU8IjSyYIJUKUP5+0nclcBtwTZaSDD1FZISIjPCO6Q+sF5E1wNvAQGOM8XOcqjDo9iKmUksAgsTN20Hv8u9vf+dYYuoFLlRKKaVUYZKS7mbkuJXZEr6GMZFMHXXVhRM+jxvWTvK1dWqnKob8+qTPGLMYOO/euMaYd4F3/RORKtRcIcjNX+L58GocqaeoIkd5JPU9Hp8cwye3t0Z0G2allJ+ISHlgEHAZEHpWtzHG3OX/qJQqGk4lp3PP2BUs23k881z3RhV5c0AzSgZfxEfZHQsg4YB1XLIs1OlSMIEqFcDyJekTkbLGGJ2CqfyvdHUcfd6Bidasql7OpSzaOpGv/yzP0HY17I1NKVUsiMhlwBKs99Qw4ChQBnBilXM4ZV90ShVucfEpDP1sGZvjEjLP3XllTZ68vgEOx0V+ubvsY99xk1vAGZTPUSoV+PI0vVNE7hGRx7K0m4jIPuCwdyfNiue5XKmC0bAPXH5HZvMZ11gmzJjH+v36OUsp5RevAcuBClizWXoAJYC7gSSgn32hKVV4uT2G+775K1vCN6ZHff7dKw8J3/EdsGWOr9367nyOUqnCIa9r+h4AkrO03wROYu1UFgU8l09xKZU33f6DJ/oyAEpKKm843mbU2D91fZ9Syh9aA+8DZ/7BcRhjMowxn2MtV/ifbZEpVYh98ftOVu45CVi19968pRn3dqydt+Ubyz4FvFtD1LkOouvkf6BKFQJ5TfqqA5sARCQK6Aj8nzHmHeBpoFv+hqfURQouiePmL/A4QwBo6NjNwNNfM3L8StKz1O9RSqkCEA4cN8Z4sKZyZt1CeDlWUqiUyoOdR0/z2pzNme0HrqnDjS2rQNxG2PAjuNMvfJPURFj1ja99xb0FEKlShUNekz4HcOYT9FVYX50s8Lb3AuXzJyylLkGFRji6vpDZHO78iYydf/CfmbE2BqWUKgZ2AWeWN2wGbs7S1wtrRoxS6iJ5PIbHJ68lNcP6yFm/YgT3d6oDJ/fAp9fCpNvh636QnnL+G639DlK9Sz3K1IbauoGLKr7ymvRtBa73Hg8E/jDGJHnblYDjOV6llL+0vhtqdQbAIYY3gj7ku9838d3yPTYHppQqwuYC13mP3wTuEJHNIrIBeAj43LbIlCqExi7ZxbJd1kdKp0N4/eZmBLscsGEKpJ+2Bu36DSYNy/7ELzUBjm0HdwYYk30DlzbDweHvSmVKBY687t75OvC1iNwOlCb7t5mdgbX5FZhSl8ThgD7vYd5vi6TGU91xmCdc4/jnD6GEuJz0bVHZ7giVUkXPGCAEwBgzUUSSgQFASeAt4BMbY1OqUJm7MY6XZ2/KbN/XsTaNK0dZjS0/Zx+8ZRZMGQHtH4Dln8C6yZCRAq4SUK4eHPHeJzgcmg/20ytQKjDlKekzxowXkT3AFcByY8yiLN1xwLT8DE6pSxJVGen5Gkyx5u4Pcc0nlWB+mLSOIHML17esZXOASqmiQkScQH3gwJlzxpjpwHTbglKqEDLG8P6C7bz+82aMd9+VehXCeaCLd+OV5JOwZ8m5F66fbP1klZEMB9f42s0HQ2hkwQSuVCGR5+fcxpjFxpg3zkr4MMY8bYyZmX+hKfU3NB0ADXpnNu9yzWJs8EtcO7UNO7950Jr6oZRSf58BVgAt8uNmItLdOzV0m4j88zzjbhIRIyKt8uPvKmWnlHQ3D3+3mtfm+BK+KqVL8MGQywlxOa0T238B47aOK7WAVnflfLPQUtnbrlBooxu4KJWnJ30i0h4oY4z5ydsui7UddWNgDvC4MWf+j1TKRiLQ639wdKtvegcQIunU3PYV636sRJObcv08pZRSF8UY4xGRvVhF2f8W71PD97DWB+4DlovINGPMxrPGRWCtFVz6d/+mUnZLSXdzz9gV/Lb1aOa5NjXL8MGtLSkbHuIbuDXL1M663aDj45CeDGvGg8Nl1extMxyqXgGJcbB/JRzdAtXba5kGpcj7k76XgcuztF8DegJbgPuAJ/IpLqX+vrBoGL4ABk4gqcVd7JVKmV3V177Fj4tX2RaaUqpI+QgYLSLBf/M+bYBtxpgdxpg04FugTw7jngdeAS6wdaFSgS3d7WHU+JXZEr6BravyzV1XZE/4PO7sSV+9btYa/r7vw4jf4ZFN0P9zqNbW+tI3oiLU7wlXjYaqbfz4ipQKXHlN+hpgTWNBRIKA/sDDxpibgH8BukpWBZagElC/JyX7vEnIg0vZ67A2comUJFJmP8Pni3de3H0y0sicc6KUUtlFALWBHSLyqYg8LyLPZfl59iLvUxmr/NEZ+7znMolIS6CqMWZGvkSulE0y3B5Gf7uaebGHM8891KUuL93YxNqpM6v9KyHpmHUcVh5imlvHIlCxMYSX81PUShVeed29MxyI9x63wZrO8pO3vRKolk9xKZXvypeOJKHf6/D9IABucS6k74zpOKQ3w66smfuF6ybD1JHWm8ywn8AZ5KeIlVKFRNZZLnfm0G+Ap//uHxERB1ZJiGEXMXY4MBygWjV9a1aBJS4+heemb2TGuoOZ50Z0rM3oa+siIudesHWO77huVy29oNQlyOv/NfuBZt7jHsB6Y8yZr2hKA0k5XqVUgIho0pP0Ot0Aq47fc0Ff8uKMDazem0vtZGNg/nPWFtB7/7QWkiulVBbGGMcFfpwXeav9QNUs7Srec2dEYK2hXyAiu4C2wLScNnMxxnxsjGlljGlVrpw+BVGB4dCpFJ6eup6rX/01W8I3rH0NHu9+Wc4JH8CWLElfva4FHKVSRVNek74JwH9EZDLwCPBNlr6WWMXblQpoQT1ewjitpTfNHdsZIPN4cMIqElLSzx18aC2c3O1rH9B1gEqpArMcqCsiNb3rAweSpRSSMeaUMSbaGFPDGFMD+BO4wRizwp5wlbp4v2yKo8Nrv/LVkt2kZXgyzw9qU42nejXMPeGLP2i9FwM4gqBWZz9Eq1TRk9ek7xmsxeMhWJu6/DdLXzNgUv6EpVQBKlsbaf9AZvM515d0ODWVf01Zjzl73d7Gs0pPatKnlCogxpgMYBTWbtixwERjzAbvusAb7I1OqUuXku7mn9+vy5bsDa6wh7lt1/Cfa6NxOHJJ+CD7Bi7V22u9PaUuUV6Ls7uBF3Pp65svESnlD1f/AzbPgsMbcYjhhaAveH19IpNWPMUtrbOsf4k9K+nbv9Ka8pnbN5JKqWJHRDxY6/ZydbFTPL31bmeede6pXMZ2usgQlbLVhGV7OJyQCkB0eDDvdS9Fm5m3I6vTIfZD6PoctBh67lq9w5tg0eu+dr1ufoxaqaIlrxu5ACAijYGOQBngOLDAGLMhPwNTqkAFh8GwGTCuP+z/C4BHgybxxfQkdtf8mOrR4dabzdEt2a87fRjiD0BU5RxuqpQqpp7j3KSvLNAVa2bMl/4OSKlAkZLu5v0F2zPbIzvX4YqUqeDxLqlIPQXTH4K1E+G656xN05wuaw39xNsh1bt/oCsUGugDb6UuVV6Ls7uw3rwGAVkfdRgRGQ8M0+LsqtAoWQaGTsM9YTDOXQsBuMMxg3fHvc3IB8cgZz/lO+PAKk36lFKZjDHP5HTeW2x9OnDKrwEpFUDGLd3DEe9TvgqRIQxqUw2+nnPuwN2/w6ddwFXCKsOwfyWc+UgZFGbV4StV9dzrlFIXJa9r+p4GbgGeAmoCJby/nwIGeH8rVXiEhOMcMolT1X1TRgYff49pf6zNvp6vVJYpn7quTyl1Ebxfgr4PjLY7FqXskJzm5oMsT/nu71SH0Ix42LvUN+iKESBZZj9nJMO+5b6EL7Iy3DkbLuvup6iVKprymvQNAV4wxrxojNltjEn1/n4ReAEYmv8hKlXAXCFEDf6MU8EVACgjiVSaOwLi1ln9zhBrDeAZB1baEKRSqpAKwVoKoVSxM27pbo4mWk/5KkaGMqB1Vdg235fQVWoJPV6B4QugcX+IOutJXkxzuHs+xDT1a9xKFUV5XdNXCfgjl74/gH/9vXCUsklIBCH93oHvbgGgNRt9fbWvgZodfO0Dq3QzF6VUJhHJqfp5MFZNvZcBLamgip3kNDcfLszylK9zbUKDnNl346znfXoX0xT6f2YdJ8TB/hWQlgQNekFQCT9GrVTRldek7wBwJTAvh7723n6lCqXQBt2Iq9mPCjunZO9oeAOUrgmhUZByCpJPWLX7StewJU6lVMDZRc67dwqwHRjp12iUCgA/rNrH0cQ0gsggOircesrnccPWub5BORVaj6gA9a/3X6BKFRN5TfrGAf/ybk89DjgIVMQqIPsvrBp+ShVaFW5+k4Q3FhDhPgFAunGyJeJKGolApRawY4E18MAqTfqUUmfcyblJXwqwG1iuG5yp4sYYw6qFU5kT/BE1JI4V1f9BiKsL7F0GycetQeEVoGIzewNVqhjJa9L3DFALeNZ7fIYA47G2rVaq8CpZBsf1r8G0uwFY4GnOmAlb+P6+aKqfnfQ16mdfnEqpgGGM+dLuGJQKGEnHiZv8GK8nTc7cOaL9lldgWxvYnWWFUN3rzq3Lp5QqMHktzp4BDBaRF4EO+Or0LQJigJWArrZVhVpYy5s5lJTM3HmzeSu9F0fT07j982VM79KEiDOD9utmLkopi4jUA2KMMQtz6OsAHDTGbPV/ZEr52b6/YMJAKp4+nO20YOD7u6xlEmfU1ULrSvnTJRVn9xZiz1aMXUTqA43yIyil7FbxqqE0qNKLhE+XQoaHXceSuG12Oj96+937V+P0ePRbSqUUwP+AjcA5SR/QC2jo/a1U0bbwFciS8M12t+bayL24Th+y1sMnW0sncARB7c42BalU8aSfWJXKRasaZXh7UAsc3k06V8eHc9REAuBMT+DrGb/YGJ1SKoC0wprxkpNFQGs/xqKUfQ6uyTx8KO1+Jtd5CdfAr60kL6vq7SEkAqWU/2jSp9R5dGtUkVf7NyMs2AkIaz21MvtWLf2V7UcS7QtOKRUoIrA2bslJOhCVS59SRcfpo5B4CIBkE8x0T3vuuLImVG0D3V/KPraeTu1Uyt806VPqAvpfXoUVT17H9/e1o3SdNpnnG7GdZ6ZtwJicdmpXShUjO4AuufRdg1XSQakiyxjDpjVLMtubTRXqVoiife2y1onWd0OL26zjEqWh0Y02RKlU8XbBNX0iUutCY7wq/s1YlApYJYKdXF69DLS9BnZ+DEBv5xLe3NqfWeur0bNJjM0RKqVsNBZ4XkT2AJ8aY1JFJAS4GxhN9t2ulSoyjDHMWHeQ93/dTrvDs/m3dxbnJk817riyBiLe9REicMM70PxWKFUVIvU9Uyl/u5iNXLaRc9HZs8lFjlOq8KrZESJiIOEg5eUkI11Tef6n0nSsV46wkEvaF0kpVfi9jrVu7x3gLRE5jrW7tQP4Hq1hq4qgY4mpPDFlHXM2xAFwZ9CezD5Xpab0ubxK9gtEoHo7f4aolMriYj6l3pFff0xEqmJ9I1oBK0H82Bjz1lljBHgL6AkkAcOMMbo/vgoMwSXh2mdgyr0A3OWcybfxnXn312083r2+raEppezhLb7eX0SuAa4DygJHgZ+NMQvsjE2pgvDzhkM8MWUdRxPTMs81cPiSvv49u4FTVxApFUgumPQZY77Kx7+XAfzDGLNSRCKAv0RkrjFmY5YxPYC63p8rgA+8v5UKDE1ugWWfwP4VhEgG/3KNY9RvFbmmfnla1yhjd3RKKZsYY34BdFtfVaR98ftOnp2+Mdu529pUouGGA+D2nijf0P+BKaXOy69fwxhjDp55ameMSQBigcpnDesDjDWWP4FSIqKTv1XgcDigx6uZzW7OFbQxaxk+dgW7j522MTCllB1EpJeIjMqlb6SI9PR3TEoVhE2H4nlp5iaiOUUIaVSIDOHLO1rz/FUlELf3qV9kZSipX4AqFWhse/YuIjWAFsDSs7oqA3uztPdxbmKolL2qXA7NBmc23wl6hzfTX2D1h3eR/PtHkJZkY3BKKT/7NxCWS18Jb79ShVpahoeHv1vDjcxjacj9LCz5OHOHN6bTZeUhbr1vYIXG9gWplMqVLUmfiIRjLW4fbYyJv8R7DBeRFSKy4siRI/kboFIX49qnITgcgDKSSGfnGvqkz6LE3P/DM+0Bm4NTSvlRfSC3teergQZ+jEWpAvH2/K2cPLiTp1xf4xRDRU8ckZsmWJ3Zkr5G9gSolDovvyd9IhKElfCNM8b8kMOQ/UDVLO0q3nPZGGM+Nsa0Msa0KleuXMEEq9T5RFS0Cs46gs7pcqyfTMLmhTYEpZSygQMIz6UvAjj3HwmlCpFVe07w/oJtjAkaT0lJ9XWs+Q6MgbgNvnOa9CkVkPya9Hl35vwMiDXGvJnLsGnAULG0BU4ZYw76LUil8qLlUHh8J9z7GzMbvsYid5PMroPfPsSGfcdtDE4p5SdrgFtz6bsVWOvHWJTKV6kZbv4xaQ2tieUG55LsnUdi4dA6OJTlSV/FJiilAo+/n/RdCdwGXCMiq70/PUVkhIiM8I6ZCezAqg/4CXC/n2NUKm9CIiCmKT1uvoeNrV8k2QQDUM/s5NuPXmTKqn02B6iUKmBvADeKyCQR6SoiDUXkOhGZBPQDXrM5PqUu2Td/7mH3kXieCcqymbs4fcdLP4SEA9axMwTK1PZvgEqpi+LXatLGmMVYRdzPN8YAI/0TkVL5R0QYcUNHtqbcS92N7wAw2vEdnb+7ggMnW3J/p9pYD7uVUkWJMWaKiDwEvAjc6D0tQCLwYC5LGXIkIt2xatU6gU+NMS+f1T8C6z3S7b3/8LPKHimVbxJS0nnv120Mds731eELKmktbZj+kNVeM8F3Qfn64PTrR0ul1EXSyplK5bO6fZ8gPdzacLasJDDa9T2vzdnM8z/F4vEYm6NTShUEY8w7WDtNX481o6U7UAlYLyKfX8w9RMQJvIdVr7YhMEhEzi54Nt4Y08QY0xx4FchtqYRSf9sni3YQfPog/3BN8p28+hFoPgTCyltt4/H16c6dSgUsTfqUym/BJQnq8WJm807XbD4Leo3f/1jIIxNXk5bhOc/FSqnCyhiTYIyZDSwDrgLWYRVrv+Uib9EG2GaM2WGMSQO+xapdm/VvZN3xOgzQb5JUgTiSkMpXi7fwQfBblBJvDdrSNaDdA9bTvCb9z71Ikz6lApY+g1eqIDTsCzWuhl2/AdDFuYrOjtXM3XA5v2yP4vLKJSkXFgR1roXmg0GnfSpVqIlIFDAAuB1o6z29BngZmJDbdWfJqU7tFTn8rZHAI0AwcM0lhqzUOYwxmcsQ3vllK495vqCFa5vVJ06kz/sQFGoNbnoL/Pl+9hvozp1KBSxN+pQqCCJwy1iYPQaz9jsEg0MM3ZwrIB3Y5R234QfY9BP0fR9KlLYxYKVUXomIA2sa5+1AbyAUOIA1RXMkVi3aRfn9d40x7wHvichg4Env3z87tuHAcIBq1arldwiqiHF7DA9+u4q5G+IoFxFC9bIlqbJ7CkNc8zPHSNfnocaVvotimkP0ZXB0s++cPulTKmDp9E6lCkrJMnDjR8iIxZi63XIft3kmfNQRDqzyX2xKqb9FRN7AqiE7HegFTMFKAKsBT3GBTctycVF1arP4FuibU4fWslV58dvWI8xYe5A0t4f9J5M5tWMFzzk/y+w3jW6Etmdtpi5iPe07IyIGwsr6KWKlVF5p0qdUQavYGLl1ItzzC/FdXmVCxccYnXY/X2Z09Y05uRs+6wpb5tgXp1IqLx4GymOVGapmjLnVGPOzMcbDpa+zWw7UFZGaIhIMDMSqXZtJROpmaV4PbL3Ev6VUpulrfOWQa8pBvgx+hVBJByC5VD3khndyXobQbBAER1jHda71R6hKqUuk0zuV8pfKlxNZ+XIGXQ3Vtx9l1PhV/JnckFeDPiJSksGdBtMegFErIDTS7miVUuf3GXAzVuK1WUS+BcYaY5Zd6g2NMRkiMgqYg1Wy4XNjzAYReQ5YYYyZBowSkWuxJoqfIIepnUrlRUq6m583HAKgEkeZWep1SiRb+wVlBEdQYsgECAnP+eKoynDXz3BoLdTv5a+QlVKXQJ/0KWWD9rWjmTSiHWsjOtA77UXiTCmrIzEOFrx8/ouVUrYzxtwDVARuBVYA9wJLRCQWeJxLfNpnjJlpjKlnjKltjHnRe+4pb8KHMeYhY0wjY0xzY0xnY8yGfHlBqthauOUICakZRHOK70q8TIlk71O/oJK4hnwP0XXOf4MKDaHZwNwTQ6VUQNCkTymb1C4XzuT72hNUrg4vpg/JPO9Z+iHEaa1lpQKdMSbFGDPBGHNmLd8YrKLp/8Ra0/eyiAwRkVA741TqfKavOUAw6XwZ/ApVzQHrpDMYBo6DaudsHquUKqQ06VPKRpVKlWDSve3YUbEbf3oaAOAwbo5OfBCMlt9SqrAwxhw0xrxqjGmMVW/vPaAuMBY4eN6LlbJJUloG82MPc5dzFo0du6yT4oCbPoPaWg1EqaJEkz6lbFY6LJhv7m7Ll6VGkmGs/yWjjy1n4fcfcPBUss3RKaXyyhizwhjzAFAJuAlYYG9ESuVsXuxhItOPMMo1xXfyuueh4Q32BaWUKhC6kYtSAaBUyWBeHH4LU9+ey01pUwFotO4lHliZyJ7IVnSoF83D19WjfIgbts2DY9utHT9P7gFnCHR7EcrWtvlVKKWyMsakY5VymHKhsUrZYfqaA4wJGk+YpFonyjeEK0bYG5RSqkBo0qdUgCgbHkKH4W9w7L1FlDUniJZ4JgS/yGeJPfh8WXcabXqXwY6fcaScOPfiY1th+AIIifB32EoppQqhU8npJG75jb6uP3wne7wCTv1oqFRRpP9nKxVAykWXI2HAFyRPvp0SGacAuMs1i7tcsyDtPBce2wbTR8NNn+ZcS0kppVSxt2zncWavP8ThhBR2H4nnZccXvs6GfaFmB/uCU0oVKE36lAowEfU7w0PLYOoo2Db3nP5jQRUp07IvUrompJyCBf+xOtZPhppXw+XD/BuwUkqpgLdk+zGGfLYUt8faJGyIcy6NgnYDkOEIxdX1BTvDU0oVME36lApEERXh1knw1xcw51+QnsQaTy0+zujF7JTWdD1Wmc7lytO4XhT1T+7Fsfpr67pZj0PlVlCxsb3xK6WUChh7jydx/7i/MhO+KnKEf7omZPantxuNq1RVu8JTSvmBJn1KBSoRaHUnNOqHSTzM5MVpzFi6B4BZ6w8xa/0hAMoGX8essD8on7wdMlJg0jAYsRiCtDSYUkoVd0lpGdwzdgUnktIBKBcWxA+lxxN+NAUAU7YuJTqNtjNEpZQfaMkGpQJdidJIuct4pk9jejapeE73sTQXg06O4LQJ8Z7YCss/9XOQSimlAo0xhkcnrWHToQQAgp0OJrfZRPmjS60B4kD6fgBBJWyMUinlD5r0KVVIOB3Ce4NbMv7uK3i0az26N6pITJT1NG+7qcxrGQMyx7oXvgbJJ+0KVSmlVAD4bPFOZq47lNn+b9coqq942Teg/YNQtbUNkSml/E2ndypViIgI7etE075ONAAej2FubBxvzdvKuIPXcqdzFtUcR3CmnuTonFeJ7vsfmyNWSillh30nknjj5y2Z7TvaVeX6HWMg/bR1olx96DTGpuiUUv6mT/qUKsQcDqFbo4rMePAq3r61De/IoMy+sNWfsHnrZhujU0opZZdnp28kOd0NQP2KETwZ9iPsXmx1ihP6vq9rv5UqRjTpU6oIEBF6NIlh8J2j2WhqAlCCNNaNG0PswXibo1NKKeVP8zbGMXdjXGb7nZYHcS5+wzegw6NQ+XIbIlNK2UWTPqWKkBbVyxLS4/nMdj/zC3GfDiDj26Ew+U7YOM3G6JRSShW05DQ3T0/bkNm+vwnU/f1R34Da10DHx22ITCllJ036lCpiarftTWLlqwBwiqGT+w9cm6bC+u9h4lDYv9LmCJVSShWUd37Zyv4lgDW6AAAgAElEQVSTyQDElHDzjxPPQ6p3xkdUNbjpM3A4bYxQKWUHTfqUKoLCe72E2xGSQ4+B2f8EY/wek1JKqYK16+hpPvltR2b766o/4TwaazWcITBgLJQsY1N0Sik7adKnVFEU0xTnyCV8VekpRqU9wKPp95KO95vdvUth3WR741NKKZXvXp61iXS39aVe/0rHqL1noq/z+tehUgubIlNK2U2TPqWKqrK16Tf0QdaUuobJ7o58ltHD1zf3KUg7bV9sSiml8tXyXceZveFMTT7DM0FjEbyzOmp3gRa32RabUsp+mvQpVYRFhgbxvwEtcDqEdzP6csREWh0JBzCL/2tvcEoppfKFx2N4YUZsZvu5mrGExy23Gg4XdH8ZRGyKTikVCDTpU6qIu7x6aZ7q1ZDTUpJXMwZmnk//7S3i98We50qllFKFwU/rDrJm70kASrnSuDX+U19n2/ugXD2bIlNKBQqX3QEopQre7e1r0CAmkkcnhrD29FyaOnYSbNJI/7Qzu9o8To3uDwICOxfAmu8gIwW6vQhRVewOXSml1Fk+Wrid2RsOERMVSp3yEXz/177Mvver/YrzgHeaZ1h56PB/NkWplAokmvQpVUy0qVmGmaM7MXbi49TfNpJgcRNGMmHLniFuw3eUD0pCTu72XXByD9w1F5z6z4RSSgWKP7Yf5aVZmwBYBcChzL5mJY/RLm6Cb/B1z0JopF/jU0oFJp3eqVQxEh7i4v7bBrHmmrHspFLm+QqnY7MnfAAHVsKSd/wcoVLFl4h0F5HNIrJNRP6ZQ/8jIrJRRNaKyHwRqW5HnMo+xhje/HlLrv3vlp2MuNOsRpXW0HRgrmOVUsWLX5M+EflcRA6LyPpc+juJyCkRWe39ecqf8SlVXLTu2IvQB/5gcvitpBlfkd5TJoyDZdr4Bv76Hzi8yYYIlSpeRMQJvAf0ABoCg0Sk4VnDVgGtjDFNgcnAq/6NUtlt4ZYjrNh9AoAgp/B8n0bc26EWXRtW4H8tj1D1yELvSIEer4BDv9tXSln8PW/rS+BdYOx5xvxmjOnln3CUKr5iypam3yPvMX5GXxKWjWOTuzJzPK3JOOBkbsRz1ErfCu400n8YQdCwaRA7DdZOBI8b+rwDZWrZ/RKUKkraANuMMTsARORboA+w8cwAY8yvWcb/CQzxa4TKVsYY3sjylG9Qm2rc1q6G1chIg/eH+wa3GAKVL/dvgEqpgObXr4CMMYuA4/78m0qp3Dkdwm29u3LVvW+xMbobqQTjxsmIxLsznwAGHVpFxss1YepI2LkQdi+GH+8HY2yOXqkipTKwN0t7n/dcbu4CZhVoRCqg/LwxjnX7TwEQ4nIwsnMdX+fSD+D4dus4JAq6PG1DhEqpQBaIz/3bicgaEZklIo3sDkap4qBplVL89MBVDO9QCxHYYqrydsaNmf0uMrJfsGcJbJvv5yiVUgAiMgRoBbx2njHDRWSFiKw4cuSI/4JTBcKTlsKO6a/T0/EnYBjarjoVIkOtzoRDsDDLTN/OYyC8nC1xKqUCV6Bty7cSqG6MSRSRnsCPQN2cBorIcGA4QLVq1fwXoVJFVGiQkyd6NuDmy6vw+7ajbD1Uia2xa6ibYU0n2u6J4RhRtHF41/j98jzU6WIV/D19DGY8bP3u9wGU0v8nlcqj/UDVLO0q3nPZiMi1wL+AjsaY1NxuZoz5GPgYoFWrVvpYvpDbNv4R7ksZB8EwxXSkw5XfWB0n98D4AZCWaLXL1YfWd9sXqFIqYAVU0meMic9yPFNE3heRaGPM0RzG6huaUgWgboUI6laIsBqpv7Dl13G8vCyNX1JqUZ6TLAoZTaikw8HVEDsdanWCb2602gCzHodBE3K7vVIqZ8uBuiJSEyvZGwgMzjpARFoAHwHdjTGH/R+issNfGzbReOdEEKvdTxbCDwPg6kfgx/vgdJYnuT1eBWeQPYEqpQJaQE3vFJGKIiLe4zZY8R2zNyqlirGQCOp1H8HzD95DsyqlOExpxrq7ZnZnzH8BJgz0JXwAm2fBiV3+j1WpQswYkwGMAuYAscBEY8wGEXlORG7wDnsNCAcmeXe4nmZTuKogZKRxYPYb7P3ta4zHA8CmQ/GsmvQSIZKefeyeP2Bcf1/C5wyGfh9DrY5+DlopVVj49UmfiEwAOgHRIrIPeBoIAjDGfAj0B+4TkQwgGRhojO4WoZTdKpcqwXf3tuPfP67ng796M9g5n3BJwXVsMxzbfNZoA8s+gW4v2hKrUoWVMWYmMPOsc09lOb7W70Gp/JeRZj2NE8l2esUXj9Bq/9cAvLtwBSeb38sva7byo5mT+ZQvqfb1lNw+I/v9SpSBgeOgent/RK+UKqT8vXvnIGNMjDEmyBhTxRjzmTHmQ2/ChzHmXWNMI2NMM2NMW2PMH/6MTymVu9AgJ6/2b8r9Pa/gc3ePcwc06O07XvU1pJ32X3BKKVUY7F4Cr9WBt5tD/MHM0/PX7qbuvu8z2yPSx7L+jxl0S5pJpCQDkFqqDiVv/QZu+RpcJayBZevA3fM04VNKXVBATe9USgU2EeGeDrVofsu/OWXCMs9/E3Qzp3p95qvdl3LKqumnlFLKZ9lHkHrKmgK/yNp8defR08z7/mOiJClzmEs8vBP0Nne6fA9+Qzo+YhVbb3gDPLDCSv7uXQRla/v7VSilCiFN+pRSedahaW2O9viYpaYhL6TfypMJfRk5YTXuVll2jVv6kdbyU0qprA6t8x2v+oak4we475u/6GPOLYFTTuIpJ9797SIrQ5ObfZ1RVazkLzjsnOuUUionmvQppS5J7ba9OHLT93zqvh4QFm87yn8OtsQElbQGHImFXb/ZGqNSSgWM1EQ4tt3Xdqcy/8tnSY3bQltHLABGnHDTZyDO7Ne2fwBcwX4MVilV1ARUyQalVOHSq2kltsYl8tb8rQB8tvw4NVztuc01D4DECXcRVro8kpoA4RWg7/sQnWPpTaWUKtriNgDZZz90PDWVU05fyQWp1w2a9IeEg/Dzk9bJEqWh5VA/BqqUKor0SZ9S6m95qEtdrm8Sk9n+KktJh/C0w0jceji5G/Ytg4lDIT3FjjCVUsrvPB7D+KV7GPDREl79atI5/ZGSzBBXlqmdLW+3frcbBR3/CZVawo2f6jROpdTfpkmfUupvcTiE129uxr0dalG7XBiHgmsw1Z3LTnKHN8K8Z/wan1JK5Stj4PgO2DgNDq7Nddjh+BSGfbmcJ6asY+nO41RJ3ZbZF+updu4FETFQx1uVQwQ6j4Hhv0JdrdShlPr7dHqnUupvKxHsZEzPBozp2QCAxKQOjJ8xnakr95BICdo5NvBk0Dhr8NIPSKjaiYjG3W2MWCmlLlLScdi/EvavgP1/WT9Jx6w+hwvumAVV22S7ZO7GOB7/fi3HT6dlnmvo2J15vLj6SOoc/y9Bpw/5Lmp+Kzj1Y5lSqmDovy5KqXwXXrIEg2++hbC6+3ls8lo2ZFTnCscmrnP+BUDypHt5c/1YxvS/mmCXTjhQSgWg4ztg0jA4uCb3MZ4Ma6dib9J3OCGFD6fM44ZtT/GqiWQUD5JCCPdeXY2mKw+A27rsnoH9Ya0b5jzhu1eLIQX3WpRSxZ5+2lJKFZg+zSsz8d52VC1TksfT7+GIiQKgvJzk1o0jmPf2CNI2zrLq+imlVCD57Y3cE76QKN/xphm4k+MZu2QXXd5YSLutb9DcsZ1rnau4P2wB4+6+gjGtg3C4veuZIypBWLS1fq+sd2OrpgOhTM0CfTlKqeJNn/QppQpU86qlWPBoZ9buO8nvf6bSd+NDANRxHKBO/Hcw8TuMw4XUvgYa94f6PSEkwuaolVLFmscDW372tWOaQdUroPLlULmVVRD9gyvh8AbISOaDD//L63GtqCpxdAlelXnZ/eXX4aoTDWt/9d2rYhPrd0g43PUzHN1ibdiilFIFSJM+pVSBczqEFtVK06LaMMzik5h5z+E4M88JEE8GbP3Z+nGFQq3OUK8r1O1qFSFWSil/OrgaTh+2jkuWhXt+BcdZtfOaDYC5T1mHx38GWjHUOReH+MoyuA6uhBO74VCWDV/OJH0AJctAtbYF9CKUUspHkz6llF/JVaOhxW1M/+kHDq37hXaOjTR27PINyEiBLbOsH4CY5tD1Bah5tS3xKqWKoa1ZnvLVue6chM/tMfyU0Y7eRnCI4UrHBmo7DnJbyCKyfJ9l2TgVDq3ztbMmfUop5Sea9Cml/E7CytJ7wD18UOEaes3eRBU5TG/Hn9zg/IMGjj3ZBx9cDV/1glZ3wrXPQmikPUErpYqPLbN9x/Ws2qPpbg8LNh9h7sZDzI89zLHTaZQNashVzg04xDCt/MeEnky0rhEHGI91vOEHOLnXdz9N+pRSNtCkTyllm/s61aZZlSjGLtnNJ7EV+CDtBqrLITo7VnONYxVXOGIJkQxr8IrPrTU2fd+DWp3sDFspVZQlxMEB77o8cULtLiSlZTD4k6Ws3nsy29AfPVdxlXMDAGEnN/s6Oj4Oi14HT7rvXgDBEVBaN2xRSvmf7t6plLJV+zrRfHjb5fz5RBeevL4BrujafOnuztD0MVyd+hZz3Zf7Bsfvg2/6Q+x0+wJWShVt2+b6jqu1w4RG8cQP685J+KLDQ4hufTPGFZr9+uBwaHsf1L7m3HtXbAwO/eillPI/fdKnlAoI0eEh3H11Le66qiYrdp9gwtI9/LTOwT3pj9DbvYRngr6irCRY35xPGgb9P4eGfaxyD8s/hW3zoWYHaDdSd/9USl26LXN8x/W6Mm7pHn5cfSDz1NB21enTvDItqpbC4RDIuB7Wf++7ptkgCI2CRv1ga5Z7gU7tVErZRpM+pVRAERFa1yhD6xpleLTbZTw1dT3TY9uzLLU+44NfpLbjIHgyMJPuQJoNtJ76pcZbF+/+HZZ9Yk2tunwYuIJtfS1KqUImIw22+8orbI5sx3Pfbsxs39KqCs/1aZz9mqYDsid9bYZbvy/rAc5gcKf5+jTpU0rZROcYKKUCVqVSJfhkaCs+uLUlJiKGgWlPst0TA4AYN6we50v4zkg6CrMeg/fbwpHNOdxVKaVysecPSEsAwB1VjTt/SiDNbW3I0jAm8tyED6xpnGeSuSa3QLl61nGJUudO8dSkTyllE036lFIBTUTo0SSGnx/uQP26dRmY9m+2eipnG3MqrAb7mj6Iicxy/vh2+LofnNrn54iVUoVWloLsfwW3Zv+pFAAiQl18MKQloUHOc69xBsFd8+DeRdD3g+x9jfr5jsUJ5RoURNRKKXVBmvQppQqFUiWD+WJYa3pf2YJBaU/yk7stC91NGZE2mhbHXuCqZW3pad7iSLsnISjMuih+P3x9IyQdtzd4pVTgMyZbqYbP4upmHr98Y1Oqlw3L/dqgUIhpBs6zVs1c1gNKRlvHNTtY45RSyga6pk8pVWi4nA6e6t2Q+hUjeGRqadIyPNn6Y4+k0WFxEz658n9ctex+a9OXo5th/AAY8I1vjV9I5DnFlpVSxdyqr60ZAkC6I5QFKfUBuKxCBD2bVLy0e4ZGwR0zYddv0KBPfkWqlFJ5pkmfUqrQuaV1Vbo0KM9fu0+w7Ugi2w4nMmvdIZLT3SSnuxmyIIz/1H2CwXuftS7YtwzeqOe7QXhF6P4SNL7RnheglAosJ/fC7CcymxPpSirWl0R3XV0TEbn0e5e7zPpRSikb6fROpVShVDY8hK6NKnJ/pzq8eUtzpo66ktrlfNOvnth6Ga857sj54sRDMPkOmPkYZKT6KWKlzk9EuovIZhHZJiL/zKG/g4isFJEMEelvR4xF0e6jiaRNGZm5gUtiWHWeT7LW4kWHh9CneSU7w1NKqXyhSZ9SqkioVyGCaaOuyvYB7b2k63gpfRDHpTTpQZGY0CjIWkh52cfweTc4vMmGiJXyEREn8B7QA2gIDBKRhmcN2wMMA8b7N7oiJm4jLHwN1nzH9F8W8cn/niJ490IADMIzzlGkEAJYNflCXDoVXClV+On0TqVUkREW4uJ/A5pzTf3yvDJrEwdOpfCRuzcfJfcGoE75cO7vEE2fPS/h3DTduujAKnj/CqjWDlrcZhV8Dwm38VWoYqoNsM0YswNARL4F+gCZReKMMbu8fZ6cbqAuQkIcfNEdUk4B0BvoneWT0HdBfZh82NoFOMTl4NYrqtkQpFJK5T9N+pRSRYqI0Kd5Zbo2rMgnv+3ggwXbSU53A7DtcCKPTEvkuRK38a/oatx07CMcJsO6cM8S62fqSIisDKVrWD/lLoPyDayfyMqQ29oeY3LvU+rCKgN7s7T3AVfYFEvRNf/ZzITvbNs9MTyd0DezfWPLKpQND/FXZEopVaA06VNKFUklgp082KUuA1pX5dPfdjBh2V4SU60E72RyBo/tu5KvJIYHXD/SxbkKF27vlQbi91k/uxdnv2lEJWh7H7S6A0IirA+Pyz6BFV9YO4P2/xwqtfDvC1UqByIyHBgOUK2aPq0CYO9yWD0us7nE3ZDLHHsoI4mkOMP5v4xRmZu3ANx1VQ0bglRKqYIhxhi7Y/jbWrVqZVasWGF3GEqpABafks63y/bwxe+7OOgtuHxGNKe4ybWIO8KWUiF1J8IF/l0MLQWX9YTNM7I/NShRGobNhApnL8VS+UlE/jLGtLI7jvwkIu2AZ4wx3bztMQDGmJdyGPsl8JMxZvLF3FvfIwGPBz69xprODcxxt+Le9EcQMXx7S1WuaFCDDcfh4e9WsyUukYGtq/LyTU1tDloppfLmfO+PmvQppYoVj8ewOS6B37cdZfG2o/y29Shuj+/fwVLBHh5uU5IBddyEntoFR2LhcKy1+UNqztPCsgmvAHfMgrK1C+5FFHNFNOlzAVuALsB+YDkw2BizIYexX6JJX96s/BqmjQIg1QRxbdqr7DUVeLRrPUZd4yvC7vEY4hJSqBgZ+vfKNCillA3O9/6ou3cqpYoVh0NoEBPJ3VfX4ss72jBn9NV0vqxcZv/JNAdPL07h6u8djKc7ide9BnfOhse2Qu+3rHV+WZWpDdc+A8ERVjsxDr66Afb9Be4Mf70sVcgZYzKAUcAcIBaYaIzZICLPicgNACLSWkT2ATcDH4nIOQmhykHySZj3TGbzQ3cv9poKXNugAvd3qpNtqMMhxESV0IRPKVXk6JM+pZQCFm45wgs/bWTr4cRz+ipGhlKrXBjNq5ZiWNsqlN87G3YsgFqdoFE/cDhh9x/w9Y2Qkey70FUCYppC+YYQUdF6ChhVFWpcCUEl/PXSipyi+KSvIBX798j5z8FvbwCw35SlS+rr1K5Ujon3tiMsRLc2UEoVHTq9UymlLkKG28P3K/fx5twtxMXnXLQ9xOXg9vY1GNGxNmXCgrN3bpsPEwaCO+38fyiyCvR4Ger30h0/L4EmfXlTrN8jU07hebMRDm/h9QfTRrEi4hqmjLySCpGhF7hYKaUKl4CZ3ikin4vIYRFZn0u/iMjbIrJNRNaKSEt/xqeUKt5cTgcDWldjwaOdeazbZdSrEE6QM3tSlprh4eNFO7j6lV94YMIqPl+8k9V7T5KS7oY6XWDYDGjQ2yrvkJv4ffDdEBg/AA6tt6afeTxW2Yfkk9a5LT/Dse0F/IqVKto2TH8rM+Hb7olhQdBVfH5Ha034lFLFjl+f9IlIByARGGuMaZxDf0/gAaAnVn2it4wxF6xTVKy/xVRKFagMt4e9J5JZv/8UHy3azvr98bmOLRMWTIXIUCqXKkHvZjH0ruXEcXA1nNwNCYes9X5b5kDS0RyuFnCFZp8eCtDqLuj6AgSXzN8XVojpk768KW7vkcYYDsWn8PbsdTy88WbKy0kAxmTcS/ehj9GxXrkL3EEppQqn870/+nUyuzFmkYjUOM+QPlgJoQH+FJFSIhJjjDnolwCVUuosLqeDmtFh1IwOo1fTGOZsiOPNuZvZEnfu2r/jp9M4fjqN2IPxzIuN48OYSB7v3pKObbr5NoZIOm6tMfrrS8hWGsKcm/ABrPgMdv0GN34ClZqf25+RCnHroUJjcGkhaVU8HTiRxE/TJhJ7JJXZp6qRnO5hsHM+5YOshO+wlOXWex6lcXVN+JRSxVOgrWCuDOzN0t7nPadJn1LKdiJC98YV6dqwAhsOxLNyzwlW7TnBqr0n2Xs8Cc9ZEydiD8Yz7IvlXFGzDMM71KLzZeVxlCwDvf8HLYbAr/+Bo1usKZ3eKWgElYSoKlYCd2idde7oFvj0Wuj+ErS5x/cHEg/Dl9db/aWqw/VvQt1r/fMfQ6kAYIzhpwW/U27B4wz3rhzpyeU8w1DudU7PHBfZ+SHKVy9vV5hKKWW7QEv6LpqIDAeGA1SrVs3maJRSxYnDITSpEkWTKlHc3r4GYE0DPZqYRlx8CnM3xvHZ4p0kp7sBWLrzOEt3Hqd62ZIMbVeDfi0qU6ZKK7jtB99N3RnWk77gcGtzF2Ng1dcw63FITwJPOsx81Nokpt1ISE2EcTdbCR9YU0jH3QSNbrSSw4iKfv6volQBMAZO7DpncySPx7D1cAKr5k6g76mxhEp6Zt91zr/o5FxNENb/fya0FKFX3OXPqJVSKuD4ffdO7/TOn3JZ0/cRsMAYM8Hb3gx0utD0zuK2XkEpFfgOx6fw1vytfLt8b7bi7wBOh9CuVll6NKlI40pRnE7NICE1A4/H0LpmGaLDs0zTPLYdJt8JB1f7zl37LOxcBNvn5/zHQ6Lg+jeg6c0F8Mrsp2v68qbQvkd6PNZuuFvnXNRwNw6ceM7t6Pg4dH4in4NTSqnAE1AlGy6Q9F2PVZz2zEYubxtj2lzonoX2DU0pVeTtPZ7EN3/uZsKyPcSnXLhYe7DLwQ3NKjGsfQ0aV46yTqYmWE/19izJ+aLrnoO4jbD22+znG/e3kr8Spf7mqwgsmvTlTaF9j1zyPswZc1FDD5WsR+mBHxFiUmDag3Bsq9XhKgEPb4CwsgUYqFJKBYaASfpEZALQCYgG4oCngSAAY8yHYu108C7QHUgC7jDGXPCdqtC+oSmlio2ktAx+XHWA71fu46/dJy7qmuplSxIW7CI0yEGlkh5eTHqWqMPLsw/K+hRj+68w/SFrqucZkVWg3f1Q42prsxeHXyv1FAhN+vKmUL5HHt0GH16VublRWnhl9iYYsn5kcToEV0gYjma3ULnbw+D0rlhJT4E/3rbqZra9Dxr1teEFKKWU/wVM0ldQCuUbmlKq2Dp4Kpk56w8xNzaOE6fTiQh1EREaRFx8Cuv2n8r1ughHKgti3qPsMe+/dy2HQu+3sxd4T4mH2WNg9Tfn3iC0FFRsAiVKWz8RMdDkZoiuk8+vsGBp0pc3hek90hjDgROnqfB9P1z7lwHgKd+I7qefZcsxa11fk8pRPN+3MU0qR+F0yPlup5RS/9/evUdHWZ17HP8+uULC/ZJwCUgEikQtyk3Uo1Btq1BU7BHRllZ6qrS2Lm1Pu1wee2pbjrXVWhFb66VQFa1ai65KhVatRcRSULwsQECJgNwJkWsSkpDkOX+8L2SSTCJIMpPM/D5rzcrs931nZs/Onjx55t3v3klFSZ+ISBvxzua9PPKvTSxctYOq+tOBAtlWzjNnrOTUk3rDqGtrz27Ut2Z+cNbv0J6mXzAlLTgbcv7N0K5TM7yDlqek7/i0ihi5eVkwMVF2DuSNhL4joGMvDpYfZumHH7Nsw8e8t/0Aa7cfYErV8/xv+h8B8JQ07jv5YWauDhZTz85IZcGN5zGgR3Y8342ISKukpE9EpI05UH6YogPllB+uoaSiih//ZTXri4K1Ac3gtokFfG5IDr06tyMzLYU9pZVsLC5lY3EpmempnNanEwPaV5Cy7nnY9DpsXAKlRY2/YHZOkPx1HQAdcoPrAA/thdLdUPZxsHB8l/7BLT0btr8DW9+A7e8G2z7/0+hJY8VB2LIcPloa3PoMh4vvOKG2UdJ3fOIeIyvLYGZB0J8iFKYPYXrp9WyoqV1KYaBtY0HGrUdn47zn8BXcV/3lo/vvnjyMK0bkxabeIiJtjJI+EZE2rrikgqmzl7Nu58EG+7IyUimrrG6wPTsjlaG9O9G9QwadMtM4yXZyVvcyRvZ07NAeWDUPtixrngr2HApXPwXd8oNp9te/DEvuhq0rwCPqllMA32lkQppjpKTv+MQsRpYUBdeVnjwOOubWbl81D56NvmTCupp+XF75Mw7RjmwO8XzGjxmUsh2A1TUDmFQ5g6pwdalLhvXhvqvOwExDOkVEomkqPrbZdfpERJJJjw6ZPHndGKbOXs6aHQfq7IuW8AGUVlazosGkMZlcPboft086ndRR1wb/kL/8YzjY5Mo4n2z3Wvj9BXDRz2Hln2DDq9GPK1obnPFp3/XEXk9al5pqePzLsGsVdDsZrv83pAdDMn3lMxxJ05bVDMVwhtt60q2aU1K28EDHx1g+/E6mbfspuVuDhK8mtR3P5v2IqsLg35R+3dpz+6TTlPCJiHxKOtMnItKG7C87zP2vFvL2R3vZvu8QOw+UU+PB2b78Htnk98impKKK1dv2U1xS2ejzfOn03twzZRiZaanBQu/vPgnF70PJruCMzaF9QWKW3QOyugcLxO/bHNwO7YPcAsgbBVk94LW7GiyeXcuCWUMHnAsnnQP9z4YOOY0ce2x0pu/4xCRGFv4DnvjP2vIls2DENMr37ST93qFH1887r2ImWzyXG7ou54eHZtUeP+A82LSktnz5wzBsCoVFB3l3y34uPCWHrtkZLfseRETaOJ3pExFJEJ2z0rl1wtCj5arqGg6WV9ElK73OWRB3Z9eBCgqLSjhQfpiD5Yd57YNiFqwKzugtWLWDA+WHeXDqCLIzO8BZ0z99pU4eC09/Jbj+7whLgRHTYOwtdYf6SdtyuJyqd58k7Z258PX5jU/28/bcuuV/zWL/kKt4ds69/FeY8L1Z8xn65g/l9nGDOH/wBHjhALz1SHB8ZMI36joYNgWAQTkdGZTTsbnflYhI0lHSJyLShqWlpkQ9A2Jm9Orcjl6d2x3dNnlEP7F14AUAAA4WSURBVHI6ZfLIvzYBsGR9MefdtYivntWfr405iZxO7Ro8zzHpNxquWwR/ngbbVsCgL8AX/w9yhn7iQ6V1q3n8ctI2LwXg0LI5tB/3/YYHleyGdQvrbtuzgd/87h4mlrwI4dKQu/Mn8eQ1Y0g5sszC+Dth50rY9lbt4/JGwUUnNtGPiIg0pKRPRCRJpKQYt00soGtWBve8/AEAe0or+c0/C3lw8YeMOKkrHdulk5WRSvfsTK4YkUdBn7pndrbuLePdLfs4d2CPuslml35w7T+C2TrbyNIP8smWZF3IWIKkr+TVWTyXMZEpZw0kLTWFyqoaCotKyFn1KD1qgtk2sRTw4MzetNI55KUUA1Bt6UyYcj1ErquXlglXzoWHxkJZMWT3DMppGsYpItLclPSJiCQRM+PGCweT17U9v37pA7btOwTA4Wpn2Ya6a/o99u9NfHfcQG64YDDVNc7vXi3kocUbqKyuoUNmGtPOGcC15+XTJSvjyJMr4Usg7s4TZWMY4l3pZXvpyV5WLXiQOUsvITM9lcKigxyuruGVjDn0CM/m/b3Xtxi7fQ7trZI8Kz76XKlDLoKsbg1fpHMeTF8Ea/8KQy+FTn1i9O5ERJKLJnIREUlSVdU1vLxmF3Ne3xhlls9aQ3I7UlpZxda9hxrs65CZxlWj+jFuSA4jB3SlXXpqS1YZ0EQux+tEYqS7s/a5OyhYdRcAG2tyubDy19SEYzZH2jrmZc4A4KC356yK+7k57Wmmpb1U94mufBwKLv30b0JERD6RJnIREZEG0lJTGH96b8af3psPd5ewfd8hSiuqKaus4qk3NvPmpiARfH9X3bUBO2amcbCiCoCSiipmv76R2a9vJDMthdH53bjks32YOKw3WRkKMW2dmVEw8UZ8/UNY+X7yU3ZxccobLKwZA8A3s5ZAuGLIX6vPpox2/L7qS3wt7R9HZ+ykXWf4zEVxegciIgJK+kREBBjYswMDe3Y4Wp50Rl8eXbqJu15cR/nh4J/3zu3TuWX8KUwekcffVu9k1ivrKSwqOfqYiqoalqwvZsn6Yma8sIZLhvVhyqh+DMvrrPXV2rLMjtjo6fDarwD4Rc4rXHPJTRR0qaDjA8uOHlZy6lcYtb8rEz97Kik7JwfrNQIUTAqu3xMRkbjR8E4REWnUpuJSHlz8IZ2z0vnW+QPpFjF5S3WNs/iDIha/v5slhcVs2F0a9TkG5XTgy8P7MumMvvTp0v6E66ThncenWWJkaTHMPA2qwiG+QybAh4tqy7mnw7eXBNd1AhzcGczmWlMNU57Qsh0iIjHQVHxU0iciIs1i+75DLFi5g6fe3Bw1ATSDcwZ25+7Jw+jd+dMnf4mc9JnZxcAsIBWY7e6/rLc/E5gLjAA+Bqa4+6amnrPZYuTCm+GNh6LvCxdjFxGR+GkqPqbEujIiIpKY+nRpz3Xnn8wr/z2WP3/7bK4YkUd2Ru3ELu6wdsdBumdrqF80ZpYK3A+MBwqAq82soN5h3wT2uvsgYCZwZ8wqeM4NkFLvqpDc02HSgzD8mphVQ0REjp+u6RMRkWZlZowa0I1RA7ox47JTefG9nTz39jZeLyzm0mF9yEjT942NGA0UuvsGADN7GrgMWBNxzGXAT8P784Dfmpl5LIbtdOkPE2fC0t9CbgGMng79z64d0ikiIq2Wkj4REWkxWRlpXH5mHpefmceO/Q2XfJA6+gJbIspbgbMaO8bdq8xsP9AdKCYWhn89uImISJuipE9ERGLiRK7jk+NjZtOB6QD9+/ePc21ERCTeNMZGRESkddgG9Iso54Xboh5jZmlAZ4IJXepw94fdfaS7j+zZs2cLVVdERNoKJX0iIiKtw5vAYDPLN7MM4Cpgfr1j5gNHZk25AvhnTK7nExGRNk3DO0VERFqB8Bq9G4AXCZZs+IO7v2dmM4AV7j4fmAM8bmaFwB6CxFBERKRJSvpERERaCXdfCCyst+22iPvlwORY10tERNo2De8UERERERFJYEr6REREREREEpiSPhERERERkQSmpE9ERERERCSBKekTERERERFJYJYIy/uY2W7goxN8mh5AcTNUJ9GoXaJTu0SndolO7RLdp22Xk9xdK44fI8XIFqV2iU7tEp3aJTq1S0PNHh8TIulrDma2wt1HxrserY3aJTq1S3Rql+jULtGpXdoO/a6iU7tEp3aJTu0SndqloZZoEw3vFBERERERSWBK+kRERERERBKYkr5aD8e7Aq2U2iU6tUt0apfo1C7RqV3aDv2uolO7RKd2iU7tEp3apaFmbxNd0yciIiIiIpLAdKZPREREREQkgSnpA8zsYjN738wKzeyWeNcnXsysn5ktMrM1Zvaemd0Ubu9mZi+b2frwZ9d41zXWzCzVzN4xsxfCcr6ZLQ/7zJ/MLCPedYwHM+tiZvPMbJ2ZrTWzs5O9v5jZ98PPz2oze8rM2iVrfzGzP5hZkZmtjtgWtX9Y4L6wjVaa2fD41VwiKUYqPn4SxciGFB+jU4wMxCM+Jn3SZ2apwP3AeKAAuNrMCuJbq7ipAn7g7gXAGOC7YVvcArzi7oOBV8JysrkJWBtRvhOY6e6DgL3AN+NSq/ibBfzd3U8BhhG0UdL2FzPrC9wIjHT304BU4CqSt788Clxcb1tj/WM8MDi8TQceiFEdpQmKkUcpPjZNMbIhxcd6FCPreJQYx8ekT/qA0UChu29w90rgaeCyONcpLtx9h7u/Hd4/SPAHqi9BezwWHvYYMCk+NYwPM8sDvgTMDssGXADMCw9JujYBMLPOwPnAHAB3r3T3fSR5fwHSgPZmlgZkATtI0v7i7q8Be+ptbqx/XAbM9cAyoIuZ9Y5NTaUJipEoPjZFMbIhxccmKUYSn/iopC/4o70lorw13JbUzGwAcCawHMh19x3hrp1AbpyqFS/3AjcDNWG5O7DP3avCcrL2mXxgN/BIOKxntpllk8T9xd23AXcDmwkC2X7gLdRfIjXWP/S3uHXS76UexccGFCMbUnyMQjHyE7VofFTSJw2YWQfgWeB77n4gcp8H070mzZSvZjYRKHL3t+Jdl1YoDRgOPODuZwKl1BuqkoT9pSvBN3L5QB8gm4bDNySUbP1D2j7Fx7oUIxul+BiFYuSxa4n+oaQPtgH9Isp54bakZGbpBAHtj+7+XLh515HTyOHPonjVLw7OBS41s00Ew5ouIBin3yUcmgDJ22e2AlvdfXlYnkcQ5JK5v3we2Ojuu939MPAcQR9Sf6nVWP/Q3+LWSb+XkOJjVIqR0Sk+RqcY2bQWjY9K+uBNYHA4c1AGwQWl8+Ncp7gIx+HPAda6+z0Ru+YD14T3rwGej3Xd4sXd/8fd89x9AEHf+Ke7fxVYBFwRHpZUbXKEu+8EtpjZkHDThcAakri/EAxZGWNmWeHn6UibJH1/idBY/5gPfD2cpWwMsD9imIvEj2Ikio+NUYyMTvGxUYqRTWvR+KjF2QEzm0AwJj0V+IO7/zzOVYoLM/sPYAmwitqx+bcSXLfwDNAf+Ai40t3rX3ya8MxsHPBDd59oZicTfKvZDXgHmOruFfGsXzyY2RkEF+9nABuAbxB8mZS0/cXMfgZMIZjt7x3gWoKx90nXX8zsKWAc0APYBfwE+AtR+kf4D8BvCYb6lAHfcPcV8ai31KUYqfh4LBQj61J8jE4xMhCP+KikT0REREREJIFpeKeIiIiIiEgCU9InIiIiIiKSwJT0iYiIiIiIJDAlfSIiIiIiIglMSZ+IiIiIiEgCU9In0oLMbJqZeSO3fXGs16NmtjVery8iIqIYKRI7afGugEiSmAzUDyBV8aiIiIhIK6MYKdLClPSJxMa77l4Y70qIiIi0QoqRIi1MwztF4ixieMv5ZvYXMysxs4/N7H4za1/v2N5mNtfMis2swsxWmtnUKM+Zb2aPm9nO8LgNZjYrynFnmtkSMyszs/Vm9u2WfK8iIiLHQzFSpHnoTJ9IbKSaWf3PW42710SUnwCeAX4HjAZuA7KBaQBmlg0sBroCtwJbgKnA42aW5e4Ph8flA28AZeFzrAf6A1+s9/qdgCeBe4EZwDeAB8zsfXdf1AzvWURE5FgoRoq0MCV9IrGxLsq2BcDEiPJCd/9heP8lM3Nghpnd4e4fEAScwcDn3P3V8Li/mVkucLuZzXH3auBnQHtgmLtvj3j+x+q9fkfgO0eCl5m9BlwEXA0ooImISKwoRoq0MA3vFImNy4FR9W7fq3fMM/XKTxN8RkeH5fOBbRHB7IgngJ5AQVj+IvBCvWAWTVnkt5XuXgF8QPCNp4iISKwoRoq0MJ3pE4mN1cdwkfquRsp9w5/dgB1RHrczYj9AdxrOghbN3ijbKoB2x/BYERGR5qIYKdLCdKZPpPXIbaS8Lfy5B+gV5XG9IvYDFFMbBEVERBKBYqTICVDSJ9J6XFmvfBVQAywPy4uBPDM7t95xXwGKgDVh+SVgopn1bqmKioiIxJhipMgJ0PBOkdg4w8x6RNm+IuL+BDP7FUFAGg38BJjr7uvD/Y8CNwHPmdmPCIanfBX4AvCt8AJ1wsdNAJaa2R1AIcG3mhe7e4Opq0VEROJMMVKkhSnpE4mNPzeyvWfE/anAD4DrgUrg98CRmcpw91IzGwvcBfySYGax94GvufsTEcdtMrMxwO3AL4AOBMNfnm+2dyMiItJ8FCNFWpi5e7zrIJLUzGwa8Agw+BguZBcREUkaipEizUPX9ImIiIiIiCQwJX0iIiIiIiIJTMM7RUREREREEpjO9ImIiIiIiCQwJX0iIiIiIiIJTEmfiIiIiIhIAlPSJyIiIiIiksCU9ImIiIiIiCQwJX0iIiIiIiIJ7P8BSC3JT4fnPmoAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "display_metrics(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51d9ee4",
      "metadata": {
        "id": "e51d9ee4"
      },
      "source": [
        "#### Evaluating the custom CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b4e9d9",
      "metadata": {
        "id": "14b4e9d9"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'Custom_CNN'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f51c4e7",
      "metadata": {
        "id": "0f51c4e7"
      },
      "source": [
        "##### Initialising the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3378adb6",
      "metadata": {
        "id": "3378adb6"
      },
      "source": [
        "##### Performing the training and validation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef86fe85",
      "metadata": {
        "id": "ef86fe85"
      },
      "source": [
        "##### Visualising the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670e9ca6",
      "metadata": {
        "id": "670e9ca6"
      },
      "source": [
        "## Tips"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98919fe9",
      "metadata": {
        "id": "98919fe9"
      },
      "source": [
        "A good starting point for small networks is LeNet5. You will find many existing implementations online."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e761b03",
      "metadata": {
        "id": "2e761b03"
      },
      "source": [
        "Don't forget the basic structure of a convnet: convolutional layer, activation and pooling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22401022",
      "metadata": {
        "id": "22401022"
      },
      "source": [
        "You can use the [`summary`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary) method of the Keras model API to print the description of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b20331c",
      "metadata": {
        "id": "7b20331c"
      },
      "source": [
        "## Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e86af3fb",
      "metadata": {
        "id": "e86af3fb"
      },
      "source": [
        "This assignment was prepared by Thomas Hossler et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
        "\n",
        "\n",
        "References\n",
        "\n",
        "[1] Yu, F. Vladlen, K. Multi-Scale Context Aggregation by Dialated Convolutions. ArXiv. 2015. [doi:10.48550/ARXIV.1511.07122](https://arxiv.org/abs/1511.07122).\n",
        "\n",
        "[2] LeCun, Y. et al. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, 1998. [doi:10.1109/5.726791](https://ieeexplore.ieee.org/document/726791).\n",
        "\n",
        "\n",
        "[3] Glorot, X. et al. Deep Sparse Rectifier Neural Networks. Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, PMLR 15:315-323, 2011. [https://proceedings.mlr.press/v15/glorot11a.html](https://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf).\n",
        "\n",
        "\n",
        "Helpful resources:\n",
        "* [Convolutional Neural Network: An Overview by S. Shah | Analytics Vidhya](https://www.analyticsvidhya.com/blog/2022/01/convolutional-neural-network-an-overview/)\n",
        "* [Understanding the receptive field of deep convolutional networks by N. Adaloglou | AI Summer](https://theaisummer.com/receptive-field/)\n",
        "* [The Dying ReLU Problem, Clearly Explained by K. Leung | Towards Data Science](https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24)\n",
        "* [What is the dying ReLU problem? by S. Azhar | Educative.io](https://www.educative.io/answers/what-is-the-dying-relu-problem)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f0bedd6f",
        "384c8474",
        "205af274",
        "18604c48",
        "34eca184",
        "41fe458d",
        "25615960",
        "e51d9ee4"
      ]
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a93fd23698b34ba98740725e2fb803df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d095fd1cda654834a6dfff126d92e554",
              "IPY_MODEL_66313e2ad05c425599ec481b132048b2",
              "IPY_MODEL_4108dc53d0194af2a8fa4f670d17662c"
            ],
            "layout": "IPY_MODEL_9d41cf71884b4205aab8afef59ccd4f7"
          }
        },
        "d095fd1cda654834a6dfff126d92e554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2279450772b47d9bc7e3cc5ea2ebbb5",
            "placeholder": "​",
            "style": "IPY_MODEL_f3e3ea05dfad4578bb8f45c33fccca20",
            "value": "Dl Completed...: 100%"
          }
        },
        "66313e2ad05c425599ec481b132048b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fac162b490074a0d8095207479d7f119",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9ddc266a7124f3e9957c3e4f0e8848c",
            "value": 4
          }
        },
        "4108dc53d0194af2a8fa4f670d17662c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42b1bf702eea4f2aab12d18800bed515",
            "placeholder": "​",
            "style": "IPY_MODEL_088c7b2aa6484285af2d5324a7264172",
            "value": " 4/4 [00:01&lt;00:00,  1.88 file/s]"
          }
        },
        "9d41cf71884b4205aab8afef59ccd4f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2279450772b47d9bc7e3cc5ea2ebbb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e3ea05dfad4578bb8f45c33fccca20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fac162b490074a0d8095207479d7f119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9ddc266a7124f3e9957c3e4f0e8848c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42b1bf702eea4f2aab12d18800bed515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "088c7b2aa6484285af2d5324a7264172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}