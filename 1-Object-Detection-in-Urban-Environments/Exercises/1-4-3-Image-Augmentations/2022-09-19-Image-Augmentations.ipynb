{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3587624",
   "metadata": {},
   "source": [
    "# Exercise 1.4.3 - Image Augmentations\n",
    "#### By Jonathan L. Moran (jonathan.moran107@gmail.com)\n",
    "\n",
    "From the Self-Driving Car Engineer Nanodegree programme offered at Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557104a5",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17584f9",
   "metadata": {},
   "source": [
    "* Understand why data augmentation is useful and how it can help with object detection for self-driving cars;\n",
    "* Experiment with the [Albumentations](https://albumentations.ai/docs/) library to perform various data augmentations;\n",
    "* Test the augmentation pipeline on a few frames from the [Waymo Open Dataset]() and the [GTSRB]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de4710",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d862cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4440f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb637caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COLAB = True                # True if running in Google Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory\n",
    "DIR_BASE = '' if not ENV_COLAB else '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subdirectory to save output files\n",
    "DIR_OUT = os.path.join(DIR_BASE, 'out/')\n",
    "# Subdirectory pointing to input data\n",
    "DIR_SRC = os.path.join(DIR_BASE, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46156d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unzipping the GTSRB dataset\n",
    "#!unzip -q /content/GTSRB.zip -d /content/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692acf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating subdirectories (if not exists)\n",
    "os.makedirs(DIR_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94910c4",
   "metadata": {},
   "source": [
    "### 1.1. Image Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b65574",
   "metadata": {},
   "source": [
    "* Why we use augmentation;\n",
    "* A few popular augmentation methods;\n",
    "* How to address the self-driving car domain shift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0d491",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b30566",
   "metadata": {},
   "source": [
    "[Data augmentation](https://en.wikipedia.org/wiki/Data_augmentation) is a fundamental technique to increase generalisability of object detection models and decrease operational costs associated with data gathering. Comprising of two keywords _data_ and _augmentation_, this technique relies on _augmentation_ strategies to modify existing _data_. Data here implies a set of images and often corresponding bounding box coordinates. Augmentation in this case refers to geometric or pixel-wise transformations. Some common examples of augmentations are _flipping_, _rotating_, _blurring_ and _brightness adjustments_. Most of these transformations are applied at random to a given dataset in order to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting) during the training process. By modifying the training dataset at random, we introduce complexities and altercations to the data. Augmentation exposes the model to different versions of the original training data and thereby acts as a [regularisation](https://en.wikipedia.org/wiki/Regularization_\\(mathematics\\)) technique to improve the generalisability of a learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636e1d9",
   "metadata": {},
   "source": [
    "#### Considerations for self-driving cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc9150",
   "metadata": {},
   "source": [
    "Generalisability is especially important in the self-driving car domain, as object detection models need to perform especially well over many different factors across driving scenes. Some real-world variables that affect the generalisability of perception models in driving domains are lighting conditions (day/night), weather conditions (sunny/foggy/rainy), scale (adjusting perceived distance), camera intrinsics (lens properties) and extrinsics (position and orientation in ego vehicle frame), just to name a few. With data augmentation we are able to mimic time-of-day shifts, occlusions (obstructions in field-of-view), and certain camera properties that we otherwise might not have enough image data to account for. By using augmentation strategies we are also able to introduce _additional_ data into our training pipeline. This can not only help with model generalisability but also reduce the cost of data collection and labelling efforts. Data augmentation might also help us produce synthetic datasets for conditions or factors that are otherwise not possible to collect in bulk (accidents, one-off traffic conditions, irregular lane markings/street signs and other temporal conditions e.g., road work, natural disasters, etc.). As Waymo says in their [using automated data augmentation](https://blog.waymo.com/2020/04/using-automated-data-augmentation-to.html) blog post, \"you have to be in the right place at the right time [to] capture a seal crossing the street or a man breakdancing on the sidewalk.\" With that said, data augmentation can help turn these rare occurrences into slightly less-rare occurrences by generating more training data needed to properly learn and react to these one-off scenarios.\n",
    "\n",
    "\n",
    "Taking into account the purpose and specific use-cases applied to the driving domain is _extremely important_ when selecting data augmentation strategies. For example, introducing _flipping_ augmentation into a dataset could actually hurt generalisability of certain object detection models. Detectors trained on front-facing dashcam images for use on U.S. roads will (almost) never encounter [left-hand traffic](https://en.wikipedia.org/wiki/Left-_and_right-hand_traffic). By flipping images in our training set to mimic right-hand traffic, we might be forcing the model to overgeneralise to a driving scenario it will never see. Likewise, indiscriminately flipping images of certain road signs may result in a model's inability to discern the correct directional cues (e.g., _left_ from _right_ arrows) in the real-world environment. In addition to flipping, _random cropping_ might not be a particularly useful augmentation strategy for AD/ADAS perception models trained to operate on consistent hardware placed on consistent locations around a given vehicle. Shifting and scaling operations in this scenario might again force unwanted generalisations of the model to unlikely scenarios. In other words, a vehicle/hardware-coupled model will likely not be applied to different vehicles with different camera/sensor placements. Random scaling and shifting in this case might hurt the performance of a network that constantly receives image data from similarly-placed cameras around a given vehicle. In short, **data augmentation strategies should not be blindly selected**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65ad57",
   "metadata": {},
   "source": [
    "#### Useful agumentations for self-driving cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15435c9",
   "metadata": {},
   "source": [
    "##### Cutout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15330b",
   "metadata": {},
   "source": [
    "[Cutout](https://albumentations.ai/docs/api_reference/augmentations/dropout/cutout/) is an augmentation technique that performs a coarse dropout of square regions in an image. In the Albumentation library, the `dropout.cutout.Cutout` augmentation class takes in several arguments that specify\n",
    "* `num_holes`: the number of regions to select from an image at random;\n",
    "* `max_h_size`, `max_w_size`: the maximum height/width of the cutout region (area to \"delete\");\n",
    "* `fill_value`: the pixel value (or list of values) to replace the cutout region with.\n",
    "\n",
    "Cutout augmentations do not drastically alter the camera intrinsic/extrinsic parameters (unlike flip, shift or scale). Instead, cutouts allow us to simulate occluded regions in a camera's field-of-view. In the real world, partially-occluded objects appear often (think: tree branch covering stop sign). Adding invariance to this type of phenomenon through simulated data via augmentation is generally a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8c964",
   "metadata": {},
   "source": [
    "##### Hue jitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff07dc",
   "metadata": {},
   "source": [
    "Hue jitter is from a class of augmentations that randomly adjust the colour properties of an image (the hue, brightness, contrast or saturation). The [`transforms.ColorJitter`](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ColorJitter) allows one to specify a scaling factor for any of these colour properties. For hue jitter, we specify a value (or range of `[min, max]` values) between `-0.5` and `0.5` to modify the hue by.\n",
    "\n",
    "Using hue jitter can be especially important for object detection in the driving domain, as you would want a model to consistently detect a class of objects (e.g., vehicle) regardless of its colour properties (e.g., 'red' vehicle should be no different than 'blue' vehicle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7698c9",
   "metadata": {},
   "source": [
    "##### Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6db27",
   "metadata": {},
   "source": [
    "##### More sophisticated techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc15ab",
   "metadata": {},
   "source": [
    "Techniques such as _selective blending_ images or modifying directional lighting exist. Their \"features\" (parameters) need to be fine-tuned, often by hand, through a time-consuming process. Handcrafting these features is mostly a trial and error process limited to the imagination, time, and experience of the researcher. Despite these limitations, data augmentation proves to be a crucial step in any ML workflow — often resulting in performance gains greater than with fine-tuning model architecture alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f473940",
   "metadata": {},
   "source": [
    "##### Note on automated data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0780eef",
   "metadata": {},
   "source": [
    "Effective data augmentation strategies often require expertise and manual labour to design. In order to formulate augmentation strategies (policies) for a _target_ task (e.g., traffic light recognition), one may utilise _proxy_ tasks (e.g., pedestrian object detection) to inform them of more-optimal augmentation strategies (the _search space_, i.e., the list of augmentations and parameters). A strong assumption made with this approach is that the selected proxy task closely relates to the broader task at-hand (e.g., object detection in the driving domain). However, even with this assumption, the learned augmentation policies may be sub-optimal with respect to the model and the dataset size in-question. \n",
    "\n",
    "\n",
    "Rather than limiting augmentation to pixel-level or geometric transformations, researchers have turned to architectures fine-tuned for data generation to produce unique image subsets. The _Smart Augmentation_ [1] model, like others in this class, is a generative model using Bayesian optimisation to learn the best sample blending (combination of images) for a specific task. Generative Adversarial Networks ([GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network)) used for data generation transform the feature space with e.g., noise, interpolations that go beyond traditional operations. GANs have also been used to make optimal selections regarding the sequence of data augmentation operations to perform [2][3].\n",
    "\n",
    "\n",
    "Because of the complications associated with optimal policy formulation, researchers at Google Brain have proposed methods for _automating_ data augmentation selection. _RandAugment_ [4] dramatically reduces the search space (\"possible augmentations to use\") by selecting augmentation strategies _at random_ with equal probability. _AutoAugment_ [5] expands on this by utilising reinforcement learning methods to search the parameter space and predict augmentation policies. The prediction strategies (denoted $S$) are used to train a child network to produce a validation accuracy $R$, which is then used as a reward for a Proximal Policy Optimization algorithm (Schulman et al., 2017).\n",
    "\n",
    "Waymo, a pioneer in self-driving, started utilising the _RandAugment_ approach in 2019 for image-based classification and detection tasks. In [this](https://blog.waymo.com/2020/04/using-automated-data-augmentation-to.html) blog post, Waymo engineers report _significant improvements_ to mean average precision ([mAP](https://www.v7labs.com/blog/mean-average-precision)) for their classifiers used to detect foreign objects such as construction equipment and animals. Waymo took image augmentation a step further and began applying similar strategies to their 3D LiDAR point cloud data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d2a425",
   "metadata": {},
   "source": [
    "<img src=https://1.bp.blogspot.com/-Wq9kxxCatYw/Xqhc_mmEMXI/AAAAAAAADww/tt_LkG8oJAE92DRBPM7dkPGBiuwpz8STwCNcBGAsYHQ/s640/auto_augment_4_1.gif alt=\"Animated GIF showing the various augmentation strategies Waymo has used on their LiDAR point cloud data. Here a pedestrian-like figurine is shown with distortions (random flip, scaling, translation) applied to the figurine.\">\n",
    "$$\n",
    "\\textrm{Fig. 1. Augmentation strategies for LiDAR point clouds (credit:} \\href{https://blog.waymo.com/2020/04/using-automated-data-augmentation-to.html}{ \\ Waymo} \\textrm{).}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79770d",
   "metadata": {},
   "source": [
    "Shown in Fig. 1 is a number of augmentations/distortions selected for this task. What is particularly novel about Waymo's approach is the introduction of the _Population Based Augmentation_ (PBA) and _Progressive Population Based Augmentation_ (PPBA) model approaches for generating nonstationary augmentation policy schedules [6][7]. In short, these models are trained with principles similar to Darwin's Natural Selection Theory to optimise augmentation strategies. They narrow down the search space at each population iteration and record the best parameters in past iterations as references for mutating parameters in future iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2e340",
   "metadata": {},
   "source": [
    "##### In summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e63235",
   "metadata": {},
   "source": [
    "At this point, you may be asking yourself: _why are these approaches all that important?_ \n",
    "\n",
    "Well, companies like Waymo have shown promising results (up to 10x improvement!) when utilising augmentation strategies on the [Waymo Open Dataset](https://waymo.com/open). Using that as motivation, _automated_ augmentation strategies like PPBA can be faster, more efficient methods over random search or hand-tuning. These efforts can enable self-driving car companies to increase efficiency beyond what is possible with manual annotation schemes, allowing them to cut costs and increase throughput — ultimately pushing us all closer to the goal of fully-autonomous driving capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d378f",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ff289",
   "metadata": {},
   "source": [
    "Write down a list of relevant augmentations and store them in the `transforms` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa84edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT RELEVANT AUGMENTATIONS\n",
    "transforms = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bec84",
   "metadata": {},
   "source": [
    "You should also implement a quick script to visualize the batches and check your augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(batch):\n",
    "    indices = np.random.choice(range(256), replace=False, size=10)\n",
    "    f, ax = plt.subplots(2, 5, figsize=(15, 5))\n",
    "    for i, idx in enumerate(indices):\n",
    "        x = i // 5\n",
    "        y = i % 5\n",
    "        im = batch[idx, ...]\n",
    "        im *= 255\n",
    "        im = im.astype(np.uint8)\n",
    "        ax[x, y].imshow(im)\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8df47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `augmentations.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_fn(image):\n",
    "    \"\"\" augment an image \"\"\"\n",
    "    aug_data = transforms(image=image.squeeze())\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    return aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(image, label):\n",
    "    \"\"\" wrapper function to apply augmentation \"\"\"\n",
    "    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n",
    "    return aug_img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a2b44",
   "metadata": {},
   "source": [
    "You can run `python augmentations.py` to display augmented images (in the Desktop window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebdd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From Udacity's `augmentations.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b72d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Augment dataset')\n",
    "parser.add_argument('-d', '--imdir', required=True, type=str,\n",
    "                    help='data directory')\n",
    "args = parser.parse_args()    \n",
    "\n",
    "dataset = image_dataset_from_directory(args.imdir, \n",
    "                                       image_size=(32, 32),\n",
    "                                       validation_split=0.1,\n",
    "                                       subset='training',\n",
    "                                       seed=123,\n",
    "                                       batch_size=1)\n",
    "\n",
    "# APPLY AUGMENTATIONS AND DISPLAY BATCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef2fd6",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a0a02",
   "metadata": {},
   "source": [
    "You should use the `Compose` API to use multiple augmentations. You can find an example of an augmentation pipeline using `Compose` [here](https://albumentations.ai/docs/examples/example/#define-an-augmentation-pipeline-using-compose-pass-the-image-to-it-and-receive-the-augmented-image)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f032f",
   "metadata": {},
   "source": [
    "This [Github repository](https://github.com/albumentations-team/albumentations_examples) contains different examples of augmentations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4a4d6",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e51213",
   "metadata": {},
   "source": [
    "This assignment was prepared by Thomas Hossler et al., Winter 2021 (link [here](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)).\n",
    "\n",
    "\n",
    "References\n",
    "\n",
    "[1] Lemely, J. et al., Smart Augmentation: Learning an Optimal Data Augmentation Strategy. IEEE Access, vol. 5:5858-5869. 2017. [doi:10.1109/ACCESS.2017.2696121](https://ieeexplore.ieee.org/document/7906545).\n",
    "\n",
    "[2] Ratner, A. et al., Learning to Compose Domain-Specific Transformations for Data Augmentation. arXiv. 2017. [doi:10.48550/ARXIV.1709.01643](https://arxiv.org/abs/1709.01643).\n",
    "\n",
    "[3] DeVries, T. et al., Dataset Augmentation in Feature Space. arXiv. 2017. [doi:10.48550/ARXIV.1702.05538](https://arxiv.org/abs/1702.05538).\n",
    "\n",
    "[4] Cubuk, E. D. et al., RandAugment: Practical automated data augmentation with a reduced search space. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 2020. [doi:10.48550/ARXIV.1909.13719](https://arxiv.org/abs/1909.13719).\n",
    "\n",
    "[5] Cubuk, E. D. et al., AutoAugment: Learning Augmentation Policies from Data. arXiv. 2018. [doi:10.48550/ARXIV.1805.09501](https://arxiv.org/abs/1805.09501).\n",
    "\n",
    "[6] Ho, D. et al., Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules. arXiv. [doi:10.48550/ARXIV.1905.05393](https://arxiv.org/abs/1905.05393).\n",
    "\n",
    "[7] Cheng, S. et al., Improving 3D Object Detection through Progressive Population Based Augmentation. arXiv. [doi:10.48550/ARXIV.2004.00831](https://arxiv.org/abs/2004.00831).\n",
    "\n",
    "\n",
    "\n",
    "Helpful resources:\n",
    "* [When Conventional Wisdom Fails: Revisiting Data Augmentation for Self-Driving Cars by M. Cooper | Medium](https://towardsdatascience.com/when-conventional-wisdom-fails-revisiting-data-augmentation-for-self-driving-cars-4831998c5509)\n",
    "\n",
    "* [Data Augmentation with GANs for Defect Detection by L. Melchoir | Medium](https://medium.com/dida-machine-learning/data-augmentation-with-gans-for-defect-detection-8318fab1a514)\n",
    "\n",
    "* [Using automated data augmentation to advance our Waymo Driver | Waymo Waypoint Blog](https://blog.waymo.com/2020/04/using-automated-data-augmentation-to.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
