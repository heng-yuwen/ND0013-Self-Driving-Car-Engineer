%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jonathan Moran at 2022-11-18 14:14:04 -0800 


%% Saved with string encoding Unicode (UTF-8) 



@book{nla.cat-vn941564,
	abstract = {This graduate-level text augments and extends beyond undergraduate studies of signal processing, particularly in regard to communication systems and digital filtering theory. Vital for students in the fields of control and communications, its contents are also relevant to students in such diverse areas as statistics, economics, bioengineering, and operations research.},
	address = { Englewood Cliffs, N.J. 07632},
	author = {Anderson, Brian D. O. and Moore, John B.},
	catalogue-url = {https://nla.gov.au/nla.cat-vn941564},
	date-added = {2022-11-18 14:07:28 -0800},
	date-modified = {2022-11-18 14:14:00 -0800},
	isbn = {0136381227},
	language = {English},
	life-dates = {1979 -},
	pages = {pp.64-65},
	publisher = {Prentice-Hall},
	subjects = {Signal processing.; Electric filters.},
	title = {Optimal Filtering},
	type = {Book},
	year = {1979}}

@misc{https://doi.org/10.48550/arxiv.2001.03343,
	abstract = {In this work, we propose an efficient and accurate monocular 3D detection framework in single shot. Most successful 3D detectors take the projection constraint from the 3D bounding box to the 2D box as an important component. Four edges of a 2D box provide only four constraints and the performance deteriorates dramatically with the small error of the 2D detector. Different from these approaches, our method predicts the nine perspective keypoints of a 3D bounding box in image space, and then utilize the geometric relationship of 3D and 2D perspectives to recover the dimension, location, and orientation in 3D space. In this method, the properties of the object can be predicted stably even when the estimation of keypoints is very noisy, which enables us to obtain fast detection speed with a small architecture. Training our method only uses the 3D properties of the object without the need for external networks or supervision data. Our method is the first real-time system for monocular image 3D detection while achieves state-of-the-art performance on the KITTI benchmark. Code will be released at https://github.com/Banconxuan/RTM3D.},
	author = {Li, Peixuan and Zhao, Huaici and Liu, Pengfei and Cao, Feidao},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-07 19:37:48 -0800},
	date-modified = {2022-11-07 19:38:07 -0800},
	doi = {10.48550/ARXIV.2001.03343},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
	publisher = {arXiv},
	title = {RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving},
	url = {https://arxiv.org/abs/2001.03343},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2001.03343},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2001.03343}}

@misc{https://doi.org/10.48550/arxiv.1803.06199,
	author = {Simon, Martin and Milz, Stefan and Amende, Karl and Gross, Horst-Michael},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-07 19:32:43 -0800},
	date-modified = {2022-11-07 19:32:43 -0800},
	doi = {10.48550/ARXIV.1803.06199},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Complex-YOLO: Real-time 3D Object Detection on Point Clouds},
	url = {https://arxiv.org/abs/1803.06199},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1803.06199},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1803.06199}}

@inproceedings{7780459,
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8Ã— deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2022-11-07 19:27:38 -0800},
	date-modified = {2022-11-07 19:29:15 -0800},
	doi = {10.1109/CVPR.2016.90},
	keywords = {Training; Degredation; Complexity theory; Image recognition; Neural networks; Visualization; Image segmentation; Image classification; Learning (artificial intelligence); Neural nets; Object detection},
	pages = {770-778},
	title = {Deep Residual Learning for Image Recognition},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR.2016.90}}

@misc{https://doi.org/10.48550/arxiv.1912.04838,
	abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at https://waymo.com/open/.},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhao, Sheng and Cheng, Shuyang and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-07 19:22:27 -0800},
	date-modified = {2022-11-07 19:22:27 -0800},
	doi = {10.48550/ARXIV.1912.04838},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
	url = {https://arxiv.org/abs/1912.04838},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1912.04838},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1912.04838}}
