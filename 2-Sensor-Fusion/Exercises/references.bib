%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jonathan Moran at 2022-10-26 19:08:14 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{burns1991system,
	abstract = {A pulsed GaAs laser rangefinder is analyzed and designed. Expressions for background 
and signal power, noise, and signal-to-noise ratio are derived. The effects of pulse rise time, 
receiver bandwidth, and SNR on probability of detection and range accuracy are discussed. 
A computer simulation is used to optimize laser power, receiver aperture, and preamplifier 
bandwidth. A method ofthreshold detection is presented and discussed. Experimental 
results include receiver preamplifier transfer function and threshold detector performance.},
	author = {Burns, Hoyt N and Christodoulou, Christos G and Boreman, Glenn D},
	date-added = {2022-10-26 19:07:38 -0700},
	date-modified = {2022-10-26 19:08:03 -0700},
	journal = {Optical engineering},
	number = {3},
	pages = {323--329},
	publisher = {SPIE},
	title = {System design of a pulsed laser rangefinder},
	volume = {30},
	year = {1991}}

@inproceedings{heinzler2019weather,
	abstract = {Lidar sensors are often used in mobile robots and autonomous vehicles to complement camera, radar and ultrasonic sensors for environment perception. Typically, perception algorithms are trained to only detect moving and static objects as well as ground estimation, but intentionally ignore weather effects to reduce false detections. In this work, we present an in-depth analysis of automotive lidar performance under harsh weather conditions, i.e. heavy rain and dense fog. An extensive data set has been recorded for various fog and rain conditions, which is the basis for the conducted in-depth analysis of the point cloud under changing environmental conditions. In addition, we introduce a novel approach to detect and classify rain or fog with lidar sensors only and achieve an mean union over intersection of 97.14% for a data set in controlled environments. The analysis of weather influences on the performance of lidar sensors and the weather detection are important steps towards improving safety levels for autonomous driving in adverse weather conditions by providing reliable information to adapt vehicle behavior.},
	author = {Heinzler, Robin and Schindler, Philipp and Seekircher, J{\"u}rgen and Ritter, Werner and Stork, Wilhelm},
	booktitle = {2019 IEEE intelligent vehicles symposium (IV)},
	date-added = {2022-10-26 18:41:44 -0700},
	date-modified = {2022-10-26 18:42:06 -0700},
	organization = {IEEE},
	pages = {1527--1534},
	title = {Weather influence and classification with automotive lidar sensors},
	year = {2019}}

@inproceedings{haran2016infrared,
	abstract = {In order to be able to evaluate the performances of different Autonomous Emergency Braking (AEB) systems for pedestrian crash avoidance and mitigation, a standard surrogate pedestrian mannequin needs to be developed. One of the requirements for pedestrian mannequin is to ensure it ``looks'' like a real representative pedestrian to each of the sensor modalities used in AEB systems. The purpose of this paper is to generate the recommended IR reflectance specifications for the standard surrogate pedestrian mannequin based on the collected data from various sources and the experiments.},
	author = {Haran, Terence and Chien, Stanley},
	booktitle = {2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)},
	date-added = {2022-10-26 18:40:31 -0700},
	date-modified = {2022-10-26 18:41:07 -0700},
	organization = {IEEE},
	pages = {2230--2235},
	title = {Infrared reflectivity of pedestrian mannequin for autonomous emergency braking testing},
	year = {2016}}

@article{Wojtanowski:2014aa,
	abstract = {Laser rangefinder performance (i.e., maximum range) is strongly affected by environment due to visibility-dependent laser attenuation in the atmosphere and target reflectivity variations induced by surface condition changes (dry vs. wet). Both factors have their unique spectral features which means that rangefinders operating at different wavelengths are affected by specific environmental changes in a different way. Current state of the art TOF (time of flight) semiconductor laser rangefinders are based mainly on two wavelengths: 905 nm and 1550 nm, which results from atmospheric transmission windows and availability of high power pulsed sources. The paper discusses the scope of maximum range degradation of hypothetical 0.9 μm and 1.5 μm rangefinders due to selected water-related environmental effects. Atmospheric extinction spectra were adapted from Standard Atmosphere Model and reflectance fingerprints of various materials have been measured. It is not the aim of the paper to determine in general which wavelength is superior for laser range finding, since a number of criteria could be considered, but to verify their susceptibility to adverse environmental conditions.},
	author = {Wojtanowski, J. and Zygmunt, M. and Kaszczuk, M. and Mierczyk, Z. and Muzal, M.},
	date = {2014/09/01},
	date-added = {2022-10-26 18:29:43 -0700},
	date-modified = {2022-10-26 18:29:43 -0700},
	doi = {10.2478/s11772-014-0190-2},
	id = {Wojtanowski2014},
	isbn = {1896-3757},
	journal = {Opto-Electronics Review},
	number = {3},
	pages = {183--190},
	title = {Comparison of 905 nm and 1550 nm semiconductor laser rangefinders'performance deterioration due to adverse environmental conditions},
	url = {https://doi.org/10.2478/s11772-014-0190-2},
	volume = {22},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.2478/s11772-014-0190-2}}

@misc{https://doi.org/10.48550/arxiv.1801.09847,
	abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
	author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-26 15:59:52 -0700},
	date-modified = {2022-10-26 16:00:06 -0700},
	doi = {10.48550/ARXIV.1801.09847},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Open3D: A Modern Library for 3D Data Processing},
	url = {https://arxiv.org/abs/1801.09847},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1801.09847},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1801.09847}}

@misc{https://doi.org/10.48550/arxiv.1912.04838,
	abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at https://waymo.com/open/.},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhao, Sheng and Cheng, Shuyang and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-26 15:58:07 -0700},
	date-modified = {2022-10-26 15:58:07 -0700},
	doi = {10.48550/ARXIV.1912.04838},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
	url = {https://arxiv.org/abs/1912.04838},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1912.04838},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1912.04838}}
