%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jonathan Moran at 2022-11-12 17:21:29 -0800 


%% Saved with string encoding Unicode (UTF-8) 



@article{10.1115/1.3662552,
	abstract = {The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the ``state-transition'' method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
	author = {Kalman, R. E.},
	date-added = {2022-11-12 17:20:50 -0800},
	date-modified = {2022-11-12 17:21:14 -0800},
	doi = {10.1115/1.3662552},
	eprint = {https://asmedigitalcollection.asme.org/fluidsengineering/article-pdf/82/1/35/5518977/35\_1.pdf},
	issn = {0021-9223},
	journal = {Journal of Basic Engineering},
	month = {03},
	number = {1},
	pages = {35-45},
	title = {A New Approach to Linear Filtering and Prediction Problems},
	url = {https://doi.org/10.1115/1.3662552},
	volume = {82},
	year = {1960}}

@misc{https://doi.org/10.48550/arxiv.1708.02002,
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: this https URL.},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-28 11:06:49 -0700},
	date-modified = {2022-10-28 11:07:07 -0700},
	doi = {10.48550/ARXIV.1708.02002},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Focal Loss for Dense Object Detection},
	url = {https://arxiv.org/abs/1708.02002},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1708.02002},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1708.02002}}

@article{DBLP:journals/corr/abs-1711-08488,
	abstract = {In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
	author = {Charles Ruizhongtai Qi and Wei Liu and Chenxia Wu and Hao Su and Leonidas J. Guibas},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1711-08488.bib},
	date-added = {2022-10-28 11:00:42 -0700},
	date-modified = {2022-10-28 11:00:55 -0700},
	eprint = {1711.08488},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Wed, 11 Nov 2020 08:48:10 +0100},
	title = {Frustum PointNets for 3D Object Detection from {RGB-D} Data},
	url = {http://arxiv.org/abs/1711.08488},
	volume = {abs/1711.08488},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1711.08488}}

@misc{https://doi.org/10.48550/arxiv.1611.07759,
	abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
	author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-28 10:59:36 -0700},
	date-modified = {2022-10-28 10:59:52 -0700},
	doi = {10.48550/ARXIV.1611.07759},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Multi-View 3D Object Detection Network for Autonomous Driving},
	url = {https://arxiv.org/abs/1611.07759},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1611.07759},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1611.07759}}

@misc{https://doi.org/10.48550/arxiv.1808.02350,
	abstract = {Object detection and classification in 3D is a key task in Automated Driving (AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction of the surrounding environment, while the task of 3D object bounding box detection in real time remains a strong algorithmic challenge. In this paper, we build on the success of the one-shot regression meta-architecture in the 2D perspective image space and extend it to generate oriented 3D object bounding boxes from LiDAR point cloud. Our main contribution is in extending the loss function of YOLO v2 to include the yaw angle, the 3D box center in Cartesian coordinates and the height of the box as a direct regression problem. This formulation enables real-time performance, which is essential for automated driving. Our results are showing promising figures on KITTI benchmark, achieving real-time performance (40 fps) on Titan X GPU.},
	author = {Ali, Waleed and Abdelkarim, Sherif and Zahran, Mohamed and Zidan, Mahmoud and Sallab, Ahmad El},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-28 10:56:05 -0700},
	date-modified = {2022-10-28 10:56:23 -0700},
	doi = {10.48550/ARXIV.1808.02350},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
	publisher = {arXiv},
	title = {YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud},
	url = {https://arxiv.org/abs/1808.02350},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1808.02350},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.02350}}

@misc{https://doi.org/10.48550/arxiv.1803.06199,
	author = {Simon, Martin and Milz, Stefan and Amende, Karl and Gross, Horst-Michael},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-28 10:52:29 -0700},
	date-modified = {2022-10-28 10:53:01 -0700},
	doi = {10.48550/ARXIV.1803.06199},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Complex-YOLO: Real-time 3D Object Detection on Point Clouds},
	url = {https://arxiv.org/abs/1803.06199},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1803.06199},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1803.06199}}

@misc{https://doi.org/10.48550/arxiv.2207.02696,
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-28 10:37:05 -0700},
	date-modified = {2022-10-28 10:37:11 -0700},
	doi = {10.48550/ARXIV.2207.02696},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
	url = {https://arxiv.org/abs/2207.02696},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2207.02696},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2207.02696}}

@article{burns1991system,
	abstract = {A pulsed GaAs laser rangefinder is analyzed and designed. Expressions for background 
and signal power, noise, and signal-to-noise ratio are derived. The effects of pulse rise time, 
receiver bandwidth, and SNR on probability of detection and range accuracy are discussed. 
A computer simulation is used to optimize laser power, receiver aperture, and preamplifier 
bandwidth. A method ofthreshold detection is presented and discussed. Experimental 
results include receiver preamplifier transfer function and threshold detector performance.},
	author = {Burns, Hoyt N and Christodoulou, Christos G and Boreman, Glenn D},
	date-added = {2022-10-26 19:07:38 -0700},
	date-modified = {2022-10-26 19:08:03 -0700},
	journal = {Optical engineering},
	number = {3},
	pages = {323--329},
	publisher = {SPIE},
	title = {System design of a pulsed laser rangefinder},
	volume = {30},
	year = {1991}}

@inproceedings{heinzler2019weather,
	abstract = {Lidar sensors are often used in mobile robots and autonomous vehicles to complement camera, radar and ultrasonic sensors for environment perception. Typically, perception algorithms are trained to only detect moving and static objects as well as ground estimation, but intentionally ignore weather effects to reduce false detections. In this work, we present an in-depth analysis of automotive lidar performance under harsh weather conditions, i.e. heavy rain and dense fog. An extensive data set has been recorded for various fog and rain conditions, which is the basis for the conducted in-depth analysis of the point cloud under changing environmental conditions. In addition, we introduce a novel approach to detect and classify rain or fog with lidar sensors only and achieve an mean union over intersection of 97.14% for a data set in controlled environments. The analysis of weather influences on the performance of lidar sensors and the weather detection are important steps towards improving safety levels for autonomous driving in adverse weather conditions by providing reliable information to adapt vehicle behavior.},
	author = {Heinzler, Robin and Schindler, Philipp and Seekircher, J{\"u}rgen and Ritter, Werner and Stork, Wilhelm},
	booktitle = {2019 IEEE intelligent vehicles symposium (IV)},
	date-added = {2022-10-26 18:41:44 -0700},
	date-modified = {2022-10-26 18:42:06 -0700},
	organization = {IEEE},
	pages = {1527--1534},
	title = {Weather influence and classification with automotive lidar sensors},
	year = {2019}}

@inproceedings{haran2016infrared,
	abstract = {In order to be able to evaluate the performances of different Autonomous Emergency Braking (AEB) systems for pedestrian crash avoidance and mitigation, a standard surrogate pedestrian mannequin needs to be developed. One of the requirements for pedestrian mannequin is to ensure it ``looks'' like a real representative pedestrian to each of the sensor modalities used in AEB systems. The purpose of this paper is to generate the recommended IR reflectance specifications for the standard surrogate pedestrian mannequin based on the collected data from various sources and the experiments.},
	author = {Haran, Terence and Chien, Stanley},
	booktitle = {2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)},
	date-added = {2022-10-26 18:40:31 -0700},
	date-modified = {2022-10-26 18:41:07 -0700},
	organization = {IEEE},
	pages = {2230--2235},
	title = {Infrared reflectivity of pedestrian mannequin for autonomous emergency braking testing},
	year = {2016}}

@article{Wojtanowski:2014aa,
	abstract = {Laser rangefinder performance (i.e., maximum range) is strongly affected by environment due to visibility-dependent laser attenuation in the atmosphere and target reflectivity variations induced by surface condition changes (dry vs. wet). Both factors have their unique spectral features which means that rangefinders operating at different wavelengths are affected by specific environmental changes in a different way. Current state of the art TOF (time of flight) semiconductor laser rangefinders are based mainly on two wavelengths: 905 nm and 1550 nm, which results from atmospheric transmission windows and availability of high power pulsed sources. The paper discusses the scope of maximum range degradation of hypothetical 0.9 μm and 1.5 μm rangefinders due to selected water-related environmental effects. Atmospheric extinction spectra were adapted from Standard Atmosphere Model and reflectance fingerprints of various materials have been measured. It is not the aim of the paper to determine in general which wavelength is superior for laser range finding, since a number of criteria could be considered, but to verify their susceptibility to adverse environmental conditions.},
	author = {Wojtanowski, J. and Zygmunt, M. and Kaszczuk, M. and Mierczyk, Z. and Muzal, M.},
	date = {2014/09/01},
	date-added = {2022-10-26 18:29:43 -0700},
	date-modified = {2022-10-26 18:29:43 -0700},
	doi = {10.2478/s11772-014-0190-2},
	id = {Wojtanowski2014},
	isbn = {1896-3757},
	journal = {Opto-Electronics Review},
	number = {3},
	pages = {183--190},
	title = {Comparison of 905 nm and 1550 nm semiconductor laser rangefinders'performance deterioration due to adverse environmental conditions},
	url = {https://doi.org/10.2478/s11772-014-0190-2},
	volume = {22},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.2478/s11772-014-0190-2}}

@misc{https://doi.org/10.48550/arxiv.1801.09847,
	abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
	author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-26 15:59:52 -0700},
	date-modified = {2022-10-26 16:00:06 -0700},
	doi = {10.48550/ARXIV.1801.09847},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Open3D: A Modern Library for 3D Data Processing},
	url = {https://arxiv.org/abs/1801.09847},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1801.09847},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1801.09847}}

@misc{https://doi.org/10.48550/arxiv.1912.04838,
	abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at https://waymo.com/open/.},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhao, Sheng and Cheng, Shuyang and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-26 15:58:07 -0700},
	date-modified = {2022-10-26 15:58:07 -0700},
	doi = {10.48550/ARXIV.1912.04838},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
	url = {https://arxiv.org/abs/1912.04838},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1912.04838},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1912.04838}}
