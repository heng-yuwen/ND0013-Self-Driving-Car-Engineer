# Self-Driving Car Engineer Nanodegree
## Course 2: Sensor Fusion
#### By Jonathan L. Moran (jonathan.moran107@gmail.com)
From the Self-Driving Car Engineer Nanodegree programme offered at Udacity.

This is Course 2: Sensor Fusion in the Self-Driving Car Engineer Nanodegree programme taught by Dr. Antje Muntzinger and Dr. Andreas Haja.


### Course Objectives
* Develop a strong understanding of the important role LiDAR plays in the autonomous vehicle;
* Learn to work with LiDAR range data, 3D point clouds and bird's-eye view (BEV) maps;
* Build 3D object detection models using deep learning with LiDAR point cloud data;
* Perform multi-target tracking with the Extended Kalman Filter (EKF) on multi-modal sensor data;
* Apply learnings to complete two real-world AD/ADAS detection and tracking software programmes. 


### Projects
* ⬜️ 2.1: Object Detection with Sensor Fusion (3D);
* ⬜️ 2.2: Multi-Target Tracking with Extended Kalman filter (MTT with EKF).


### Exercises
* Will be announced as the course progresses.


### Course Contents

The following topics are covered in course exercises:
* Will be announced as the course progresses.


Other topics covered in course lectures and reading material:
* Importance of multi-modal sensors in autonomous vehicles
* In-depth information on camera / LiDAR / radar / ultrasonics for AVs 
* Selecting the best sensor(s) for a given job
* And much more ... (will be announced as course progresses)


### Learning Outcomes
#### Lesson 1: Introduction to Sensor Fusion and Perception
* Distinguish the strengths and weaknesses of each sensor modality;
* Understand how each sensor contributes to autonomous vehicle perception systems;
* Pick the most adequate sensor for a particular perception task.
* Pick the most adequate model for a particular ML task;
* Select and visualise the LiDAR range data and construct 3D point clouds.

#### Lesson 2: The LiDAR Sensor
* Explain the role and importance of LiDAR in autonomous driving systems;
* Extract LiDAR range data from the Waymo Open Dataset;
* Extract LiDAR attribute data e.g., point correspondences;
* Visualise the LiDAR range data.

#### Lesson 3: Detecting Objects in LiDAR Data
* Transform 3D point clouds into bird's-eye view (BEV) maps;
* Perform model inference using BEV maps;
* Visualise the detection results;
* Evaluate object detection model performance with metrics;
* Experiment with state-of-the-art (SOTA) models and compare their performances.


### Material
Syllabus:
* [Program Syllabus | Udacity Nanodegree](https://d20vrrgs8k4bvw.cloudfront.net/documents/en-US/Self-Driving+Car+Engineer+Nanodegree+Syllabus+nd0013+.pdf).

Literature:
* See specific assignments for related literature.

Datasets:
* [Waymo Open Dataset: Perception](https://waymo.com/open/).

Lectures:
* Lecture materials (videos, slides) available offline. Course lecture notes available on request.

### Other resources
Companion code:
* [Sensor Fusion and Tracking | Starter code](https://github.com/udacity/nd013-c2-fusion-starter).
